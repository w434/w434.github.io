<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning | w434's blog</title><meta name="author" content="w434"><meta name="copyright" content="w434"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement LearningAbstract DeepSeek-R1-Zero 是一个通过大规模强化学习（RL）训练的模型，没有使用监督微调（SFT）作为初步步骤：  通过 RL，DeepSeek-R1-Zero 自然涌现出许多强大且有趣的推理行为 面临一些挑战，">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning">
<meta property="og:url" content="http://example.com/post/30f06d20.html">
<meta property="og:site_name" content="w434&#39;s blog">
<meta property="og:description" content="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement LearningAbstract DeepSeek-R1-Zero 是一个通过大规模强化学习（RL）训练的模型，没有使用监督微调（SFT）作为初步步骤：  通过 RL，DeepSeek-R1-Zero 自然涌现出许多强大且有趣的推理行为 面临一些挑战，">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2025/02/21/D6Cb5LuMrkVQeiz.jpg">
<meta property="article:published_time" content="2025-02-21T04:36:00.000Z">
<meta property="article:modified_time" content="2025-02-21T08:01:21.360Z">
<meta property="article:author" content="w434">
<meta property="article:tag" content="deepseek">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2025/02/21/D6Cb5LuMrkVQeiz.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/post/30f06d20.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-02-21 16:01:21'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2024/07/16/lsEXfWtGT6eRu7k.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">82</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">28</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">22</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/notes/"><i class="fa-fw fa-solid fa-school"></i><span> 课程笔记</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-face-smile"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/matches/"><i class="fa-fw fa-solid fa-futbol"></i><span> 足球赛事</span></a></li><li><a class="site-page child" href="/chess/"><i class="fa-fw fa-solid fa-chess"></i><span> 国际象棋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw fa-solid fa-film"></i><span> 影评&amp;演出评论&amp;书评</span></a></li><li><a class="site-page child" href="/jottings/"><i class="fa-fw fa-solid fa-book"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/varietyShow/"><i class="fa-fw fa-solid fa-tv"></i><span> 综艺</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/musics/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2025/02/21/D6Cb5LuMrkVQeiz.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="w434's blog"><span class="site-name">w434's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/notes/"><i class="fa-fw fa-solid fa-school"></i><span> 课程笔记</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-face-smile"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/matches/"><i class="fa-fw fa-solid fa-futbol"></i><span> 足球赛事</span></a></li><li><a class="site-page child" href="/chess/"><i class="fa-fw fa-solid fa-chess"></i><span> 国际象棋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw fa-solid fa-film"></i><span> 影评&amp;演出评论&amp;书评</span></a></li><li><a class="site-page child" href="/jottings/"><i class="fa-fw fa-solid fa-book"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/varietyShow/"><i class="fa-fw fa-solid fa-tv"></i><span> 综艺</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/musics/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-02-21T04:36:00.000Z" title="Created 2025-02-21 12:36:00">2025-02-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-02-21T08:01:21.360Z" title="Updated 2025-02-21 16:01:21">2025-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning"><a href="#DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"></a>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><p>DeepSeek-R1-Zero 是一个通过大规模强化学习（RL）训练的模型，没有使用监督微调（SFT）作为初步步骤：</p>
<ol>
<li>通过 RL，DeepSeek-R1-Zero 自然涌现出许多强大且有趣的推理行为</li>
<li>面临一些挑战，如可读性差和语言混合</li>
</ol>
</li>
<li><p>DeepSeek-R1在 RL 之前结合了多阶段训练和冷启动数据</p>
</li>
</ul>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul>
<li><p>post-training 被证明可以提高推理任务的准确性、与社会价值观对齐并适应用户偏好，同时相对于预训练所需的计算资源较少</p>
</li>
<li><p>OpenAI 的 o1 系列模型首次引入了inference-time scaling，通过增加 Chain-of-Thought（CoT）推理过程的长度：</p>
<ol>
<li>在数学、编程和科学推理等各种推理任务中取得了显著改进</li>
<li>test-time scaling仍然很大</li>
</ol>
</li>
<li><p>deepseek迈出了通过纯强化学习（RL）提升语言模型推理能力的第一步</p>
</li>
<li><p>目标：探索 LLMs 在没有监督数据的情况下发展推理能力的潜力，专注于通过纯 RL 过程进行自我进化</p>
</li>
<li><p>使用 DeepSeek-V3-Base 作为基础模型，并采用 GRPO（Shao et al., 2024）作为 RL 框架来提升模型在推理任务中的表现。经过数千次 RL 步骤后，DeepSeek-R1-Zero 在推理基准测试中表现出色</p>
</li>
<li><p>DeepSeek-R1-Zero 面临一些挑战，如可读性差和语言混合</p>
</li>
<li><p>DeepSeek-R1结合了少量冷启动数据和多阶段训练流程：</p>
<ol>
<li>首先收集数千条冷启动数据来微调 DeepSeek-V3-Base 模型</li>
<li>随后进行面向推理的 RL</li>
<li>在 RL 过程接近收敛时，通过对 RL 检查点进行拒绝采样来创建新的 SFT 数据，并结合来自 DeepSeek-V3 的写作、事实问答和自我认知等领域的监督数据</li>
<li>然后重新训练 DeepSeek-V3-Base 模型</li>
<li>在使用新数据进行微调后，检查点会经历额外的 RL 过程，考虑所有场景的提示</li>
</ol>
</li>
<li><p>从 DeepSeek-R1 到更小密集模型的distillation(蒸馏)：直接从 DeepSeek-R1 进行蒸馏的效果优于在其上应用 RL。这表明较大基础模型发现的推理模式对于提升推理能力至关重要</p>
</li>
</ul>
<h3 id="1-1-Contributions"><a href="#1-1-Contributions" class="headerlink" title="1.1. Contributions"></a>1.1. Contributions</h3><h4 id="Post-Training-在基础模型上进行大规模强化学习"><a href="#Post-Training-在基础模型上进行大规模强化学习" class="headerlink" title="Post-Training:在基础模型上进行大规模强化学习"></a>Post-Training:在基础模型上进行大规模强化学习</h4><ul>
<li><p>直接在基础模型上应用 RL，而不依赖监督微调（SFT）作为初步步骤。这种方法允许模型探索 Chain-of-Thought（CoT）来解决复杂问题，从而开发出 DeepSeek-R1-Zero</p>
</li>
<li><p>DeepSeek-R1-Zero 展示了自我验证、反思和生成长 CoT 等能力</p>
</li>
<li><p>第一个通过纯 RL 激励 LLMs 推理能力的开放研究，无需 SFT</p>
</li>
<li><p>开发 DeepSeek-R1 的流程：</p>
<ol>
<li>两个 RL 阶段，旨在发现改进的推理模式并与人类偏好对齐</li>
<li>两个 SFT 阶段，作为模型推理和非推理能力的种子</li>
</ol>
</li>
</ul>
<h4 id="Distillation-小模型也可以很强大"><a href="#Distillation-小模型也可以很强大" class="headerlink" title="Distillation:小模型也可以很强大"></a>Distillation:小模型也可以很强大</h4><ul>
<li><p>可以将大模型的推理模式蒸馏到小模型中，从而获得比在小模型上通过 RL 发现的推理模式更好的性能</p>
</li>
<li><p>蒸馏后的小型密集模型在基准测试中表现优异</p>
</li>
</ul>
<h3 id="1-2-Summary-of-Evaluation-Results"><a href="#1-2-Summary-of-Evaluation-Results" class="headerlink" title="1.2. Summary of Evaluation Results"></a>1.2. Summary of Evaluation Results</h3><h4 id="Reasoning-tasks"><a href="#Reasoning-tasks" class="headerlink" title="Reasoning tasks"></a>Reasoning tasks</h4><h4 id="Knowledge"><a href="#Knowledge" class="headerlink" title="Knowledge"></a>Knowledge</h4><h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4><h2 id="2-Approach"><a href="#2-Approach" class="headerlink" title="2. Approach"></a>2. Approach</h2><h3 id="2-1-Overview"><a href="#2-1-Overview" class="headerlink" title="2.1. Overview"></a>2.1. Overview</h3><ul>
<li><p>之前的工作严重依赖大量监督数据来提升模型性能</p>
</li>
<li><p>本研究证明了即使不使用监督微调（SFT）作为冷启动，通过大规模强化学习（RL）也可以显著提升推理能力</p>
</li>
<li><p>结合少量冷启动数据可以进一步提升性能</p>
</li>
<li><p>在接下来的部分中，我们将介绍：</p>
<ol>
<li>DeepSeek-R1-Zero，它直接在基础模型上应用 RL，不使用任何 SFT 数据</li>
<li>DeepSeek-R1，它从经过数千条长 Chain-of-Thought（CoT）示例微调的检查点开始应用 RL</li>
<li>将 DeepSeek-R1 的推理能力蒸馏到小型密集模型中</li>
</ol>
</li>
</ul>
<h3 id="2-2-DeepSeek-R1-Zero-Reinforcement-Learning-on-the-Base-Model"><a href="#2-2-DeepSeek-R1-Zero-Reinforcement-Learning-on-the-Base-Model" class="headerlink" title="2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model"></a>2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model</h3><ul>
<li><p>强化学习在推理任务中展示了显著的有效性。然而，这些工作严重依赖监督数据，而这些数据的收集非常耗时</p>
</li>
<li><p>在本节中，我们探索 LLMs 在没有监督数据的情况下发展推理能力的潜力，专注于通过纯强化学习过程进行自我进化</p>
</li>
</ul>
<h4 id="2-2-1-Reinforcement-Learning-Algorithm"><a href="#2-2-1-Reinforcement-Learning-Algorithm" class="headerlink" title="2.2.1. Reinforcement Learning Algorithm"></a>2.2.1. Reinforcement Learning Algorithm</h4><h5 id="Group-Relative-Policy-Optimization-组相对策略优化"><a href="#Group-Relative-Policy-Optimization-组相对策略优化" class="headerlink" title="Group Relative Policy Optimization(组相对策略优化)"></a>Group Relative Policy Optimization(组相对策略优化)</h5><ul>
<li><p>为了节省 RL 的训练成本，采用组相对策略优化（GRPO）：放弃了通常与策略模型大小相同的critic model，而是从组分数中估计基线</p>
</li>
<li><p>具体来说，对于每个问题 q，GRPO 从旧策略 πθold 中采样一组输出 {o1,o2,⋯ ,oG}{o1,o2,⋯,oG}，然后通过最大化以下目标来优化策略模型 πθ：</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/02/21/vJSnhfZyrdB4XWc.png" alt="image.png"></p>
<ul>
<li>其中 ε 和 β 是超参数，Ai是优势，通过使用组内每个输出对应的奖励 {r1,r2,…,rG}计算：</li>
</ul>
<p><img src="https://s2.loli.net/2025/02/21/PA41dqbvwF8sDfo.png" alt="image.png"></p>
<h4 id="2-2-2-Reward-Modeling"><a href="#2-2-2-Reward-Modeling" class="headerlink" title="2.2.2. Reward Modeling"></a>2.2.2. Reward Modeling</h4><ul>
<li><p>奖励是训练信号的来源，决定了 RL 的优化方向</p>
</li>
<li><p>训练 DeepSeek-R1-Zero 采用了基于规则的奖励系统，主要包括两种类型的奖励：</p>
<ol>
<li>Accuracy rewards:准确性奖励模型评估响应是否正确</li>
<li>Format rewards:格式奖励模型强制模型将其思考过程放在 ‘<think>‘ 和 ‘</think>‘ 标签之间</li>
</ol>
</li>
<li><p>没有在开发 DeepSeek-R1-Zero 时应用outcome or process neural reward model，因为神经奖励模型在大规模强化学习过程中可能会受到奖励攻击（reward hacking）的影响，并且重新训练奖励模型需要额外的训练资源，这会使整个训练流程复杂化</p>
</li>
</ul>
<h4 id="2-2-3-Training-Template"><a href="#2-2-3-Training-Template" class="headerlink" title="2.2.3. Training Template"></a>2.2.3. Training Template</h4><ul>
<li><p>为了训练 DeepSeek-R1-Zero，首先设计了一个简单的模板，指导基础模型遵循我们指定的指令</p>
</li>
<li><p>该模板要求 DeepSeek-R1-Zero 首先生成推理过程，然后提供最终答案</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/02/21/9lIEr6nvatpQOJR.png" alt="image.png"></p>
<ul>
<li>有意将约束限制在这种结构格式上，避免任何内容特定的偏见——例如强制反思推理或推广特定的问题解决策略——以确保我们能够准确观察模型在 RL 过程中的自然进展</li>
</ul>
<h4 id="2-2-4-Performance-Self-evolution-Process-and-Aha-Moment-of-DeepSeek-R1-Zero"><a href="#2-2-4-Performance-Self-evolution-Process-and-Aha-Moment-of-DeepSeek-R1-Zero" class="headerlink" title="2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero"></a>2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero</h4><h5 id="Performance-of-DeepSeek-R1-Zero"><a href="#Performance-of-DeepSeek-R1-Zero" class="headerlink" title="Performance of DeepSeek-R1-Zero"></a>Performance of DeepSeek-R1-Zero</h5><ul>
<li><p>随着 RL 训练的推进，DeepSeek-R1-Zero 表现出稳定且一致的性能提升</p>
</li>
<li><p>RL 使 DeepSeek-R1-Zero 能够在不需要任何监督微调数据的情况下获得强大的推理能力</p>
</li>
<li><p>通过应用多数投票(majority voting)，DeepSeek-R1-Zero 的性能可以进一步提升</p>
</li>
</ul>
<h5 id="Self-evolution-Process-of-DeepSeek-R1-Zero"><a href="#Self-evolution-Process-of-DeepSeek-R1-Zero" class="headerlink" title="Self-evolution Process of DeepSeek-R1-Zero"></a>Self-evolution Process of DeepSeek-R1-Zero</h5><ul>
<li><p>DeepSeek-R1-Zero 的自我进化过程展示了 RL 如何驱动模型自主提升其推理能力</p>
</li>
<li><p>通过直接从基础模型启动 RL，可以密切监控模型的进展，而不受监督微调阶段的影响。这种方法清晰地展示了模型如何随着时间的推移而进化，特别是在处理复杂推理任务的能力方面</p>
</li>
<li><p>DeepSeek-R1-Zero 的思考时间在整个训练过程中持续改善。这种改进不是外部调整的结果，而是模型内部的固有发展</p>
</li>
<li><p>DeepSeek-R1-Zero 通过利用扩展的测试时间计算(extended test-time computation)，自然地获得了解决日益复杂的推理任务的能力</p>
</li>
<li><p>自我进化中最引人注目的方面之一是随着测试时间计算的增加，复杂行为的涌现。例如反思——模型重新审视和重新评估其先前步骤——以及探索替代问题解决方法的行为自发地出现</p>
</li>
<li><p>这些行为没有被明确编程，而是作为模型与强化学习环境交互的结果而出现。这种自发的发展显著增强了 DeepSeek-R1-Zero 的推理能力，使其能够更高效、更准确地应对更具挑战性的任务</p>
</li>
</ul>
<h4 id="Aha-Moment-顿悟时刻-of-DeepSeek-R1-Zero"><a href="#Aha-Moment-顿悟时刻-of-DeepSeek-R1-Zero" class="headerlink" title="Aha Moment(顿悟时刻) of DeepSeek-R1-Zero"></a>Aha Moment(顿悟时刻) of DeepSeek-R1-Zero</h4><ul>
<li><p>在 DeepSeek-R1-Zero 的训练过程中观察到一个特别有趣的现象，即“顿悟”时刻的出现</p>
</li>
<li><p>这一时刻发生在模型的中间版本中。在这个阶段，DeepSeek-R1-Zero 学会了通过重新评估其初始方法来分配更多的思考时间</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/02/21/25GhjJ6oMcA1qzf.png" alt="image.png"></p>
<ul>
<li><p>这种行为不仅证明了模型不断增长的推理能力，也是强化学习如何导致意外和复杂结果的例子</p>
</li>
<li><p>不是明确地教模型如何解决问题，而是简单地为其提供正确的激励，它就会自主地发展出高级的问题解决策略</p>
</li>
<li><p>顿悟”时刻说明，RL 有潜力在人工系统中解锁新的智能水平，为未来更自主和自适应的模型铺平道路</p>
</li>
</ul>
<h4 id="Drawback-of-DeepSeek-R1-Zero"><a href="#Drawback-of-DeepSeek-R1-Zero" class="headerlink" title="Drawback of DeepSeek-R1-Zero"></a>Drawback of DeepSeek-R1-Zero</h4><ul>
<li><p>DeepSeek-R1-Zero 在处理可读性差和语言混合等挑战时遇到困难</p>
</li>
<li><p>为了使推理过程更具可读性并与其他地区共享，我们探索了 DeepSeek-R1，这是一种利用 RL 和人类友好的冷启动数据(cold-start data)的方法</p>
</li>
</ul>
<h3 id="2-3-DeepSeek-R1-Reinforcement-Learning-with-Cold-Start"><a href="#2-3-DeepSeek-R1-Reinforcement-Learning-with-Cold-Start" class="headerlink" title="2.3. DeepSeek-R1: Reinforcement Learning with Cold Start"></a>2.3. DeepSeek-R1: Reinforcement Learning with Cold Start</h3><ul>
<li><p>受到 DeepSeek-R1-Zero 的鼓舞，两个自然的问题出现了：</p>
<ol>
<li>通过结合少量高质量数据作为冷启动，能否进一步提升推理性能或加速收敛？</li>
<li>如何训练一个用户友好的模型，既能生成清晰且连贯的 Chain-of-Thought（CoT），又能展示强大的通用能力？</li>
</ol>
</li>
<li><p>DeepSeek-R1流程分为下面四个阶段</p>
</li>
</ul>
<h4 id="2-3-1-Cold-Start"><a href="#2-3-1-Cold-Start" class="headerlink" title="2.3.1. Cold Start"></a>2.3.1. Cold Start</h4><ul>
<li><p>为了防止从基础模型开始的 RL 训练早期不稳定阶段，对于 DeepSeek-R1，我们构建并收集了少量长 CoT 数据来微调模型作为初始 RL 参与者</p>
</li>
<li><p>收集这些数据的几种方法：使用带有长 CoT 示例的少样本提示，直接提示模型生成带有反思和验证的详细答案，收集 DeepSeek-R1-Zero 的可读格式输出，并通过人工注释员的后处理来优化结果</p>
</li>
<li><p>与 DeepSeek-R1-Zero 相比，冷启动数据的优势包括：</p>
<ol>
<li>可读性：DeepSeek-R1-Zero 的一个关键限制是其内容通常不适合阅读。在为 DeepSeek-R1 创建冷启动数据时，设计了一种可读的模式，包括在每个响应的末尾添加摘要，并过滤掉不适合阅读的响应。将输出格式定义为 |special_token|<reasoning_process>|special_token|&lt;summary，其中推理过程是查询的 CoT，摘要用于总结推理结果</li>
<li>潜力：与 DeepSeek-R1-Zero 相比，性能有所提升。相信迭代训练是推理模型的更好方式</li>
</ol>
</li>
</ul>
<h4 id="2-3-2-Reasoning-oriented-Reinforcement-Learning-面向推理的强化学习"><a href="#2-3-2-Reasoning-oriented-Reinforcement-Learning-面向推理的强化学习" class="headerlink" title="2.3.2. Reasoning-oriented Reinforcement Learning(面向推理的强化学习)"></a>2.3.2. Reasoning-oriented Reinforcement Learning(面向推理的强化学习)</h4><ul>
<li><p>在对 DeepSeek-V3-Base 进行冷启动数据微调后，应用了与 DeepSeek-R1-Zero 相同的大规模强化学习训练过程</p>
</li>
<li><p>为了缓解语言混合问题，我们在 RL 训练期间引入了语言一致性奖励，该奖励计算为目标语言单词在 CoT 中的比例</p>
</li>
<li><p>这种对齐会导致模型性能略有下降，但这种奖励与人类偏好一致，使其更具可读性</p>
</li>
<li><p>最后将推理任务的准确性和语言一致性奖励直接相加，形成最终奖励。然后在微调后的模型上应用 RL 训练，直到其在推理任务上达到收敛</p>
</li>
</ul>
<h4 id="2-3-3-Rejection-Sampling-and-Supervised-Fine-Tuning"><a href="#2-3-3-Rejection-Sampling-and-Supervised-Fine-Tuning" class="headerlink" title="2.3.3. Rejection Sampling and Supervised Fine-Tuning"></a>2.3.3. Rejection Sampling and Supervised Fine-Tuning</h4></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/deepseek/">deepseek</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2025/02/21/D6Cb5LuMrkVQeiz.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/post/e0e02130.html" title="计算机视觉 22 Image Generation"><img class="cover" src="https://s2.loli.net/2024/11/18/zAG9fYxh35LIWK2.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">计算机视觉 22 Image Generation</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2024/07/16/lsEXfWtGT6eRu7k.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">w434</div><div class="author-info__description">An undergraduate majoring in AI at PKU.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">82</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">28</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">22</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/w434"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/w434" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcom to my blog.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning"><span class="toc-number">1.</span> <span class="toc-text">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-number">1.2.</span> <span class="toc-text">1. Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Contributions"><span class="toc-number">1.2.1.</span> <span class="toc-text">1.1. Contributions</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Post-Training-%E5%9C%A8%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E4%B8%8A%E8%BF%9B%E8%A1%8C%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">Post-Training:在基础模型上进行大规模强化学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Distillation-%E5%B0%8F%E6%A8%A1%E5%9E%8B%E4%B9%9F%E5%8F%AF%E4%BB%A5%E5%BE%88%E5%BC%BA%E5%A4%A7"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">Distillation:小模型也可以很强大</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Summary-of-Evaluation-Results"><span class="toc-number">1.2.2.</span> <span class="toc-text">1.2. Summary of Evaluation Results</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Reasoning-tasks"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">Reasoning tasks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Knowledge"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">Knowledge</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Others"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">Others</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Approach"><span class="toc-number">1.3.</span> <span class="toc-text">2. Approach</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Overview"><span class="toc-number">1.3.1.</span> <span class="toc-text">2.1. Overview</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-DeepSeek-R1-Zero-Reinforcement-Learning-on-the-Base-Model"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-Reinforcement-Learning-Algorithm"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">2.2.1. Reinforcement Learning Algorithm</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Group-Relative-Policy-Optimization-%E7%BB%84%E7%9B%B8%E5%AF%B9%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96"><span class="toc-number">1.3.2.1.1.</span> <span class="toc-text">Group Relative Policy Optimization(组相对策略优化)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-Reward-Modeling"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">2.2.2. Reward Modeling</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-3-Training-Template"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">2.2.3. Training Template</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-4-Performance-Self-evolution-Process-and-Aha-Moment-of-DeepSeek-R1-Zero"><span class="toc-number">1.3.2.4.</span> <span class="toc-text">2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Performance-of-DeepSeek-R1-Zero"><span class="toc-number">1.3.2.4.1.</span> <span class="toc-text">Performance of DeepSeek-R1-Zero</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Self-evolution-Process-of-DeepSeek-R1-Zero"><span class="toc-number">1.3.2.4.2.</span> <span class="toc-text">Self-evolution Process of DeepSeek-R1-Zero</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Aha-Moment-%E9%A1%BF%E6%82%9F%E6%97%B6%E5%88%BB-of-DeepSeek-R1-Zero"><span class="toc-number">1.3.2.5.</span> <span class="toc-text">Aha Moment(顿悟时刻) of DeepSeek-R1-Zero</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Drawback-of-DeepSeek-R1-Zero"><span class="toc-number">1.3.2.6.</span> <span class="toc-text">Drawback of DeepSeek-R1-Zero</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-DeepSeek-R1-Reinforcement-Learning-with-Cold-Start"><span class="toc-number">1.3.3.</span> <span class="toc-text">2.3. DeepSeek-R1: Reinforcement Learning with Cold Start</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-1-Cold-Start"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">2.3.1. Cold Start</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-2-Reasoning-oriented-Reinforcement-Learning-%E9%9D%A2%E5%90%91%E6%8E%A8%E7%90%86%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">2.3.2. Reasoning-oriented Reinforcement Learning(面向推理的强化学习)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-3-Rejection-Sampling-and-Supervised-Fine-Tuning"><span class="toc-number">1.3.3.3.</span> <span class="toc-text">2.3.3. Rejection Sampling and Supervised Fine-Tuning</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/30f06d20.html" title="DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"><img src="https://s2.loli.net/2025/02/21/D6Cb5LuMrkVQeiz.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"/></a><div class="content"><a class="title" href="/post/30f06d20.html" title="DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning">DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a><time datetime="2025-02-21T04:36:00.000Z" title="Created 2025-02-21 12:36:00">2025-02-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/e0e02130.html" title="计算机视觉 22 Image Generation"><img src="https://s2.loli.net/2024/11/18/zAG9fYxh35LIWK2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机视觉 22 Image Generation"/></a><div class="content"><a class="title" href="/post/e0e02130.html" title="计算机视觉 22 Image Generation">计算机视觉 22 Image Generation</a><time datetime="2025-02-14T11:06:00.000Z" title="Created 2025-02-14 19:06:00">2025-02-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/4ff2381f.html" title="计算机视觉 21 Image Segmentation"><img src="https://s2.loli.net/2024/11/18/zAG9fYxh35LIWK2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机视觉 21 Image Segmentation"/></a><div class="content"><a class="title" href="/post/4ff2381f.html" title="计算机视觉 21 Image Segmentation">计算机视觉 21 Image Segmentation</a><time datetime="2025-02-14T04:30:00.000Z" title="Created 2025-02-14 12:30:00">2025-02-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/3b6aa2ce.html" title="计算机视觉 20 RNN and Transformers"><img src="https://s2.loli.net/2024/11/18/zAG9fYxh35LIWK2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机视觉 20 RNN and Transformers"/></a><div class="content"><a class="title" href="/post/3b6aa2ce.html" title="计算机视觉 20 RNN and Transformers">计算机视觉 20 RNN and Transformers</a><time datetime="2025-02-07T12:01:00.000Z" title="Created 2025-02-07 20:01:00">2025-02-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/1edc9dfc.html" title="计算机视觉 19 Visualizing Models and Generating Images"><img src="https://s2.loli.net/2024/11/18/zAG9fYxh35LIWK2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机视觉 19 Visualizing Models and Generating Images"/></a><div class="content"><a class="title" href="/post/1edc9dfc.html" title="计算机视觉 19 Visualizing Models and Generating Images">计算机视觉 19 Visualizing Models and Generating Images</a><time datetime="2025-02-05T06:44:00.000Z" title="Created 2025-02-05 14:44:00">2025-02-05</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By w434</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcom to my blog.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">简</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>