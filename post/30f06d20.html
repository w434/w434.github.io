<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning | w434's blog</title><meta name="author" content="w434"><meta name="copyright" content="w434"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement LearningAbstract DeepSeek-R1-Zero 是一个通过大规模强化学习（RL）训练的模型，没有使用监督微调（SFT）作为初步步骤：  通过 RL，DeepSeek-R1-Zero 自然涌现出许多强大且有趣的推理行为 面临一些挑战，">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning">
<meta property="og:url" content="http://example.com/post/30f06d20.html">
<meta property="og:site_name" content="w434&#39;s blog">
<meta property="og:description" content="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement LearningAbstract DeepSeek-R1-Zero 是一个通过大规模强化学习（RL）训练的模型，没有使用监督微调（SFT）作为初步步骤：  通过 RL，DeepSeek-R1-Zero 自然涌现出许多强大且有趣的推理行为 面临一些挑战，">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2025/02/21/D6Cb5LuMrkVQeiz.jpg">
<meta property="article:published_time" content="2025-02-21T04:36:00.000Z">
<meta property="article:modified_time" content="2025-02-24T05:54:43.607Z">
<meta property="article:author" content="w434">
<meta property="article:tag" content="deepseek">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2025/02/21/D6Cb5LuMrkVQeiz.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/post/30f06d20.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-02-24 13:54:43'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2024/07/16/lsEXfWtGT6eRu7k.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">115</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">29</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/notes/"><i class="fa-fw fa-solid fa-school"></i><span> 课程笔记</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-face-smile"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/matches/"><i class="fa-fw fa-solid fa-futbol"></i><span> 足球赛事</span></a></li><li><a class="site-page child" href="/chess/"><i class="fa-fw fa-solid fa-chess"></i><span> 国际象棋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw fa-solid fa-film"></i><span> 影评&amp;演出评论&amp;书评</span></a></li><li><a class="site-page child" href="/jottings/"><i class="fa-fw fa-solid fa-book"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/varietyShow/"><i class="fa-fw fa-solid fa-tv"></i><span> 综艺</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/musics/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2025/02/21/D6Cb5LuMrkVQeiz.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="w434's blog"><span class="site-name">w434's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/notes/"><i class="fa-fw fa-solid fa-school"></i><span> 课程笔记</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-face-smile"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/matches/"><i class="fa-fw fa-solid fa-futbol"></i><span> 足球赛事</span></a></li><li><a class="site-page child" href="/chess/"><i class="fa-fw fa-solid fa-chess"></i><span> 国际象棋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw fa-solid fa-film"></i><span> 影评&amp;演出评论&amp;书评</span></a></li><li><a class="site-page child" href="/jottings/"><i class="fa-fw fa-solid fa-book"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/varietyShow/"><i class="fa-fw fa-solid fa-tv"></i><span> 综艺</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/musics/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-02-21T04:36:00.000Z" title="Created 2025-02-21 12:36:00">2025-02-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-02-24T05:54:43.607Z" title="Updated 2025-02-24 13:54:43">2025-02-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="DeepSeek-R1:Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning"><a href="#DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"></a>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><p>DeepSeek-R1-Zero 是一个通过大规模强化学习（RL）训练的模型，没有使用监督微调（SFT）作为初步步骤：</p>
<ol>
<li>通过 RL，DeepSeek-R1-Zero 自然涌现出许多强大且有趣的推理行为</li>
<li>面临一些挑战，如可读性差和语言混合</li>
</ol>
</li>
<li><p>DeepSeek-R1在 RL 之前结合了多阶段训练和冷启动数据</p>
</li>
</ul>
<hr>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul>
<li><p>post-training 被证明可以提高推理任务的准确性、与社会价值观对齐并适应用户偏好，同时相对于预训练所需的计算资源较少</p>
</li>
<li><p>OpenAI 的 o1 系列模型首次引入了inference-time scaling，通过增加 Chain-of-Thought（CoT）推理过程的长度：</p>
<ol>
<li>在数学、编程和科学推理等各种推理任务中取得了显著改进</li>
<li>test-time scaling仍然很大</li>
</ol>
</li>
<li><p>deepseek迈出了通过纯强化学习（RL）提升语言模型推理能力的第一步</p>
</li>
<li><p>目标：探索 LLMs 在没有监督数据的情况下发展推理能力的潜力，专注于通过纯 RL 过程进行自我进化</p>
</li>
<li><p>使用 DeepSeek-V3-Base 作为基础模型，并采用 GRPO（Shao et al., 2024）作为 RL 框架来提升模型在推理任务中的表现。经过数千次 RL 步骤后，DeepSeek-R1-Zero 在推理基准测试中表现出色</p>
</li>
<li><p>DeepSeek-R1-Zero 面临一些挑战，如可读性差和语言混合</p>
</li>
<li><p>DeepSeek-R1结合了少量冷启动数据和多阶段训练流程：</p>
<ol>
<li>首先收集数千条冷启动数据来微调 DeepSeek-V3-Base 模型</li>
<li>随后进行面向推理的 RL</li>
<li>在 RL 过程接近收敛时，通过对 RL 检查点进行拒绝采样来创建新的 SFT 数据，并结合来自 DeepSeek-V3 的写作、事实问答和自我认知等领域的监督数据</li>
<li>然后重新训练 DeepSeek-V3-Base 模型</li>
<li>在使用新数据进行微调后，检查点会经历额外的 RL 过程，考虑所有场景的提示</li>
</ol>
</li>
<li><p>从 DeepSeek-R1 到更小密集模型的distillation(蒸馏)：直接从 DeepSeek-R1 进行蒸馏的效果优于在其上应用 RL。这表明较大基础模型发现的推理模式对于提升推理能力至关重要</p>
</li>
</ul>
<h3 id="1-1-Contributions"><a href="#1-1-Contributions" class="headerlink" title="1.1. Contributions"></a>1.1. Contributions</h3><h4 id="Post-Training-在基础模型上进行大规模强化学习"><a href="#Post-Training-在基础模型上进行大规模强化学习" class="headerlink" title="Post-Training:在基础模型上进行大规模强化学习"></a>Post-Training:在基础模型上进行大规模强化学习</h4><ul>
<li><p>直接在基础模型上应用 RL，而不依赖监督微调（SFT）作为初步步骤。这种方法允许模型探索 Chain-of-Thought（CoT）来解决复杂问题，从而开发出 DeepSeek-R1-Zero</p>
</li>
<li><p>DeepSeek-R1-Zero 展示了自我验证、反思和生成长 CoT 等能力</p>
</li>
<li><p>第一个通过纯 RL 激励 LLMs 推理能力的开放研究，无需 SFT</p>
</li>
<li><p>开发 DeepSeek-R1 的流程：</p>
<ol>
<li>两个 RL 阶段，旨在发现改进的推理模式并与人类偏好对齐</li>
<li>两个 SFT 阶段，作为模型推理和非推理能力的种子</li>
</ol>
</li>
</ul>
<h4 id="Distillation-小模型也可以很强大"><a href="#Distillation-小模型也可以很强大" class="headerlink" title="Distillation:小模型也可以很强大"></a>Distillation:小模型也可以很强大</h4><ul>
<li><p>可以将大模型的推理模式蒸馏到小模型中，从而获得比在小模型上通过 RL 发现的推理模式更好的性能</p>
</li>
<li><p>蒸馏后的小型密集模型在基准测试中表现优异</p>
</li>
</ul>
<h3 id="1-2-Summary-of-Evaluation-Results"><a href="#1-2-Summary-of-Evaluation-Results" class="headerlink" title="1.2. Summary of Evaluation Results"></a>1.2. Summary of Evaluation Results</h3><h4 id="Reasoning-tasks"><a href="#Reasoning-tasks" class="headerlink" title="Reasoning tasks"></a>Reasoning tasks</h4><h4 id="Knowledge"><a href="#Knowledge" class="headerlink" title="Knowledge"></a>Knowledge</h4><h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4><hr>
<h2 id="2-Approach"><a href="#2-Approach" class="headerlink" title="2. Approach"></a>2. Approach</h2><h3 id="2-1-Overview"><a href="#2-1-Overview" class="headerlink" title="2.1. Overview"></a>2.1. Overview</h3><ul>
<li><p>之前的工作严重依赖大量监督数据来提升模型性能</p>
</li>
<li><p>本研究证明了即使不使用监督微调（SFT）作为冷启动，通过大规模强化学习（RL）也可以显著提升推理能力</p>
</li>
<li><p>结合少量冷启动数据可以进一步提升性能</p>
</li>
<li><p>在接下来的部分中，我们将介绍：</p>
<ol>
<li>DeepSeek-R1-Zero，它直接在基础模型上应用 RL，不使用任何 SFT 数据</li>
<li>DeepSeek-R1，它从经过数千条长 Chain-of-Thought（CoT）示例微调的检查点开始应用 RL</li>
<li>将 DeepSeek-R1 的推理能力蒸馏到小型密集模型中</li>
</ol>
</li>
</ul>
<h3 id="2-2-DeepSeek-R1-Zero-Reinforcement-Learning-on-the-Base-Model"><a href="#2-2-DeepSeek-R1-Zero-Reinforcement-Learning-on-the-Base-Model" class="headerlink" title="2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model"></a>2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model</h3><ul>
<li><p>强化学习在推理任务中展示了显著的有效性。然而，这些工作严重依赖监督数据，而这些数据的收集非常耗时</p>
</li>
<li><p>在本节中，我们探索 LLMs 在没有监督数据的情况下发展推理能力的潜力，专注于通过纯强化学习过程进行自我进化</p>
</li>
</ul>
<h4 id="2-2-1-Reinforcement-Learning-Algorithm"><a href="#2-2-1-Reinforcement-Learning-Algorithm" class="headerlink" title="2.2.1. Reinforcement Learning Algorithm"></a>2.2.1. Reinforcement Learning Algorithm</h4><h5 id="Group-Relative-Policy-Optimization-组相对策略优化"><a href="#Group-Relative-Policy-Optimization-组相对策略优化" class="headerlink" title="Group Relative Policy Optimization(组相对策略优化)"></a>Group Relative Policy Optimization(组相对策略优化)</h5><ul>
<li><p>为了节省 RL 的训练成本，采用组相对策略优化（GRPO）：放弃了通常与策略模型大小相同的critic model，而是从组分数中估计基线</p>
</li>
<li><p>具体来说，对于每个问题 q，GRPO 从旧策略 πθold 中采样一组输出 {o1,o2,⋯ ,oG}{o1,o2,⋯,oG}，然后通过最大化以下目标来优化策略模型 πθ：</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/02/21/vJSnhfZyrdB4XWc.png" alt="image.png"></p>
<ul>
<li>其中 ε 和 β 是超参数，Ai是优势，通过使用组内每个输出对应的奖励 {r1,r2,…,rG}计算：</li>
</ul>
<p><img src="https://s2.loli.net/2025/02/21/PA41dqbvwF8sDfo.png" alt="image.png"></p>
<h4 id="2-2-2-Reward-Modeling"><a href="#2-2-2-Reward-Modeling" class="headerlink" title="2.2.2. Reward Modeling"></a>2.2.2. Reward Modeling</h4><ul>
<li><p>奖励是训练信号的来源，决定了 RL 的优化方向</p>
</li>
<li><p>训练 DeepSeek-R1-Zero 采用了基于规则的奖励系统，主要包括两种类型的奖励：</p>
<ol>
<li>Accuracy rewards:准确性奖励模型评估响应是否正确</li>
<li>Format rewards:格式奖励模型强制模型将其思考过程放在 ‘(think)’ 和 ‘(&#x2F;think)’ 标签之间</li>
</ol>
</li>
<li><p>没有在开发 DeepSeek-R1-Zero 时应用outcome or process neural reward model，因为神经奖励模型在大规模强化学习过程中可能会受到奖励攻击（reward hacking）的影响，并且重新训练奖励模型需要额外的训练资源，这会使整个训练流程复杂化</p>
</li>
</ul>
<h4 id="2-2-3-Training-Template"><a href="#2-2-3-Training-Template" class="headerlink" title="2.2.3. Training Template"></a>2.2.3. Training Template</h4><ul>
<li><p>为了训练 DeepSeek-R1-Zero，首先设计了一个简单的模板，指导基础模型遵循我们指定的指令</p>
</li>
<li><p>该模板要求 DeepSeek-R1-Zero 首先生成推理过程，然后提供最终答案</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/02/21/9lIEr6nvatpQOJR.png" alt="image.png"></p>
<ul>
<li>有意将约束限制在这种结构格式上，避免任何内容特定的偏见——例如强制反思推理或推广特定的问题解决策略——以确保我们能够准确观察模型在 RL 过程中的自然进展</li>
</ul>
<h4 id="2-2-4-Performance-Self-evolution-Process-and-Aha-Moment-of-DeepSeek-R1-Zero"><a href="#2-2-4-Performance-Self-evolution-Process-and-Aha-Moment-of-DeepSeek-R1-Zero" class="headerlink" title="2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero"></a>2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero</h4><h5 id="Performance-of-DeepSeek-R1-Zero"><a href="#Performance-of-DeepSeek-R1-Zero" class="headerlink" title="Performance of DeepSeek-R1-Zero"></a>Performance of DeepSeek-R1-Zero</h5><ul>
<li><p>随着 RL 训练的推进，DeepSeek-R1-Zero 表现出稳定且一致的性能提升</p>
</li>
<li><p>RL 使 DeepSeek-R1-Zero 能够在不需要任何监督微调数据的情况下获得强大的推理能力</p>
</li>
<li><p>通过应用多数投票(majority voting)，DeepSeek-R1-Zero 的性能可以进一步提升</p>
</li>
</ul>
<h5 id="Self-evolution-Process-of-DeepSeek-R1-Zero"><a href="#Self-evolution-Process-of-DeepSeek-R1-Zero" class="headerlink" title="Self-evolution Process of DeepSeek-R1-Zero"></a>Self-evolution Process of DeepSeek-R1-Zero</h5><ul>
<li><p>DeepSeek-R1-Zero 的自我进化过程展示了 RL 如何驱动模型自主提升其推理能力</p>
</li>
<li><p>通过直接从基础模型启动 RL，可以密切监控模型的进展，而不受监督微调阶段的影响。这种方法清晰地展示了模型如何随着时间的推移而进化，特别是在处理复杂推理任务的能力方面</p>
</li>
<li><p>DeepSeek-R1-Zero 的思考时间在整个训练过程中持续改善。这种改进不是外部调整的结果，而是模型内部的固有发展</p>
</li>
<li><p>DeepSeek-R1-Zero 通过利用扩展的测试时间计算(extended test-time computation)，自然地获得了解决日益复杂的推理任务的能力</p>
</li>
<li><p>自我进化中最引人注目的方面之一是随着测试时间计算的增加，复杂行为的涌现。例如反思——模型重新审视和重新评估其先前步骤——以及探索替代问题解决方法的行为自发地出现</p>
</li>
<li><p>这些行为没有被明确编程，而是作为模型与强化学习环境交互的结果而出现。这种自发的发展显著增强了 DeepSeek-R1-Zero 的推理能力，使其能够更高效、更准确地应对更具挑战性的任务</p>
</li>
</ul>
<h4 id="Aha-Moment-顿悟时刻-of-DeepSeek-R1-Zero"><a href="#Aha-Moment-顿悟时刻-of-DeepSeek-R1-Zero" class="headerlink" title="Aha Moment(顿悟时刻) of DeepSeek-R1-Zero"></a>Aha Moment(顿悟时刻) of DeepSeek-R1-Zero</h4><ul>
<li><p>在 DeepSeek-R1-Zero 的训练过程中观察到一个特别有趣的现象，即“顿悟”时刻的出现</p>
</li>
<li><p>这一时刻发生在模型的中间版本中。在这个阶段，DeepSeek-R1-Zero 学会了通过重新评估其初始方法来分配更多的思考时间</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/02/21/25GhjJ6oMcA1qzf.png" alt="image.png"></p>
<ul>
<li><p>这种行为不仅证明了模型不断增长的推理能力，也是强化学习如何导致意外和复杂结果的例子</p>
</li>
<li><p>不是明确地教模型如何解决问题，而是简单地为其提供正确的激励，它就会自主地发展出高级的问题解决策略</p>
</li>
<li><p>顿悟”时刻说明，RL 有潜力在人工系统中解锁新的智能水平，为未来更自主和自适应的模型铺平道路</p>
</li>
</ul>
<h4 id="Drawback-of-DeepSeek-R1-Zero"><a href="#Drawback-of-DeepSeek-R1-Zero" class="headerlink" title="Drawback of DeepSeek-R1-Zero"></a>Drawback of DeepSeek-R1-Zero</h4><ul>
<li><p>DeepSeek-R1-Zero 在处理可读性差和语言混合等挑战时遇到困难</p>
</li>
<li><p>为了使推理过程更具可读性并与其他地区共享，我们探索了 DeepSeek-R1，这是一种利用 RL 和人类友好的冷启动数据(cold-start data)的方法</p>
</li>
</ul>
<h3 id="2-3-DeepSeek-R1-Reinforcement-Learning-with-Cold-Start"><a href="#2-3-DeepSeek-R1-Reinforcement-Learning-with-Cold-Start" class="headerlink" title="2.3. DeepSeek-R1: Reinforcement Learning with Cold Start"></a>2.3. DeepSeek-R1: Reinforcement Learning with Cold Start</h3><ul>
<li><p>受到 DeepSeek-R1-Zero 的鼓舞，两个自然的问题出现了：</p>
<ol>
<li>通过结合少量高质量数据作为冷启动，能否进一步提升推理性能或加速收敛？</li>
<li>如何训练一个用户友好的模型，既能生成清晰且连贯的 Chain-of-Thought（CoT），又能展示强大的通用能力？</li>
</ol>
</li>
<li><p>DeepSeek-R1流程分为下面四个阶段</p>
</li>
</ul>
<h4 id="2-3-1-Cold-Start"><a href="#2-3-1-Cold-Start" class="headerlink" title="2.3.1. Cold Start"></a>2.3.1. Cold Start</h4><ul>
<li><p>为了防止从基础模型开始的 RL 训练早期不稳定阶段，对于 DeepSeek-R1，我们构建并收集了少量长 CoT 数据来微调模型作为初始 RL 参与者</p>
</li>
<li><p>收集这些数据的几种方法：使用带有长 CoT 示例的少样本提示，直接提示模型生成带有反思和验证的详细答案，收集 DeepSeek-R1-Zero 的可读格式输出，并通过人工注释员的后处理来优化结果</p>
</li>
<li><p>与 DeepSeek-R1-Zero 相比，冷启动数据的优势包括：</p>
<ol>
<li>可读性：DeepSeek-R1-Zero 的一个关键限制是其内容通常不适合阅读。在为 DeepSeek-R1 创建冷启动数据时，设计了一种可读的模式，包括在每个响应的末尾添加摘要，并过滤掉不适合阅读的响应。将输出格式定义为 |special_token|&lt;reasoning_process&gt;|special_token|&lt;summary&gt;，其中推理过程是查询的 CoT，摘要用于总结推理结果</li>
<li>潜力：与 DeepSeek-R1-Zero 相比，性能有所提升。相信迭代训练是推理模型的更好方式</li>
</ol>
</li>
</ul>
<h4 id="2-3-2-Reasoning-oriented-Reinforcement-Learning-面向推理的强化学习"><a href="#2-3-2-Reasoning-oriented-Reinforcement-Learning-面向推理的强化学习" class="headerlink" title="2.3.2. Reasoning-oriented Reinforcement Learning(面向推理的强化学习)"></a>2.3.2. Reasoning-oriented Reinforcement Learning(面向推理的强化学习)</h4><ul>
<li><p>在对 DeepSeek-V3-Base 进行冷启动数据微调后，应用了与 DeepSeek-R1-Zero 相同的大规模强化学习训练过程</p>
</li>
<li><p>为了缓解语言混合问题，我们在 RL 训练期间引入了语言一致性奖励，该奖励计算为目标语言单词在 CoT 中的比例</p>
</li>
<li><p>这种对齐会导致模型性能略有下降，但这种奖励与人类偏好一致，使其更具可读性</p>
</li>
<li><p>最后将推理任务的准确性和语言一致性奖励直接相加，形成最终奖励。然后在微调后的模型上应用 RL 训练，直到其在推理任务上达到收敛</p>
</li>
</ul>
<h4 id="2-3-3-Rejection-Sampling-and-Supervised-Fine-Tuning"><a href="#2-3-3-Rejection-Sampling-and-Supervised-Fine-Tuning" class="headerlink" title="2.3.3. Rejection Sampling and Supervised Fine-Tuning"></a>2.3.3. Rejection Sampling and Supervised Fine-Tuning</h4><ul>
<li><p>当面向推理的 RL 收敛时，我们利用生成的检查点来收集下一轮的 SFT（监督微调）数据</p>
</li>
<li><p>与主要关注推理的初始冷启动数据不同，这一阶段结合了来自其他领域的数据，以增强模型在写作、角色扮演和其他通用任务中的能力</p>
</li>
</ul>
<h5 id="Reasoning-data"><a href="#Reasoning-data" class="headerlink" title="Reasoning data"></a>Reasoning data</h5><ul>
<li><p>策划推理提示，并通过从上述 RL 训练的检查点执行拒绝采样(Rejection Sampling)来生成推理轨迹</p>
</li>
<li><p>前一阶段仅包含可以使用基于规则的奖励评估的数据。然而在这一阶段，通过结合其他数据扩展了数据集，其中一些数据使用生成奖励模型，通过将真实值和模型预测输入 DeepSeek-V3 进行判断</p>
</li>
<li><p>由于模型输出有时混乱且难以阅读，需要过滤掉混合语言、长段落和代码块的 Chain-of-Thought</p>
</li>
<li><p>对于每个提示，我们采样多个响应并仅保留正确的响应</p>
</li>
</ul>
<h5 id="Non-Reasoning-data"><a href="#Non-Reasoning-data" class="headerlink" title="Non-Reasoning data"></a>Non-Reasoning data</h5><ul>
<li><p>对于非推理数据，如写作、事实问答、自我认知和翻译，采用 DeepSeek-V3 的流程，并重用 DeepSeek-V3 的部分 SFT 数据集</p>
</li>
<li><p>对于某些非推理任务，我们调用 DeepSeek-V3 在回答问题之前生成潜在的 Chain-of-Thought</p>
</li>
<li><p>对于更简单的查询，例如“你好”，我们不会在响应中提供 CoT</p>
</li>
</ul>
<h4 id="2-3-4-Reinforcement-Learning-for-all-Scenarios"><a href="#2-3-4-Reinforcement-Learning-for-all-Scenarios" class="headerlink" title="2.3.4. Reinforcement Learning for all Scenarios"></a>2.3.4. Reinforcement Learning for all Scenarios</h4><ul>
<li><p>为了进一步将模型与人类偏好对齐，我们实施了第二阶段的强化学习，旨在提高模型的有用性和无害性，同时完善其推理能力</p>
</li>
<li><p>使用奖励信号和多样化的提示分布来训练模型：</p>
<ol>
<li>对于推理数据，遵循 DeepSeek-R1-Zero 的方法论，使用基于规则的奖励来指导数学、代码和逻辑推理领域的学习过程</li>
<li>对于一般数据，依靠奖励模型来捕捉复杂和微妙场景中的人类偏好</li>
</ol>
</li>
<li><p>对于有用性，专注于最终摘要，确保评估强调响应对用户的实用性和相关性，同时最小化对底层推理过程的干扰</p>
</li>
<li><p>对于无害性，评估模型的整个响应，包括推理过程和摘要，以识别和减轻生成过程中可能出现的任何潜在风险、偏见或有害内容</p>
</li>
<li><p>最终，奖励信号和多样化数据分布的整合使我们能够训练出一个在推理方面表现出色，同时优先考虑有用性和无害性的模型</p>
</li>
</ul>
<h3 id="2-4-Distillation-Empower-Small-Models-with-Reasoning-Capability"><a href="#2-4-Distillation-Empower-Small-Models-with-Reasoning-Capability" class="headerlink" title="2.4. Distillation: Empower Small Models with Reasoning Capability"></a>2.4. Distillation: Empower Small Models with Reasoning Capability</h3><ul>
<li><p>为了使更高效的小型模型具备像 DeepSeek-R1 一样的推理能力，可以直接使用 DeepSeek-R1 生成的80万条样本对开源模型如 Qwen和 Llama进行微调</p>
</li>
<li><p>这种简单的蒸馏方法显著增强了小型模型的推理能力</p>
</li>
<li><p>对于蒸馏模型，我们仅应用 SFT，不包括 RL 阶段，尽管结合 RL 可以显著提升模型性能。我们的主要目标是展示蒸馏技术的有效性</p>
</li>
</ul>
<hr>
<h2 id="3-Experiment"><a href="#3-Experiment" class="headerlink" title="3. Experiment"></a>3. Experiment</h2><h3 id="Benchmarks"><a href="#Benchmarks" class="headerlink" title="Benchmarks"></a>Benchmarks</h3><h3 id="Evaluation-Prompts"><a href="#Evaluation-Prompts" class="headerlink" title="Evaluation Prompts"></a>Evaluation Prompts</h3><h3 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h3><h3 id="Evaluation-Setup"><a href="#Evaluation-Setup" class="headerlink" title="Evaluation Setup"></a>Evaluation Setup</h3><ul>
<li><p>将模型的最大生成长度设置为 32,768 个tokens</p>
</li>
<li><p>使用贪婪解码评估长输出推理模型会导致更高的重复率和不同检查点之间的显著变异性。因此默认使用 pass@𝑘 评估，并使用非零温度报告 pass@1</p>
</li>
<li><p>使用采样温度为 0.6，top-𝑝 值为 0.95 来生成 𝑘 个响应（通常在 4 到 64 之间，取决于测试集大小），然后计算 pass@1：</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/02/21/t7XEVB9OIPWHmGb.png" alt="image.png"></p>
<ul>
<li>其中 pi 表示第 𝑖 个响应的正确性。这种方法提供了更可靠的性能估计</li>
</ul>
<h3 id="3-1-DeepSeek-R1-Evaluation"><a href="#3-1-DeepSeek-R1-Evaluation" class="headerlink" title="3.1. DeepSeek-R1 Evaluation"></a>3.1. DeepSeek-R1 Evaluation</h3><h3 id="3-2-Distilled-Model-Evaluation"><a href="#3-2-Distilled-Model-Evaluation" class="headerlink" title="3.2. Distilled Model Evaluation"></a>3.2. Distilled Model Evaluation</h3><hr>
<h2 id="4-Discussion"><a href="#4-Discussion" class="headerlink" title="4. Discussion"></a>4. Discussion</h2><h3 id="4-1-Distillation-v-s-Reinforcement-Learning"><a href="#4-1-Distillation-v-s-Reinforcement-Learning" class="headerlink" title="4.1. Distillation v.s. Reinforcement Learning"></a>4.1. Distillation v.s. Reinforcement Learning</h3><ul>
<li><p>通过蒸馏 DeepSeek-R1，小型模型可以取得令人印象深刻的结果。然而，还有一个问题：模型是否可以通过本文讨论的大规模 RL 训练在不进行蒸馏的情况下取得相当的性能？</p>
</li>
<li><p>实验后得出两个结论：</p>
<ol>
<li>将更强大的模型蒸馏到较小的模型中会产生出色的结果，而依赖本文中提到的大规模 RL 的小型模型需要巨大的计算能力，甚至可能无法达到蒸馏的性能</li>
<li>虽然蒸馏策略既经济又有效，但超越智能的边界可能仍然需要更强大的基础模型和更大规模的强化学习</li>
</ol>
</li>
</ul>
<h3 id="4-2-Unsuccessful-Attempts"><a href="#4-2-Unsuccessful-Attempts" class="headerlink" title="4.2. Unsuccessful Attempts"></a>4.2. Unsuccessful Attempts</h3><h4 id="Process-Reward-Model-PRM-过程奖励模型"><a href="#Process-Reward-Model-PRM-过程奖励模型" class="headerlink" title="Process Reward Model (PRM) (过程奖励模型)"></a>Process Reward Model (PRM) (过程奖励模型)</h4><ul>
<li><p>PRM 是一种合理的方法，可以引导模型采用更好的方法来解决推理任务</p>
</li>
<li><p>然而，在实践中，PRM 有三个主要限制，可能会阻碍其最终成功：</p>
<ol>
<li>在一般推理中明确定义细粒度步骤具有挑战性</li>
<li>确定当前中间步骤是否正确是一项艰巨的任务。使用模型进行自动注释可能无法产生令人满意的结果，而手动注释不利于扩展</li>
<li>一旦引入基于模型的 PRM，不可避免地会导致奖励攻击，并且重新训练奖励模型需要额外的训练资源，这会使整个训练流程复杂化</li>
</ol>
</li>
<li><p>虽然 PRM 展示了重新排序模型生成的前 N 个响应或辅助引导搜索的良好能力（Snell et al., 2024），但在我们实验中的大规模强化学习过程中，其优势相对于额外的计算开销是有限的</p>
</li>
</ul>
<h4 id="Monte-Carlo-Tree-Search-MCTS-蒙特卡洛树搜索"><a href="#Monte-Carlo-Tree-Search-MCTS-蒙特卡洛树搜索" class="headerlink" title="Monte Carlo Tree Search (MCTS)(蒙特卡洛树搜索)"></a>Monte Carlo Tree Search (MCTS)(蒙特卡洛树搜索)</h4><ul>
<li><p>受到 AlphaGo（Silver et al.,）和 AlphaZero（Silver et al.,）的启发，我们探索了使用蒙特卡罗树搜索（MCTS）来增强测试时计算的可扩展性</p>
</li>
<li><p>这种方法涉及将答案分解为更小的部分，以允许模型系统地探索解决方案空间</p>
</li>
<li><p>为了促进这一点，我们提示模型生成多个标签，这些标签对应于搜索所需的特定推理步骤</p>
</li>
<li><p>对于训练，首先使用收集的提示通过 MCTS 找到答案，该搜索由预训练的价值模型引导。随后，我们使用生成的问题-答案对来训练演员模型和价值模型( actor model and the value model)，迭代地完善过程</p>
</li>
<li><p>然而，这种方法在扩展训练时遇到了几个挑战:</p>
<ol>
<li>与棋不同，棋的搜索空间相对明确，而标记生成呈现了指数级更大的搜索空间。为了解决这个问题，我们为每个节点设置了最大扩展限制，但这可能导致模型陷入局部最优</li>
<li>价值模型直接影响生成的质量，因为它指导搜索过程的每一步。训练一个细粒度的价值模型本质上是困难的，这使得模型难以迭代改进</li>
</ol>
</li>
<li><p>虽然 MCTS 在与预训练价值模型配对时可以在推理过程中提高性能，但通过自我搜索迭代提升模型性能仍然是一个重大挑战</p>
</li>
</ul>
<hr>
<h2 id="5-Conclusion-Limitations-and-Future-Work"><a href="#5-Conclusion-Limitations-and-Future-Work" class="headerlink" title="5. Conclusion, Limitations, and Future Work"></a>5. Conclusion, Limitations, and Future Work</h2><ul>
<li><p>DeepSeek-R1-Zero 代表了一种不依赖冷启动数据的纯 RL 方法，在各种任务中表现出色</p>
</li>
<li><p>DeepSeek-R1 更强大，结合了冷启动数据和迭代 RL 微调</p>
</li>
<li><p>在未来，我们计划在以下方向为 DeepSeek-R1 进行研究：</p>
<ol>
<li>通用能力：DeepSeek-R1 在函数调用、多轮对话、复杂角色扮演和 JSON 输出等任务中的能力不如 DeepSeek-V3。未来，我们计划探索如何利用长 CoT 来增强这些领域的任务</li>
<li>语言混合：DeepSeek-R1 目前针对中文和英文进行了优化，这可能导致在处理其他语言的查询时出现语言混合问题</li>
<li>提示工程：在评估 DeepSeek-R1 时，我们观察到它对提示敏感。少样本提示通常会降低其性能</li>
<li>软件工程任务：由于评估时间较长，影响了 RL 过程的效率，大规模 RL 尚未广泛应用于软件工程任务。因此，DeepSeek-R1 在软件工程基准测试中并未表现出比 DeepSeek-V3 的巨大改进</li>
</ol>
</li>
</ul>
<hr>
<h1 id="论文关键点总结"><a href="#论文关键点总结" class="headerlink" title="论文关键点总结"></a>论文关键点总结</h1><h2 id="核心目标"><a href="#核心目标" class="headerlink" title="核心目标"></a>核心目标</h2><p>提升大语言模型（LLMs）的推理能力：通过强化学习（RL）和蒸馏技术，激励模型在数学、编程、逻辑等复杂任务中的推理能力，缩小与OpenAI系列模型（如o1-1217）的性能差距。</p>
<h2 id="主要模型"><a href="#主要模型" class="headerlink" title="主要模型"></a>主要模型</h2><ul>
<li>核心方法：纯RL激励推理 → 冷启动+多阶段训练 → 蒸馏小模型</li>
</ul>
<h3 id="DeepSeek-R1-Zero"><a href="#DeepSeek-R1-Zero" class="headerlink" title="DeepSeek-R1-Zero"></a>DeepSeek-R1-Zero</h3><ul>
<li><p>纯强化学习（RL）训练：直接从基础模型（DeepSeek-V3-Base）启动RL，无需监督微调（SFT）</p>
</li>
<li><p>自我进化：通过RL自然涌现出自我验证、反思、生成长推理链（CoT）等能力</p>
</li>
<li><p>局限性：可读性差、语言混合问题</p>
</li>
</ul>
<h3 id="DeepSeek-R1"><a href="#DeepSeek-R1" class="headerlink" title="DeepSeek-R1"></a>DeepSeek-R1</h3><ul>
<li><p>多阶段训练：结合冷启动数据（少量高质量CoT示例）+ 两阶段RL（推理优化与人类偏好对齐）+ 两阶段SFT（推理与非推理能力）</p>
</li>
<li><p>改进点：通过格式约束和语言一致性奖励提升可读性</p>
</li>
</ul>
<h2 id="重要创新理论"><a href="#重要创新理论" class="headerlink" title="重要创新理论"></a>重要创新理论</h2><h3 id="1-纯强化学习驱动推理"><a href="#1-纯强化学习驱动推理" class="headerlink" title="1.纯强化学习驱动推理"></a>1.纯强化学习驱动推理</h3><ul>
<li><p>无需监督数据：首次验证仅通过RL（无SFT）可激励LLMs的推理能力，突破传统依赖大量标注数据的限制</p>
</li>
<li><p>GRPO算法：采用组相对策略优化（Group Relative Policy Optimization），通过组内奖励对比估计基线，省去传统RL中的批评模型，降低计算成本</p>
</li>
</ul>
<h3 id="2-冷启动与多阶段训练"><a href="#2-冷启动与多阶段训练" class="headerlink" title="2.冷启动与多阶段训练"></a>2.冷启动与多阶段训练</h3><ul>
<li><p>冷启动数据：少量长CoT示例微调基础模型，加速RL收敛并提升可读性</p>
</li>
<li><p>混合奖励设计：结合准确性奖励（规则验证）和语言一致性奖励，平衡性能与人类偏好</p>
</li>
</ul>
<h3 id="3-蒸馏小型密集模型"><a href="#3-蒸馏小型密集模型" class="headerlink" title="3.蒸馏小型密集模型"></a>3.蒸馏小型密集模型</h3><ul>
<li><p>知识迁移：从DeepSeek-R1生成80万条推理数据，直接蒸馏到Qwen&#x2F;Llama系列小模型（1.5B~70B）</p>
</li>
<li><p>经济性：蒸馏成本远低于大规模RL训练，但需依赖大模型的先验知识</p>
</li>
</ul>
<h2 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h2><ul>
<li><p>语言混合：中英文优化导致其他语言推理时出现混合问题</p>
</li>
<li><p>提示敏感性：少样本提示可能降低性能，需零样本指令明确输出格式</p>
</li>
<li><p>工程任务不足：软件工程（如SWE-Bench）因RL训练效率低，改进有限</p>
</li>
</ul>
<h2 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h2><ul>
<li><p>通用能力扩展：探索长CoT在函数调用、多轮对话中的应用</p>
</li>
<li><p>多语言优化：解决非中英文查询的语言混合问题</p>
</li>
<li><p>异步RL训练：提升软件工程任务的训练效率</p>
</li>
<li><p>蒸馏结合RL：在蒸馏模型中引入RL进一步优化性能</p>
</li>
</ul>
<hr>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/deepseek/">deepseek</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2025/02/21/D6Cb5LuMrkVQeiz.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/e232842e.html" title="Janus:Decoupling Visual Encoding for Unified Multimodel Understanding and Generation"><img class="cover" src="https://s2.loli.net/2025/02/22/PtwTxgJ8dYWh2IZ.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">Janus:Decoupling Visual Encoding for Unified Multimodel Understanding and Generation</div></div></a></div><div class="next-post pull-right"><a href="/post/e0e02130.html" title="计算机视觉 22 Image Generation"><img class="cover" src="https://s2.loli.net/2024/11/18/zAG9fYxh35LIWK2.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">计算机视觉 22 Image Generation</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2024/07/16/lsEXfWtGT6eRu7k.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">w434</div><div class="author-info__description">An undergraduate majoring in AI at PKU.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">115</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">29</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/w434"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/w434" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcom to my blog.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning"><span class="toc-number">1.</span> <span class="toc-text">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-number">1.2.</span> <span class="toc-text">1. Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Contributions"><span class="toc-number">1.2.1.</span> <span class="toc-text">1.1. Contributions</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Post-Training-%E5%9C%A8%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E4%B8%8A%E8%BF%9B%E8%A1%8C%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">Post-Training:在基础模型上进行大规模强化学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Distillation-%E5%B0%8F%E6%A8%A1%E5%9E%8B%E4%B9%9F%E5%8F%AF%E4%BB%A5%E5%BE%88%E5%BC%BA%E5%A4%A7"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">Distillation:小模型也可以很强大</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Summary-of-Evaluation-Results"><span class="toc-number">1.2.2.</span> <span class="toc-text">1.2. Summary of Evaluation Results</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Reasoning-tasks"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">Reasoning tasks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Knowledge"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">Knowledge</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Others"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">Others</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Approach"><span class="toc-number">1.3.</span> <span class="toc-text">2. Approach</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Overview"><span class="toc-number">1.3.1.</span> <span class="toc-text">2.1. Overview</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-DeepSeek-R1-Zero-Reinforcement-Learning-on-the-Base-Model"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-Reinforcement-Learning-Algorithm"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">2.2.1. Reinforcement Learning Algorithm</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Group-Relative-Policy-Optimization-%E7%BB%84%E7%9B%B8%E5%AF%B9%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96"><span class="toc-number">1.3.2.1.1.</span> <span class="toc-text">Group Relative Policy Optimization(组相对策略优化)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-Reward-Modeling"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">2.2.2. Reward Modeling</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-3-Training-Template"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">2.2.3. Training Template</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-4-Performance-Self-evolution-Process-and-Aha-Moment-of-DeepSeek-R1-Zero"><span class="toc-number">1.3.2.4.</span> <span class="toc-text">2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Performance-of-DeepSeek-R1-Zero"><span class="toc-number">1.3.2.4.1.</span> <span class="toc-text">Performance of DeepSeek-R1-Zero</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Self-evolution-Process-of-DeepSeek-R1-Zero"><span class="toc-number">1.3.2.4.2.</span> <span class="toc-text">Self-evolution Process of DeepSeek-R1-Zero</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Aha-Moment-%E9%A1%BF%E6%82%9F%E6%97%B6%E5%88%BB-of-DeepSeek-R1-Zero"><span class="toc-number">1.3.2.5.</span> <span class="toc-text">Aha Moment(顿悟时刻) of DeepSeek-R1-Zero</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Drawback-of-DeepSeek-R1-Zero"><span class="toc-number">1.3.2.6.</span> <span class="toc-text">Drawback of DeepSeek-R1-Zero</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-DeepSeek-R1-Reinforcement-Learning-with-Cold-Start"><span class="toc-number">1.3.3.</span> <span class="toc-text">2.3. DeepSeek-R1: Reinforcement Learning with Cold Start</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-1-Cold-Start"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">2.3.1. Cold Start</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-2-Reasoning-oriented-Reinforcement-Learning-%E9%9D%A2%E5%90%91%E6%8E%A8%E7%90%86%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">2.3.2. Reasoning-oriented Reinforcement Learning(面向推理的强化学习)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-3-Rejection-Sampling-and-Supervised-Fine-Tuning"><span class="toc-number">1.3.3.3.</span> <span class="toc-text">2.3.3. Rejection Sampling and Supervised Fine-Tuning</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Reasoning-data"><span class="toc-number">1.3.3.3.1.</span> <span class="toc-text">Reasoning data</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Non-Reasoning-data"><span class="toc-number">1.3.3.3.2.</span> <span class="toc-text">Non-Reasoning data</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-4-Reinforcement-Learning-for-all-Scenarios"><span class="toc-number">1.3.3.4.</span> <span class="toc-text">2.3.4. Reinforcement Learning for all Scenarios</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Distillation-Empower-Small-Models-with-Reasoning-Capability"><span class="toc-number">1.3.4.</span> <span class="toc-text">2.4. Distillation: Empower Small Models with Reasoning Capability</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Experiment"><span class="toc-number">1.4.</span> <span class="toc-text">3. Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Benchmarks"><span class="toc-number">1.4.1.</span> <span class="toc-text">Benchmarks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Evaluation-Prompts"><span class="toc-number">1.4.2.</span> <span class="toc-text">Evaluation Prompts</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Baselines"><span class="toc-number">1.4.3.</span> <span class="toc-text">Baselines</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Evaluation-Setup"><span class="toc-number">1.4.4.</span> <span class="toc-text">Evaluation Setup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-DeepSeek-R1-Evaluation"><span class="toc-number">1.4.5.</span> <span class="toc-text">3.1. DeepSeek-R1 Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Distilled-Model-Evaluation"><span class="toc-number">1.4.6.</span> <span class="toc-text">3.2. Distilled Model Evaluation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Discussion"><span class="toc-number">1.5.</span> <span class="toc-text">4. Discussion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Distillation-v-s-Reinforcement-Learning"><span class="toc-number">1.5.1.</span> <span class="toc-text">4.1. Distillation v.s. Reinforcement Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Unsuccessful-Attempts"><span class="toc-number">1.5.2.</span> <span class="toc-text">4.2. Unsuccessful Attempts</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Process-Reward-Model-PRM-%E8%BF%87%E7%A8%8B%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.5.2.1.</span> <span class="toc-text">Process Reward Model (PRM) (过程奖励模型)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Monte-Carlo-Tree-Search-MCTS-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2"><span class="toc-number">1.5.2.2.</span> <span class="toc-text">Monte Carlo Tree Search (MCTS)(蒙特卡洛树搜索)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Conclusion-Limitations-and-Future-Work"><span class="toc-number">1.6.</span> <span class="toc-text">5. Conclusion, Limitations, and Future Work</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E5%85%B3%E9%94%AE%E7%82%B9%E6%80%BB%E7%BB%93"><span class="toc-number">2.</span> <span class="toc-text">论文关键点总结</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E7%9B%AE%E6%A0%87"><span class="toc-number">2.1.</span> <span class="toc-text">核心目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.2.</span> <span class="toc-text">主要模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#DeepSeek-R1-Zero"><span class="toc-number">2.2.1.</span> <span class="toc-text">DeepSeek-R1-Zero</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DeepSeek-R1"><span class="toc-number">2.2.2.</span> <span class="toc-text">DeepSeek-R1</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E5%88%9B%E6%96%B0%E7%90%86%E8%AE%BA"><span class="toc-number">2.3.</span> <span class="toc-text">重要创新理论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%BA%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%A9%B1%E5%8A%A8%E6%8E%A8%E7%90%86"><span class="toc-number">2.3.1.</span> <span class="toc-text">1.纯强化学习驱动推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%86%B7%E5%90%AF%E5%8A%A8%E4%B8%8E%E5%A4%9A%E9%98%B6%E6%AE%B5%E8%AE%AD%E7%BB%83"><span class="toc-number">2.3.2.</span> <span class="toc-text">2.冷启动与多阶段训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%92%B8%E9%A6%8F%E5%B0%8F%E5%9E%8B%E5%AF%86%E9%9B%86%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.3.3.</span> <span class="toc-text">3.蒸馏小型密集模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">2.4.</span> <span class="toc-text">局限性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91"><span class="toc-number">2.5.</span> <span class="toc-text">未来方向</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/b0093abe.html" title="生成模型基础 02 Autoencoders"><img src="https://s2.loli.net/2025/09/16/g4dekXTQ6smLh3v.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="生成模型基础 02 Autoencoders"/></a><div class="content"><a class="title" href="/post/b0093abe.html" title="生成模型基础 02 Autoencoders">生成模型基础 02 Autoencoders</a><time datetime="2025-09-16T02:30:00.000Z" title="Created 2025-09-16 10:30:00">2025-09-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/57eee2fd.html" title="代数结构与组合数学 期末考试复习"><img src="https://s2.loli.net/2025/09/05/uOdZECJDbL74UzB.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="代数结构与组合数学 期末考试复习"/></a><div class="content"><a class="title" href="/post/57eee2fd.html" title="代数结构与组合数学 期末考试复习">代数结构与组合数学 期末考试复习</a><time datetime="2025-09-05T10:29:00.000Z" title="Created 2025-09-05 18:29:00">2025-09-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/9c268db1.html" title="金融学概论 期末考试复习"><img src="https://s2.loli.net/2025/09/05/R5qIuhsKygTxNUV.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="金融学概论 期末考试复习"/></a><div class="content"><a class="title" href="/post/9c268db1.html" title="金融学概论 期末考试复习">金融学概论 期末考试复习</a><time datetime="2025-09-05T10:28:00.000Z" title="Created 2025-09-05 18:28:00">2025-09-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/3939330f.html" title="自然语言处理 期中考试复习"><img src="https://s2.loli.net/2025/03/18/WoNFLe1mGTDzSsb.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="自然语言处理 期中考试复习"/></a><div class="content"><a class="title" href="/post/3939330f.html" title="自然语言处理 期中考试复习">自然语言处理 期中考试复习</a><time datetime="2025-09-02T16:53:00.000Z" title="Created 2025-09-03 00:53:00">2025-09-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/588ac659.html" title="多智能体基础 深度强化学习部分考试复习"><img src="https://s2.loli.net/2025/09/02/UQIPZmnylTAHOCk.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="多智能体基础 深度强化学习部分考试复习"/></a><div class="content"><a class="title" href="/post/588ac659.html" title="多智能体基础 深度强化学习部分考试复习">多智能体基础 深度强化学习部分考试复习</a><time datetime="2025-09-02T13:41:00.000Z" title="Created 2025-09-02 21:41:00">2025-09-02</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By w434</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcom to my blog.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">简</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>