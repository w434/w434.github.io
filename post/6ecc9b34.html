<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大模型基础与对齐 03 DeepSeek &amp; LLaMA | w434's blog</title><meta name="author" content="w434"><meta name="copyright" content="w434"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="DeepSeek DetailsThe DeepSeek-V3 Architecture The Mixture of Expert(MoE) 将transformer中的FNN编程多个小的FNN  决定走哪个小的FNN取决于专家选择，将专家合在一起  总的计算量不变     专家数量增加，颗粒度越精细，同时设置shared专家    负载均衡的路由策略：为解决MoE中专家负载失衡可能导致的“路由">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型基础与对齐 03 DeepSeek &amp; LLaMA">
<meta property="og:url" content="http://example.com/post/6ecc9b34.html">
<meta property="og:site_name" content="w434&#39;s blog">
<meta property="og:description" content="DeepSeek DetailsThe DeepSeek-V3 Architecture The Mixture of Expert(MoE) 将transformer中的FNN编程多个小的FNN  决定走哪个小的FNN取决于专家选择，将专家合在一起  总的计算量不变     专家数量增加，颗粒度越精细，同时设置shared专家    负载均衡的路由策略：为解决MoE中专家负载失衡可能导致的“路由">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2025/02/22/Od6DiNyG73wULja.jpg">
<meta property="article:published_time" content="2025-03-12T04:42:00.000Z">
<meta property="article:modified_time" content="2025-03-12T04:44:42.504Z">
<meta property="article:author" content="w434">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2025/02/22/Od6DiNyG73wULja.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/post/6ecc9b34.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大模型基础与对齐 03 DeepSeek & LLaMA',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-12 12:44:42'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2024/07/16/lsEXfWtGT6eRu7k.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">125</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">30</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/notes/"><i class="fa-fw fa-solid fa-school"></i><span> 课程笔记</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-face-smile"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/matches/"><i class="fa-fw fa-solid fa-futbol"></i><span> 足球赛事</span></a></li><li><a class="site-page child" href="/chess/"><i class="fa-fw fa-solid fa-chess"></i><span> 国际象棋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw fa-solid fa-film"></i><span> 影评&amp;演出评论&amp;书评</span></a></li><li><a class="site-page child" href="/jottings/"><i class="fa-fw fa-solid fa-book"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/varietyShow/"><i class="fa-fw fa-solid fa-tv"></i><span> 综艺</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/musics/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2025/02/22/Od6DiNyG73wULja.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="w434's blog"><span class="site-name">w434's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/notes/"><i class="fa-fw fa-solid fa-school"></i><span> 课程笔记</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-face-smile"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/matches/"><i class="fa-fw fa-solid fa-futbol"></i><span> 足球赛事</span></a></li><li><a class="site-page child" href="/chess/"><i class="fa-fw fa-solid fa-chess"></i><span> 国际象棋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw fa-solid fa-film"></i><span> 影评&amp;演出评论&amp;书评</span></a></li><li><a class="site-page child" href="/jottings/"><i class="fa-fw fa-solid fa-book"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/varietyShow/"><i class="fa-fw fa-solid fa-tv"></i><span> 综艺</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/musics/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大模型基础与对齐 03 DeepSeek &amp; LLaMA</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-03-12T04:42:00.000Z" title="Created 2025-03-12 12:42:00">2025-03-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-03-12T04:44:42.504Z" title="Updated 2025-03-12 12:44:42">2025-03-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%AF%B9%E9%BD%90/">大模型基础与对齐</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="大模型基础与对齐 03 DeepSeek &amp; LLaMA"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="DeepSeek-Details"><a href="#DeepSeek-Details" class="headerlink" title="DeepSeek Details"></a>DeepSeek Details</h1><h2 id="The-DeepSeek-V3-Architecture"><a href="#The-DeepSeek-V3-Architecture" class="headerlink" title="The DeepSeek-V3 Architecture"></a>The DeepSeek-V3 Architecture</h2><p><img src="https://s2.loli.net/2025/03/11/hmSrM4zBN3x2pEj.png" alt="image.png"></p>
<h3 id="The-Mixture-of-Expert-MoE"><a href="#The-Mixture-of-Expert-MoE" class="headerlink" title="The Mixture of Expert(MoE)"></a>The Mixture of Expert(MoE)</h3><ul>
<li><p>将transformer中的FNN编程多个小的FNN</p>
</li>
<li><p>决定走哪个小的FNN取决于专家选择，将专家合在一起</p>
</li>
<li><p>总的计算量不变</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/11/1f8lZjaQePiRUGs.png" alt="image.png"></p>
<ul>
<li>专家数量增加，颗粒度越精细，同时设置shared专家</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/11/RwYKIZbT7rWBd36.png" alt="image.png"></p>
<ul>
<li><p>负载均衡的路由策略：为解决MoE中专家负载失衡可能导致的“路由塌陷”问题（即部分专家过载而模型性能下降），DeepSeek-V3提出了无辅助损失的负载均衡策略。传统MoE（如 GShard、Switch Transformer）需要在损失函数中加入均衡项来惩罚不均衡，但过强的辅助损失会损害模型性能</p>
</li>
<li><p>DeepSeek-V3引入动态偏置：为每个专家的得分增加一个可调偏置，仅用于路由决策，不影响实际输出权重。训练中每步监控各专家的使用量，若某专家过载则降低其偏置，反之增加偏置，以动态平衡专家流量</p>
</li>
<li><p>Expert choice: 让expert选token而不是token选expert，传统的token选择expert方法在路由过程中无法控制每个专家接收的token数量，容易导致负载不均衡</p>
</li>
<li><p>偏执项不直接参与作用到梯度计算中,避免了权衡load balance在loss中的权重</p>
</li>
</ul>
<h3 id="The-Multi-head-Latent-Attention"><a href="#The-Multi-head-Latent-Attention" class="headerlink" title="The Multi-head Latent Attention"></a>The Multi-head Latent Attention</h3><ul>
<li>低秩key-value压缩：MLA的核心思想是压缩key-value成一个更紧凑的表示——latent vactor，存储的时候只用存latent vector</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/11/KHGk1Uqnc4VOeN6.png" alt="image.png"></p>
<ul>
<li>为了更进一步的节约显存，也可以对query进行压缩：</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/11/tSwhirsn2BPEmvz.png" alt="image.png"></p>
<ul>
<li><p>降低显存可以进一步增加窗口长度，提高了对长窗口上下文的支撑能力</p>
</li>
<li><p>DeepSeek-V3能够支持超长上下文（其上下文长度经两阶段扩展到32K乃至128K）而不会因为缓存开销爆炸</p>
</li>
</ul>
<h1 id="The-Llama-Architecture"><a href="#The-Llama-Architecture" class="headerlink" title="The Llama Architecture"></a>The Llama Architecture</h1><ul>
<li>New Architecture: <ol>
<li>Architectural differences between the vanilla Transformer and LLaMA</li>
<li>RMS Normalization (with review of Layer Normalization)</li>
<li>Rotary Positional Embeddings</li>
<li>KV-Cache</li>
<li>Multi-Query Attention</li>
<li>Grouped Multi-Query Attention</li>
<li>SwiGLUActivation Function</li>
</ol>
</li>
</ul>
<h2 id="The-Llama-Architecture-1"><a href="#The-Llama-Architecture-1" class="headerlink" title="The Llama Architecture"></a>The Llama Architecture</h2><p><img src="https://s2.loli.net/2025/03/11/iodLAuRlawv4yUe.png" alt="image.png"></p>
<h2 id="The-Layer-Normalization"><a href="#The-Layer-Normalization" class="headerlink" title="The Layer Normalization"></a>The Layer Normalization</h2><ul>
<li><p>Batch norm在NLP不适用，因为句子会变长，没有办法每个batch中句子的长度</p>
</li>
<li><p>LayerNorm的问题：计算均值非常消耗时间，从而引入RMSNorm，但是计算准确度会降低一些</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/11/RfC5X6bZgWLFSAu.png" alt="image.png"></p>
<ul>
<li>前置layer norm的原因：</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/11/S7tL4smNbFAGjyW.png" alt="image.png"></p>
<h2 id="The-Position-Embedding"><a href="#The-Position-Embedding" class="headerlink" title="The Position Embedding"></a>The Position Embedding</h2><ul>
<li><p>在 NLP 任务中，输入通常是序列化的数据（如句子），单词的顺序是非常重要的</p>
</li>
<li><p>虽然 Transformer 能够通过自注意力机制捕捉输入序列中各个元素之间的关系，但它本身对输入的顺序信息并不敏感</p>
</li>
<li><p>因此需要一种方式将顺序信息传递给模型，这就是Positional Embedding的作用</p>
</li>
<li><p>为什么不能用01编码：</p>
<ol>
<li>不高效</li>
<li>不能表达距离越远越不重要，距离越近的上下文影响越大</li>
</ol>
</li>
</ul>
<h3 id="固定位置编码"><a href="#固定位置编码" class="headerlink" title="固定位置编码"></a>固定位置编码</h3><p><img src="https://s2.loli.net/2025/03/11/aZQh2jYnbycVOTp.png" alt="image.png"></p>
<ul>
<li><p>sin&#x2F;cos编码的优点：</p>
<ol>
<li>位置编码具有平滑性</li>
<li>具有相对位置信息</li>
<li>具有周期性可以无限延伸，具有可扩展性</li>
</ol>
</li>
<li><p>position embedding和embedding的size一定相同，直接相加形成embedding input</p>
</li>
</ul>
<h3 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h3><ul>
<li><p>绝对位置编码：固定的向量，添加到词元的嵌入中，用于表示该词元在句子中的绝对位置。因此，它一次只处理一个词元。e.g.猫在桌子上，猫是第一个单词</p>
</li>
<li><p>相对位置编码：一次处理两个词元，并在我们计算注意力时起作用，由于注意力机制捕捉的是两个词之间的“关联强度”，相对位置编码告诉注意力机制这两个词之间的距离。因此，给定两个词元，我们会创建一个表示它们距离的向量。e.g.不关注猫出现在第几个单词，而是猫出现在前面还是后面，距离强度</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/11/qdxny9gpmlWBQsY.png" alt="image.png"></p>
<h3 id="The-ROPE-Position-Embedding"><a href="#The-ROPE-Position-Embedding" class="headerlink" title="The ROPE Position Embedding"></a>The ROPE Position Embedding</h3><ul>
<li><p>注意力机制中使用的点积是一种内积，可以看作是点积的一种广义形式</p>
</li>
<li><p>能否找到一种内积，作用于注意力机制中使用的两个向量 q（查询）和 k（键），使其只依赖于这两个向量和它们所代表的词元之间的相对距离</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/11/god6BZea4ykfb8t.png" alt="image.png"></p>
<ul>
<li><p>写成矩阵形式，在二维相当于乘了一个旋转矩阵</p>
</li>
<li><p>ROPE只作用在Q和K上，将相对位置的编码加入到QK点乘计算中</p>
</li>
<li><p>n维通项公式：</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/11/cReErPBG7KTXOAM.png" alt="image.png"></p>
<p><img src="https://s2.loli.net/2025/03/11/BuF3WiXxeC9PLYZ.png" alt="image.png"></p>
<ul>
<li><p>ROPE实现思路：</p>
<ol>
<li>计算旋转因子theta:<br>  <img src="https://s2.loli.net/2025/03/11/PxymkuMnWswev9o.png" alt="image.png"></li>
<li>计算序列索引：<br>  <img src="https://s2.loli.net/2025/03/11/pc4nUCPyT8gBMxO.png" alt="image.png"></li>
<li>计算旋转角度索引：<br>  <img src="https://s2.loli.net/2025/03/11/BG3EmwSjP65U9Yo.png" alt="image.png"></li>
<li>缓存sin和cos位置编码：<br>  <img src="https://s2.loli.net/2025/03/11/xSvqn9t4dY72UAB.png" alt="image.png"></li>
<li>应用ROPE:<br>  <img src="https://s2.loli.net/2025/03/11/tlHdLb9NOc2frEB.png" alt="image.png"></li>
</ol>
</li>
<li><p>应用ROPE后由于sin&#x2F;cos的特性，距离越近的单词强度越大，影响越大，而距离远的单词的影响小</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/11/HmYZM5q9EfKg4uA.png" alt="image.png"></p>
<ul>
<li><p>通过预训练或者SFT阶段进行长文本的拓展</p>
</li>
<li><p>长文本中的新位置是原位置的position&#x2F;scale</p>
</li>
<li><p>只需要少量的成文本微调即可</p>
</li>
</ul>
<h3 id="拉长窗口"><a href="#拉长窗口" class="headerlink" title="拉长窗口"></a>拉长窗口</h3><p><img src="https://s2.loli.net/2025/03/11/saDuhyoe4M3K8fB.png" alt="image.png"></p>
<p><img src="https://s2.loli.net/2025/03/11/6qITPfAubkyoQHr.png" alt="image.png"></p>
<ul>
<li>窗口过短：agent应用的时候无法处理强推理的任务</li>
</ul>
<h2 id="KV-Cache"><a href="#KV-Cache" class="headerlink" title="KV Cache"></a>KV Cache</h2><ul>
<li><p>思路：每次新来一个Q，此前的K和V已经算过了，因此可以将其存储下来，此前的K和V本身不会改变</p>
</li>
<li><p>每次计算完以后，k,v矩阵只会加一行或一列</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/11/G1bif8Dezv5NAkV.png" alt="image.png"></p>
<ul>
<li><p>可以提高计算速度，是“空间换时间”，损失更多显存，空间占用率非常高</p>
</li>
<li><p>也正因如此，问完问题后，第一个单词生成时间很慢，而后续单词的计算时间大幅提高</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/11/6bqcKPeif1wgohx.png" alt="image.png"></p>
<ul>
<li>KV Cache处理不当会导致三个层面现存浪费：<ol>
<li>Reservation 涉及为页面或块预留空间以加速存储管理，某片空间不能使用</li>
<li>Internal fragmentation 是由于固定大小的页面未被完全利用造成的内存浪费，固定大小的内存块还没用完就去用其他块</li>
<li>External fragmentation 是由于分散的空闲内存块无法形成连续空间，导致无法为较大请求分配足够的内存</li>
</ol>
</li>
</ul>
<h3 id="Paged-Attention-with-KV-Cache"><a href="#Paged-Attention-with-KV-Cache" class="headerlink" title="Paged Attention with KV Cache"></a>Paged Attention with KV Cache</h3><ul>
<li><p>PagedAttention 将数据的逻辑组织与其物理存储分离。属于同一序列的逻辑块通过一个块表映射到可能不连续的物理块上</p>
</li>
<li><p>划分 KV 缓存：KV 缓存被划分为固定大小的块或“页面”。每个块包含来自原始缓存的部分键值对</p>
</li>
<li><p>构建查找表：构建并维护一个查找表（block table），将查询键映射到存储相应值的特定页面。该表将每个可能的查询键映射到存储对应值的特定页面，从而实现快速分配和检索</p>
</li>
<li><p>选择性加载：在推理过程中，模型只加载处理当前输入序列所需的页面。与加载整个 KV 缓存相比，这减少了总体内存占用</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/11/pqYi9lVbMt6C7JE.png" alt="image.png"></p>
<p><img src="https://s2.loli.net/2025/03/11/ualcrwCHfOe54nS.png" alt="image.png"></p>
<h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><ul>
<li><p>KV cache和Page attention都是用于提高模型推理效率的技术，尤其在自注意力机制</p>
</li>
<li><p>KV cache是指在处理序列时，将计算出的键（Key）和值（Value）存储在缓存中，以避免在后续的步骤中重复计算</p>
</li>
<li><p>在自回归生成模型中，如语言模型，使用KV cache可以加速推理过程。当模型处理新的输入时，可以直接使用缓存中的键和值，从而节省计算资源和时间</p>
</li>
<li><p>Page attention是一种特定类型的注意力机制，旨在处理更长序列或更复杂的上下文</p>
</li>
<li><p>它通过将输入序列划分为多个“页面”或段落来管理内存使用，并且通常在处理长序列时，能够有效地利用KV cache来减少计算复杂性</p>
</li>
<li><p>相辅相成：KV cache可以在Page attention中被利用，以提高对长序列的处理效率。Page attention可以将长序列分段处理，同时使用KV cache来避免重复计算，从而加速整体推理过程</p>
</li>
</ul>
<h2 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h2><p><img src="https://s2.loli.net/2025/03/11/iUDlFSsOaJn1g3x.png" alt="image.png"></p>
<ul>
<li>问题：<ol>
<li>计算复杂度高</li>
<li>数据传输多，通讯效率低</li>
<li>存储墙问题<br>  <img src="https://s2.loli.net/2025/03/11/O3Sn1RszTW4ADQu.png" alt="image.png"><br>  <img src="https://s2.loli.net/2025/03/11/6OedfthuAoTz4cw.png" alt="image.png"></li>
</ol>
</li>
</ul>
<h3 id="Multi-head-Attention-with-KV-Cache"><a href="#Multi-head-Attention-with-KV-Cache" class="headerlink" title="Multi-head Attention with KV Cache"></a>Multi-head Attention with KV Cache</h3><p><img src="https://s2.loli.net/2025/03/11/CuQiV6djtbnO3Ex.png" alt="image.png"></p>
<ul>
<li><p>因此大部分的时间都浪费在搬运算力而非计算本身</p>
</li>
<li><p>需要将Multi-head Attention和KV Cache有机统一：去掉h维度，但是不降低性能</p>
</li>
</ul>
<h3 id="The-Multi-query-Attention"><a href="#The-Multi-query-Attention" class="headerlink" title="The Multi-query Attention"></a>The Multi-query Attention</h3><p><img src="https://s2.loli.net/2025/03/11/CHjfnIrQosqLkSu.png" alt="image.png"></p>
<ul>
<li>将k和v复制多份，并没有降低性能，因为仍是多头注意力机制</li>
</ul>
<h2 id="The-Group-query-Attention"><a href="#The-Group-query-Attention" class="headerlink" title="The Group-query Attention"></a>The Group-query Attention</h2><p><img src="https://s2.loli.net/2025/03/11/MYymqkcEiCxho3D.png" alt="image.png"></p>
<h2 id="SwiGLU-Activation-Function"><a href="#SwiGLU-Activation-Function" class="headerlink" title="SwiGLU Activation Function"></a>SwiGLU Activation Function</h2><ul>
<li>SwiGLU非单调，在0处连续，精度比ReLU要高</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/12/ubMJ9NvEK5rdLIQ.png" alt="image.png"></p>
<h2 id="Emergent-Capability"><a href="#Emergent-Capability" class="headerlink" title="Emergent Capability"></a>Emergent Capability</h2><ul>
<li><p>涌现是指系统中的量变导致行为的质变</p>
</li>
<li><p>性能几乎是随机的，直到达到某个临界阈值，然后大幅提高。可能的解释：emergent abilities</p>
</li>
<li><p>涌现能力只存在于大模型中，在小模型中不存在</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/12/gbi46c3NR5FAWaJ.png" alt="image.png"></p>
<ul>
<li><p>锐度：涌现能力瞬间从不存在过渡到涌现</p>
</li>
<li><p>不可预见：转变出现涌现能力的规模是不可预见的</p>
</li>
<li><p>其他的解释：</p>
<ol>
<li>尽管模型族的 per-token 错误率会随着模型规模的增加进行平滑、持续且可预测地变化，但看似尖锐和不可预测的变化可能是由研究者选择的测量方法引起的</li>
<li>涌现能力的成因并非是随规模增长而导致的模型行为的本质变化，而是对非连续度量的使用</li>
<li>大模型的“涌现能力（Emergent Abilities）”并不是由模型规模本身导致的，而是由研究人员选择的评估指标（metrics）决定的。如果换用更平滑的指标（如Token Edit Distance），能力变化会显得更加平稳。Token Edit Distance 计算的是输出与正确答案的编辑距离，数值是连续变化的</li>
</ol>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/12/9SZKEuF2rQOWlxa.png" alt="image.png"></p>
<h2 id="The-grokking-effect"><a href="#The-grokking-effect" class="headerlink" title="The grokking effect"></a>The grokking effect</h2><ul>
<li><p>泛化似乎在拟合训练数据后很长一段时间内突然发生，称为 grokking</p>
</li>
<li><p>权重衰减在提高我们研究的任务的泛化方面特别有效</p>
</li>
<li><p>随着数据集大小的减小，泛化所需的优化量会迅速增加</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/12/uFXLZJYvSMI6Tbz.png" alt="image.png"></p>
<h3 id="The-double-descent"><a href="#The-double-descent" class="headerlink" title="The double descent"></a>The double descent</h3><ul>
<li><p>Double Descent 的现象表明，在模型容量继续增加时，误差曲线会再次下降，形成第⼆次下降</p>
</li>
<li><p>这⼀现象打破了传统的偏差-⽅差权衡理论，在模型容量⼤到⼀定程度后，增加模型的复杂性不仅不会导致过拟合，反⽽可以改善泛化性能</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/12/2LiCweoSquzMK9U.png" alt="image.png"></p>
<ul>
<li><p>模型泛化的提升：Grokking 和 Double Descent 都涉及模型在训练过程中的泛化能力如何随时间或模型容量变化。在 Grokking 中，模型通过长时间的训练获得了突然的泛化提升，而在 Double Descent 中，模型通过增加复杂性或容量来达到更好的泛化。这两个现象都表明了模型在某些条件下能够打破传统的泛化理解</p>
</li>
<li><p>异常的训练动态：Grokking 和 Double Descent 都展现了深度学习训练中的非线性、非单调的动态表现。在 Grokking 中，验证集上的误差可能在长时间内没有改善，直到突然变好。而在 Double Descent 中，误差会先上升，然后再次下降</p>
</li>
</ul>
<hr>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2025/02/22/Od6DiNyG73wULja.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/fdd3721e.html" title="角色动画与运动仿真 04 Keyframe Animation"><img class="cover" src="https://s2.loli.net/2025/03/12/otfm56i8uqDXbKQ.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">角色动画与运动仿真 04 Keyframe Animation</div></div></a></div><div class="next-post pull-right"><a href="/post/e080db9.html" title="LLaMA3 自学笔记"><img class="cover" src="https://s2.loli.net/2025/03/07/u2SEqVOMvexjFKJ.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">LLaMA3 自学笔记</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/post/6ccd62c9.html" title="大模型基础与对齐 01 LLM概述"><img class="cover" src="https://s2.loli.net/2025/02/22/Od6DiNyG73wULja.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-22</div><div class="title">大模型基础与对齐 01 LLM概述</div></div></a></div><div><a href="/post/139c00ec.html" title="大模型基础与对齐 02 Transformer &amp; GPT-3"><img class="cover" src="https://s2.loli.net/2025/02/22/Od6DiNyG73wULja.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-05</div><div class="title">大模型基础与对齐 02 Transformer &amp; GPT-3</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2024/07/16/lsEXfWtGT6eRu7k.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">w434</div><div class="author-info__description">An undergraduate majoring in AI at PKU.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">125</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">30</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/w434"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/w434" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcom to my blog.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#DeepSeek-Details"><span class="toc-number">1.</span> <span class="toc-text">DeepSeek Details</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-DeepSeek-V3-Architecture"><span class="toc-number">1.1.</span> <span class="toc-text">The DeepSeek-V3 Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Mixture-of-Expert-MoE"><span class="toc-number">1.1.1.</span> <span class="toc-text">The Mixture of Expert(MoE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Multi-head-Latent-Attention"><span class="toc-number">1.1.2.</span> <span class="toc-text">The Multi-head Latent Attention</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#The-Llama-Architecture"><span class="toc-number">2.</span> <span class="toc-text">The Llama Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Llama-Architecture-1"><span class="toc-number">2.1.</span> <span class="toc-text">The Llama Architecture</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Layer-Normalization"><span class="toc-number">2.2.</span> <span class="toc-text">The Layer Normalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Position-Embedding"><span class="toc-number">2.3.</span> <span class="toc-text">The Position Embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BA%E5%AE%9A%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">2.3.1.</span> <span class="toc-text">固定位置编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">2.3.2.</span> <span class="toc-text">相对位置编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-ROPE-Position-Embedding"><span class="toc-number">2.3.3.</span> <span class="toc-text">The ROPE Position Embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8B%89%E9%95%BF%E7%AA%97%E5%8F%A3"><span class="toc-number">2.3.4.</span> <span class="toc-text">拉长窗口</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KV-Cache"><span class="toc-number">2.4.</span> <span class="toc-text">KV Cache</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Paged-Attention-with-KV-Cache"><span class="toc-number">2.4.1.</span> <span class="toc-text">Paged Attention with KV Cache</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary"><span class="toc-number">2.4.2.</span> <span class="toc-text">summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-head-Attention"><span class="toc-number">2.5.</span> <span class="toc-text">Multi-head Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-head-Attention-with-KV-Cache"><span class="toc-number">2.5.1.</span> <span class="toc-text">Multi-head Attention with KV Cache</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Multi-query-Attention"><span class="toc-number">2.5.2.</span> <span class="toc-text">The Multi-query Attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Group-query-Attention"><span class="toc-number">2.6.</span> <span class="toc-text">The Group-query Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SwiGLU-Activation-Function"><span class="toc-number">2.7.</span> <span class="toc-text">SwiGLU Activation Function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Emergent-Capability"><span class="toc-number">2.8.</span> <span class="toc-text">Emergent Capability</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-grokking-effect"><span class="toc-number">2.9.</span> <span class="toc-text">The grokking effect</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-double-descent"><span class="toc-number">2.9.1.</span> <span class="toc-text">The double descent</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/7a89f995.html" title="The Principles of Diffusion Models中文翻译版"><img src="https://s2.loli.net/2025/11/08/RUCW5uwpO3yJXka.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="The Principles of Diffusion Models中文翻译版"/></a><div class="content"><a class="title" href="/post/7a89f995.html" title="The Principles of Diffusion Models中文翻译版">The Principles of Diffusion Models中文翻译版</a><time datetime="2025-11-08T10:12:00.000Z" title="Created 2025-11-08 18:12:00">2025-11-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/c7f70a00.html" title="生成模型基础 06 GAN"><img src="https://s2.loli.net/2025/09/16/g4dekXTQ6smLh3v.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="生成模型基础 06 GAN"/></a><div class="content"><a class="title" href="/post/c7f70a00.html" title="生成模型基础 06 GAN">生成模型基础 06 GAN</a><time datetime="2025-10-19T09:45:00.000Z" title="Created 2025-10-19 17:45:00">2025-10-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/18cfc8b1.html" title="生成模型基础 05 Debugging"><img src="https://s2.loli.net/2025/09/16/g4dekXTQ6smLh3v.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="生成模型基础 05 Debugging"/></a><div class="content"><a class="title" href="/post/18cfc8b1.html" title="生成模型基础 05 Debugging">生成模型基础 05 Debugging</a><time datetime="2025-10-18T12:25:00.000Z" title="Created 2025-10-18 20:25:00">2025-10-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/68bc60d9.html" title="生成模型基础 04 Autoregressive models"><img src="https://s2.loli.net/2025/09/16/g4dekXTQ6smLh3v.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="生成模型基础 04 Autoregressive models"/></a><div class="content"><a class="title" href="/post/68bc60d9.html" title="生成模型基础 04 Autoregressive models">生成模型基础 04 Autoregressive models</a><time datetime="2025-10-14T05:54:00.000Z" title="Created 2025-10-14 13:54:00">2025-10-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/232f050.html" title="生成模型基础 03 Transformer Model"><img src="https://s2.loli.net/2025/09/16/g4dekXTQ6smLh3v.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="生成模型基础 03 Transformer Model"/></a><div class="content"><a class="title" href="/post/232f050.html" title="生成模型基础 03 Transformer Model">生成模型基础 03 Transformer Model</a><time datetime="2025-09-29T14:10:00.000Z" title="Created 2025-09-29 22:10:00">2025-09-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By w434</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcom to my blog.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">简</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>