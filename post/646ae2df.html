<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>MetaMorph:Multimodal Understanding and Generation via Instruction Tuning | w434's blog</title><meta name="author" content="w434"><meta name="copyright" content="w434"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="MetaMorph:Multimodal Understanding and Generation via Instruction Tuning(通过指令微调实现多模态理解和生成)Abstract 在这项工作中，我们提出了视觉预测指令微调（Visual-Predictive Instruction Tuning, VPiT）——这是对视觉指令微调的一个简单而有效的扩展，它使得预训练的大语言模型（L">
<meta property="og:type" content="article">
<meta property="og:title" content="MetaMorph:Multimodal Understanding and Generation via Instruction Tuning">
<meta property="og:url" content="http://example.com/post/646ae2df.html">
<meta property="og:site_name" content="w434&#39;s blog">
<meta property="og:description" content="MetaMorph:Multimodal Understanding and Generation via Instruction Tuning(通过指令微调实现多模态理解和生成)Abstract 在这项工作中，我们提出了视觉预测指令微调（Visual-Predictive Instruction Tuning, VPiT）——这是对视觉指令微调的一个简单而有效的扩展，它使得预训练的大语言模型（L">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2025/03/24/b8F6sUEPxJuMcQn.jpg">
<meta property="article:published_time" content="2025-03-24T00:05:00.000Z">
<meta property="article:modified_time" content="2025-03-24T02:10:07.719Z">
<meta property="article:author" content="w434">
<meta property="article:tag" content="MetaMorph">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2025/03/24/b8F6sUEPxJuMcQn.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/post/646ae2df.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'MetaMorph:Multimodal Understanding and Generation via Instruction Tuning',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-24 10:10:07'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2024/07/16/lsEXfWtGT6eRu7k.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">123</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">45</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">30</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/notes/"><i class="fa-fw fa-solid fa-school"></i><span> 课程笔记</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-face-smile"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/matches/"><i class="fa-fw fa-solid fa-futbol"></i><span> 足球赛事</span></a></li><li><a class="site-page child" href="/chess/"><i class="fa-fw fa-solid fa-chess"></i><span> 国际象棋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw fa-solid fa-film"></i><span> 影评&amp;演出评论&amp;书评</span></a></li><li><a class="site-page child" href="/jottings/"><i class="fa-fw fa-solid fa-book"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/varietyShow/"><i class="fa-fw fa-solid fa-tv"></i><span> 综艺</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/musics/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2025/03/24/b8F6sUEPxJuMcQn.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="w434's blog"><span class="site-name">w434's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/notes/"><i class="fa-fw fa-solid fa-school"></i><span> 课程笔记</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-face-smile"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/matches/"><i class="fa-fw fa-solid fa-futbol"></i><span> 足球赛事</span></a></li><li><a class="site-page child" href="/chess/"><i class="fa-fw fa-solid fa-chess"></i><span> 国际象棋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw fa-solid fa-film"></i><span> 影评&amp;演出评论&amp;书评</span></a></li><li><a class="site-page child" href="/jottings/"><i class="fa-fw fa-solid fa-book"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/varietyShow/"><i class="fa-fw fa-solid fa-tv"></i><span> 综艺</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/musics/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">MetaMorph:Multimodal Understanding and Generation via Instruction Tuning</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-03-24T00:05:00.000Z" title="Created 2025-03-24 08:05:00">2025-03-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-03-24T02:10:07.719Z" title="Updated 2025-03-24 10:10:07">2025-03-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="MetaMorph:Multimodal Understanding and Generation via Instruction Tuning"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="MetaMorph-Multimodal-Understanding-and-Generation-via-Instruction-Tuning-通过指令微调实现多模态理解和生成"><a href="#MetaMorph-Multimodal-Understanding-and-Generation-via-Instruction-Tuning-通过指令微调实现多模态理解和生成" class="headerlink" title="MetaMorph:Multimodal Understanding and Generation via Instruction Tuning(通过指令微调实现多模态理解和生成)"></a>MetaMorph:Multimodal Understanding and Generation via Instruction Tuning(通过指令微调实现多模态理解和生成)</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><p>在这项工作中，我们提出了视觉预测指令微调（Visual-Predictive Instruction Tuning, VPiT）——这是对视觉指令微调的一个简单而有效的扩展，它使得预训练的大语言模型（LLM）能够快速转变为一个统一的自回归模型，能够生成文本和视觉token</p>
</li>
<li><p>VPiT 教会LLM从任何以指令跟随格式整理的图像和文本数据输入序列中预测离散的文本token和连续的视觉token</p>
</li>
<li><p>实证研究揭示了VPiT的几个有趣特性：</p>
<ol>
<li>视觉生成能力作为视觉理解能力提升的自然副产品出现，并且可以通过少量的生成数据高效解锁</li>
<li>虽然我们发现理解和生成是相互促进的，但理解数据对这两种能力的贡献比生成数据更有效</li>
</ol>
</li>
<li><p>基于这些发现，我们训练了MetaMorph模型，并在视觉理解和生成方面取得了有竞争力的表现</p>
</li>
<li><p>在视觉生成方面，MetaMorph能够利用从LLM预训练中获得的世界知识和推理能力，并克服其他生成模型常见的失败模式</p>
</li>
<li><p>我们的结果表明，LLM可能具有强大的“先验”视觉能力，这些能力可以通过相对简单的指令微调过程高效地适应视觉理解和生成任务</p>
</li>
</ul>
<hr>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ul>
<li><p>多模态大语言模型（Multimodal Large Language Models, MLLMs）在视觉理解方面取得了显著进展，从基本的图像描述发展到复杂的视觉推理。这些模型处理多模态输入——主要是图像和语言——并生成文本token</p>
</li>
<li><p>多模态LLM通常利用预训练的视觉编码器、预训练的语言模型，并通过MLP或交叉注意力模块等连接器对齐这些模态</p>
</li>
<li><p>在MLLM的训练方法中，视觉指令微调已被广泛使用。它将预训练视觉编码器的输出嵌入视为连续的“视觉token”，并直接将其作为输入传递给预训练的LLM</p>
</li>
<li><p>视觉指令微调的一个好处是它在数据和计算上是高效的。通过使用数百万个图像-文本问答对进行指令微调，预训练的LLM可以用适度的计算和数据重新用作多模态LLM</p>
</li>
<li><p>视觉指令微调的有效性表明，LLM已经具备了相当多的内在视觉知识，这使得它们能够在指令微调过程中高效地学习和发展视觉理解能力</p>
</li>
<li><p>受此启发，我们研究了LLM是否也可以通过微调来生成视觉信息，并且具有相当的效率和效果</p>
</li>
<li><p>当前的“统一”模型——即能够同时进行多模态理解和生成的模型——通常将视觉生成视为与视觉理解正交的能力</p>
</li>
<li><p>往往需要对原始MLLM架构进行重大修改，并进行大量的多模态预训练和&#x2F;或微调</p>
</li>
<li><p>设计这样的方法具有挑战性，过去的研究采取了不同的方法，包括将视觉输入token化为离散token、结合扩散目标，以及将视觉解耦为单独的理解和生成模式</p>
</li>
<li><p>例如，像LWM、Show-o和Chameleon这样的方法需要数十亿的图像-文本对进行广泛的预训练和微调</p>
</li>
<li><p>在这项工作中，我们提出了视觉预测指令微调（VPiT）——这是对视觉指令微调的一个简单扩展，它建立在将连续视觉token作为输入传递给LLM的现有范式之上</p>
</li>
<li><p>VPiT在微调阶段训练LLM输出连续视觉token和离散文本token。模型接受预训练视觉编码器的嵌入以及文本token作为输入，并输出文本token和连续视觉token的组合</p>
</li>
<li><p>为了可视化生成的视觉token，我们微调了一个扩散模型，将嵌入映射回像素空间，如下图所示</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/24/qcVO9B6TKnZLS7X.png" alt="image.png"></p>
<ul>
<li><p>这个框架使我们能够研究视觉理解、视觉生成和预训练LLM之间的协同作用，从而得出以下几个有趣的发现：</p>
<ol>
<li>预测视觉token的能力从理解视觉输入中自然涌现，并且只需要最少的额外训练。与视觉指令微调类似，VPiT高效且有效地将LLM转变为一个“统一”模型，能够理解和生成多模态token</li>
<li>理解和生成视觉token的能力是内在关联且不对称的。增加理解数据可以提高视觉理解（通过更高的VQA分数衡量）和生成性能（通过更低的FID分数衡量）。相反，增加生成数据可以提高生成质量，同时也有助于增强视觉理解——但程度较小。训练每种能力对模型整体视觉性能的影响的不对称性：以理解为中心的训练在提高视觉理解和生成方面显著优于以生成为中心的训练</li>
</ol>
</li>
<li><p>基于这些发现，我们训练了一个名为MetaMorph的统一模型，使用VPiT预测多模态token</p>
</li>
<li><p>利用从常见的视觉问答数据集到没有文本注释的纯图像和视频数据的多样化数据源</p>
</li>
<li><p>MetaMorph在视觉理解和视觉生成基准测试中均取得了有竞争力的表现</p>
</li>
<li><p>此外，我们展示了这种统一建模方法使模型能够利用LLM的强大能力。例如，MetaMorph在生成视觉token时可以从预训练的LLM中提取知识</p>
</li>
<li><p>更令人惊讶的是，我们观察到MetaMorph可以在生成视觉token之前隐式执行推理步骤——例如，当提示为“由帝王蝶毛毛虫变态而成的动物”时，MetaMorph成功生成了蝴蝶的图像(上图右)</p>
</li>
<li><p>我们的结果表明：</p>
<ol>
<li>通过指令微调训练统一模型是可行的</li>
<li>LLM具有强大的先验视觉能力，这些能力可以通过显著少于广泛预训练的样本数量来激活</li>
</ol>
</li>
<li><p>这些见解为混合模态模型的开发提供了启示</p>
</li>
<li><p>推进基础LLM、指令微调技术和数据来改进多模态LLM中的视觉理解也可能隐式地导致模型在视觉生成方面表现更好</p>
</li>
</ul>
<hr>
<h2 id="2-Visual-Predictive-Instruction-Tuning-视觉预测指令微调"><a href="#2-Visual-Predictive-Instruction-Tuning-视觉预测指令微调" class="headerlink" title="2 Visual-Predictive Instruction Tuning(视觉预测指令微调)"></a>2 Visual-Predictive Instruction Tuning(视觉预测指令微调)</h2><ul>
<li><p>LLaVA引入的视觉指令微调表明，LLM可以通过数百万级数据的微调来理解视觉输入</p>
</li>
<li><p>后期融合指令微调的成功表明，LLM可能已经具备了内在的视觉理解能力，只需通过轻量级微调即可解锁</p>
</li>
<li><p>类似地，我们假设LLM已经具备了一定程度的内在视觉生成能力，只需通过轻量级微调即可解锁</p>
</li>
<li><p>受此启发，我们提出了视觉预测指令微调（VPiT，下图）——这是一个简单的设计，扩展了现有的指令微调方法，使其能够生成视觉token而不仅仅是文本</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/24/qcVO9B6TKnZLS7X.png" alt="image.png"></p>
<ul>
<li><p>使用相同的架构和下一个token预测范式来解锁视觉生成能力，无需复杂的修改</p>
</li>
<li><p>采用预训练的LLM并对其进行微调，以预测离散文本token和连续视觉token。视觉token可以通过适配的扩散模型进行可视化</p>
</li>
</ul>
<h3 id="2-1-From-Unimodal-to-Multimodal-Next-Token-Prediction"><a href="#2-1-From-Unimodal-to-Multimodal-Next-Token-Prediction" class="headerlink" title="2.1 From Unimodal to Multimodal Next-Token Prediction"></a>2.1 From Unimodal to Multimodal Next-Token Prediction</h3><ul>
<li><p>标准的指令微调设置由对话轮次的输入序列组成：(Pi,Ri)(i&#x3D;1~N)，其中Pi和Ri分别表示第i轮对话的提示和响应。模型被训练为根据提示生成响应</p>
</li>
<li><p>VPiT在标准指令微调设置中添加了以下机制，以解锁视觉理解和生成能力</p>
</li>
</ul>
<h4 id="Tokenizing-multimodal-data"><a href="#Tokenizing-multimodal-data" class="headerlink" title="Tokenizing multimodal data"></a>Tokenizing multimodal data</h4><ul>
<li><p>我们将Pi和Ri扩展为包含文本和图像</p>
</li>
<li><p>为了将视觉数据集成到预训练的LLM中，我们紧密遵循视觉指令微调处理数据：</p>
<ol>
<li>文本数据：文本通过LLM使用的标准tokenizer进行token化，生成离散token</li>
<li>视觉数据：图像通过预训练的视觉编码器（如SigLIP）进行编码。输出是连续的视觉token，然后将其插值为m&#x3D;64个token。为了将视觉token作为输入传递给LLM，我们应用了一个可训练的投影层，以将其维度与LLM对齐</li>
</ol>
</li>
</ul>
<h4 id="Model-architecture"><a href="#Model-architecture" class="headerlink" title="Model architecture"></a>Model architecture</h4><ul>
<li><p>采用预训练的LLM并对其进行微调，以处理任意序列的文本和视觉token（详见第2.2节）</p>
</li>
<li><p>保留原始的LLM头部用于文本预测，并附加一个单独的视觉头部到LLM，用于预测视觉token，即视觉编码器处理图像时生成的输出token</p>
</li>
<li><p>视觉头部是一个投影层，将LLM的维度投影到视觉编码器的维度</p>
</li>
<li><p>然后，所有响应token都可以自回归地训练和预测，提示token作为上下文</p>
</li>
<li><p>与传统的视觉指令微调不同，在VPiT中，视觉token也是LLM的输出——而不仅仅是输入</p>
</li>
<li><p>为了使LLM意识到视觉token的存在，我们引入了特殊token ⟨image_start⟩和 ⟨image_end⟩ 来指示视觉token序列的边界以及何时使用视觉头部</p>
</li>
</ul>
<h4 id="Loss-functions"><a href="#Loss-functions" class="headerlink" title="Loss functions"></a>Loss functions</h4><ul>
<li><p>语言头部输出词汇表上的概率分布，并通过交叉熵损失进行下一个token预测的训练</p>
</li>
<li><p>视觉预测使用LLM预测的视觉token与视觉编码器生成的视觉token之间的余弦相似度损失</p>
</li>
<li><p>与指令微调实践一致，模型仅在响应token上进行预测并产生损失</p>
</li>
</ul>
<h3 id="2-2-Using-Broad-Types-of-Data"><a href="#2-2-Using-Broad-Types-of-Data" class="headerlink" title="2.2 Using Broad Types of Data"></a>2.2 Using Broad Types of Data</h3><ul>
<li><p>由于VPiT使模型能够在其响应中预测文本和视觉token，因此它允许使用更广泛的训练数据</p>
</li>
<li><p>传统的视觉指令微调主要依赖于问答对</p>
</li>
<li><p>我们的大多数数据集是公开可用的，我们将其分为以下三大类：</p>
<ol>
<li>视觉理解数据：这类数据以图像或视频作为输入，并输出文本响应<ol>
<li>ImageQA：Cambrian-7M（Tong et al.）。模型根据输入图像回答问题。Pi∈{视觉token⟩,(文本提示)}，Ri∈{(文本响应)}</li>
<li>VideoQA：VideoStar和ShareVideo。模型根据输入视频回答问题。对于VideoQA中的视频，我们以1 FPS处理帧。Pi∈{视觉token⟩,⋯,(视觉token), (文本提示)}，Ri∈{(文本响应)}</li>
</ol>
</li>
<li>视觉生成数据：MetaCLIP。模型根据图像描述预测视觉token。我们最多使用500万对数据。将数据整理为问答格式。Pi∈{(文本提示)}，Ri∈{(文本响应), (视觉token)} 通过指令提示模型生成视觉token，如“生成一个…的图像”。文本响应为“这是根据您请求生成的图像…”</li>
<li>其他视觉数据：这类数据要求模型在给定交错的输入视觉token和文本token的情况下预测视觉token<ol>
<li>视频数据：SomethingSomethingV2和HowTo100M。模型按顺序预测帧。我们设计了不同的问答对来探究视频，例如询问未来帧、过去帧和重新排序帧。Pi∈{视觉token,⋯, (视觉token), (文本提示)}，Ri∈{视觉token,⋯, (视觉token)}</li>
<li>视觉思维数据：Visualization-of-Thought和VStar。模型在解决问题之前预测其响应中的多模态token。例如，它在生成文本响应之前预测图像的放大视图。Pi∈{视觉token, (文本提示)}，Ri∈{文本响应, (视觉token), (文本响应)} 在响应中，模型将输出“我将从视觉上思考”，然后输出表示图像放大片段的视觉token，接着回答问题</li>
<li>图像到图像数据：InstructPix2Pix和Aurora。模型根据文本描述和输入图像生成转换后的图像。Pi∈{视觉token, (文本提示)}，Ri∈{视觉token}</li>
</ol>
</li>
</ol>
</li>
</ul>
<h3 id="2-3-Mapping-Tokens-to-Images-through-Diffusion-通过扩散模型将token映射到图像"><a href="#2-3-Mapping-Tokens-to-Images-through-Diffusion-通过扩散模型将token映射到图像" class="headerlink" title="2.3 Mapping Tokens to Images through Diffusion(通过扩散模型将token映射到图像)"></a>2.3 Mapping Tokens to Images through Diffusion(通过扩散模型将token映射到图像)</h3><ul>
<li><p>由于使用VPiT训练的模型学会预测连续视觉token，我们需要将预测的token映射回像素空间</p>
</li>
<li><p>利用了“扩散自编码器”的概念，其中扩散模型可以适应于以图像嵌入而不是文本嵌入为条件</p>
</li>
<li><p>具体来说，我们微调了一个现有的扩散模型，使其以视觉编码器的输出为条件，使用保留的训练数据</p>
</li>
<li><p>在推理时，如果生成了标签token⟨image_start⟩，模型开始输出视觉token，直到⟨image_end⟩</p>
</li>
<li><p>然后，我们将生成的视觉token插入扩散模型，以在像素空间中可视化预测</p>
</li>
</ul>
<hr>
<h2 id="3-Findings-on-Unlocking-Visual-Generation"><a href="#3-Findings-on-Unlocking-Visual-Generation" class="headerlink" title="3 Findings on Unlocking Visual Generation"></a>3 Findings on Unlocking Visual Generation</h2><ul>
<li>我们研究了以下关于视觉理解和生成效果及其协同作用的问题，基于我们的VPiT框架：<ol>
<li>SS3.1 视觉生成是否可以通过轻量级微调解锁，还是需要大量数据？</li>
<li>SS3.2 视觉理解和生成是相互促进的还是正交的？</li>
<li>SS3.3 更多的视觉理解或生成数据对理解和生成质量的贡献有多大？</li>
<li>SS3.4 哪些视觉理解任务与生成性能最相关？</li>
</ol>
</li>
</ul>
<h3 id="3-1-Visual-Generation-Can-Be-Unlocked-Efficiently-by-Joint-Training-with-Visual-Understanding-视觉生成可以通过与视觉理解联合训练高效解锁"><a href="#3-1-Visual-Generation-Can-Be-Unlocked-Efficiently-by-Joint-Training-with-Visual-Understanding-视觉生成可以通过与视觉理解联合训练高效解锁" class="headerlink" title="3.1 Visual Generation Can Be Unlocked Efficiently by Joint Training with Visual Understanding(视觉生成可以通过与视觉理解联合训练高效解锁)"></a>3.1 Visual Generation Can Be Unlocked Efficiently by Joint Training with Visual Understanding(视觉生成可以通过与视觉理解联合训练高效解锁)</h3><ul>
<li><p>我们首先研究了教会语言模型生成高质量视觉token所需的图像-文本样本数量</p>
</li>
<li><p>为此，我们从生成数据中随机抽取{1k, 5k, 10k, 50k, 200k, 1M, 3M, 5M}图像-文本对。我们探索了两种设置：</p>
<ol>
<li>仅使用视觉生成数据微调LLM</li>
<li>将视觉生成与视觉理解和其他数据类型（如第2.2节所述）联合训练</li>
</ol>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/24/gUmKXE8Tvne92h4.png" alt="image.png"></p>
<ul>
<li><p>如上图左所示，仅使用视觉生成数据进行训练的效果显著低于与所有其他数据联合训练的效果</p>
</li>
<li><p>即使使用超过300万图像-文本对，模型仍然难以生成高质量的视觉图像（FID分数约为40），并且性能仍然低于使用500万对数据进行联合训练的效果</p>
</li>
<li><p>这表明仅使用视觉生成数据进行训练显著降低了样本效率</p>
</li>
<li><p>该研究也表明，当仅使用生成数据进行训练时，LLM不容易被微调以生成视觉token</p>
</li>
<li><p>相比之下，与其他数据集联合训练显著提高了生成性能</p>
</li>
<li><p>这表明视觉生成并不是一种正交的能力，而是一种受益于其他任务并在联合训练中更有效涌现的能力</p>
</li>
<li><p>为了更好地理解每种数据类型对视觉生成的贡献，我们使用200k视觉生成数据进行了对照实验，分别与第2.2节中定义的每种数据类型进行联合训练</p>
</li>
<li><p>如上图右所示，虽然所有数据类型都增强了模型的视觉生成能力，但改进的程度各不相同：视觉理解数据，如ImageQA和VideoQA，显著提升了模型的视觉生成能力</p>
</li>
<li><p>这表明理解视觉内容的能力与生成视觉token的能力之间存在强烈的联系。此外，将所有数据类型结合在训练中进一步提高了性能，这表明不同数据类型的益处可以叠加</p>
</li>
<li><p>发现1：当模型与视觉理解数据联合训练时，视觉生成能力可以通过显著较少的生成数据解锁，相比之下，仅使用生成数据进行训练则需要更多的数据</p>
</li>
</ul>
<h3 id="3-2-Visual-Understanding-and-Generation-are-Mutually-Beneficial-视觉理解和生成是相互促进的"><a href="#3-2-Visual-Understanding-and-Generation-are-Mutually-Beneficial-视觉理解和生成是相互促进的" class="headerlink" title="3.2 Visual Understanding and Generation are Mutually Beneficial(视觉理解和生成是相互促进的)"></a>3.2 Visual Understanding and Generation are Mutually Beneficial(视觉理解和生成是相互促进的)</h3><h4 id="More-understanding-data-leads-to-better-understanding-and-generation"><a href="#More-understanding-data-leads-to-better-understanding-and-generation" class="headerlink" title="More understanding data leads to better understanding and generation"></a>More understanding data leads to better understanding and generation</h4><ul>
<li><p>基于上一小节的发现，我们进行了对照实验，研究视觉理解能力与视觉生成能力之间的相关性</p>
</li>
<li><p>如下图，结果表明，更强的VQA能力与更好的生成性能相关</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/24/oRqBun6lgYGWwkz.png" alt="image.png"></p>
<h4 id="More-generation-data-leads-to-better-understanding-and-generation"><a href="#More-generation-data-leads-to-better-understanding-and-generation" class="headerlink" title="More generation data leads to better understanding and generation"></a>More generation data leads to better understanding and generation</h4><ul>
<li><p>我们研究了相反的方向：增强模型的视觉生成能力是否也与更高的VQA性能相关？</p>
</li>
<li><p>为了探索这一点，我们使用1M固定的VQA样本作为理解基线，然后通过改变生成数据的数量（{200k, 500k, 1M, 2M, 3M, 4M}）来调整生成能力，同时与固定的1M VQA数据进行联合训练</p>
</li>
<li><p>结果如下图所示，在1M VQA设置中，更强的生成能力与改进的VQA性能相关。这意味着增加生成数据的数量不仅增强了生成能力，还对VQA性能产生了积极影响</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/24/NFAJ5Liv3hu9nrl.png" alt="image.png"></p>
<h4 id="This-synergy-scales-across-different-LLMs-这种协同作用在不同LLM之间具有扩展性"><a href="#This-synergy-scales-across-different-LLMs-这种协同作用在不同LLM之间具有扩展性" class="headerlink" title="This synergy scales across different LLMs(这种协同作用在不同LLM之间具有扩展性)"></a>This synergy scales across different LLMs(这种协同作用在不同LLM之间具有扩展性)</h4><ul>
<li>下图展示了不同LLM之间的扩展行为</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/24/2Q51KtRSWAa8lIP.png" alt="image.png"></p>
<ul>
<li>发现2：视觉理解和生成是协同的。增加任何一种能力的数据都会同时增强这两种能力</li>
</ul>
<h3 id="3-3-Understanding-Data-Contributes-More"><a href="#3-3-Understanding-Data-Contributes-More" class="headerlink" title="3.3 Understanding Data Contributes More"></a>3.3 Understanding Data Contributes More</h3><ul>
<li><p>我们研究了理解和生成数据是否贡献相等。在这里，我们联合训练不同规模的VQA数据{1M, 4M, 7M}和生成数据{200k, 500k, 1M, 2M, 3M, 4M}</p>
</li>
<li><p>下图总结了这些发现，x轴表示VQA数据，y轴表示生成数据。结果通过热图可视化，颜色越深表示性能越好</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/24/Ijiw1gKCTMnfet6.png" alt="image.png"></p>
<ul>
<li><p>结果表明，增加VQA数据在所有三个指标上都带来了最显著的改进</p>
</li>
<li><p>当VQA数据相对较低（1M）时，增加生成数据会带来明显的改进，如图中颜色逐渐变深所示</p>
</li>
<li><p>然而，随着VQA数据的增加（从1M到4M再到7M），VQA数据的影响变得更加显著，表现为热图中颜色的急剧变化</p>
</li>
<li><p>最终，在7M VQA数据的情况下，增加生成数据的贡献微乎其微</p>
</li>
<li><p>这些结果证明了理解数据在增强理解和生成性能方面的关键作用</p>
</li>
<li><p>发现3：虽然增加数据总体上提高了性能，但视觉理解数据的影响显著大于视觉生成数据的影响</p>
</li>
</ul>
<h3 id="3-4-Certain-Understanding-Tasks-Correlate-More-with-Generation-Performance"><a href="#3-4-Certain-Understanding-Tasks-Correlate-More-with-Generation-Performance" class="headerlink" title="3.4 Certain Understanding Tasks Correlate More with Generation Performance"></a>3.4 Certain Understanding Tasks Correlate More with Generation Performance</h3><ul>
<li><p>鉴于理解任务的多样性，如OCR、视觉中心任务和基于知识的任务，我们研究了哪些任务与生成能力最密切相关</p>
</li>
<li><p>受Cambrian-1的启发，我们将VQA任务分为五类：通用、文本和图表、高分辨率、知识和视觉中心VQA</p>
</li>
<li><p>使用我们早期实验的结果，这些实验联合训练了不同规模的VQA数据和不同数量的生成数据，我们在下图中绘制了每个基准的VQA性能与生成性能的关系</p>
</li>
<li><p>我们还计算了VQA分数与FID&#x2F;CLIP分数之间的Pearson相关性（ρ）</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/24/qew8WujJ5Vh9fXs.png" alt="image.png"></p>
<ul>
<li><p>通用、视觉中心和文本与图表VQA任务与生成性能密切相关，每个任务的Pearson相关系数（ρ）均高于0.85</p>
</li>
<li><p>高分辨率VQA表现出中等相关性，ρ约为0.7</p>
</li>
<li><p>相比之下，知识VQA任务（如MMMU）与生成性能的相关性较弱</p>
</li>
<li><p>这些发现表明，生成能力与模型的视觉能力更密切相关，而不是与知识特定任务相关</p>
</li>
<li><p>发现4：通用、视觉中心和文本理解的VQA任务与视觉生成表现出强相关性，而基于知识的VQA任务则没有这种相关性</p>
</li>
</ul>
<hr>
<h2 id="4-MetaMorph-Model"><a href="#4-MetaMorph-Model" class="headerlink" title="4 MetaMorph Model"></a>4 MetaMorph Model</h2><ul>
<li><p>基于第3节的见解，我们训练了我们的统一模型MetaMorph，基于LLaMA-3.1 8B，使用VPiT和第2.2节中整理的数据</p>
</li>
<li><p>我们在三个部分中展示了实验结果：定量性能（第4.1节）、MetaMorph在视觉生成中利用LLM知识的证据（第4.2节）以及多模态上下文中的隐式推理技能（第4.3节）</p>
</li>
</ul>
<h3 id="4-1-Competitive-Performance-in-Understanding-and-Generation"><a href="#4-1-Competitive-Performance-in-Understanding-and-Generation" class="headerlink" title="4.1 Competitive Performance in Understanding and Generation"></a>4.1 Competitive Performance in Understanding and Generation</h3><ul>
<li>将MetaMorph与其他统一模型进行了比较，结果如下图所示</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/24/6mPyzRDZnIVSUbj.png" alt="image.png"></p>
<ul>
<li><p>MetaMorph展示了竞争性的表现，并在大多数基准测试中优于其他统一模型——即使之前的模型可能使用了更多的数据进行训练</p>
</li>
<li><p>MetaMorph利用了最新预训练LLM的优势，并在理解和生成方面取得了竞争性的表现。MetaMorph强调了统一模型可以从预训练的LLM中有效开发</p>
</li>
</ul>
<h3 id="4-2-MetaMorph-can-Leverage-LLM-Knowledge-for-Visual-Generation-MetaMorph可以利用LLM知识进行视觉生成"><a href="#4-2-MetaMorph-can-Leverage-LLM-Knowledge-for-Visual-Generation-MetaMorph可以利用LLM知识进行视觉生成" class="headerlink" title="4.2 MetaMorph can Leverage LLM Knowledge for Visual Generation(MetaMorph可以利用LLM知识进行视觉生成)"></a>4.2 MetaMorph can Leverage LLM Knowledge for Visual Generation(MetaMorph可以利用LLM知识进行视觉生成)</h3><ul>
<li>MetaMorph有效地利用了预训练LLM中嵌入的世界知识，如下图所示</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/24/rCgSd8xcPoN32vt.png" alt="image.png"></p>
<ul>
<li><p>我们提示模型生成需要非平凡和专业知识的概念。示例包括“Chhogori”（世界第二高峰）、“Oncilla”（南美洲的一种小型野猫）和“Chizarira”（津巴布韦的一个孤立荒野地区）</p>
</li>
<li><p>MetaMorph成功地将领域特定知识转化为准确的视觉token，从而展示了利用LLM中的世界知识的能力</p>
</li>
<li><p>MetaMorph比CLIP和T5等文本嵌入模型更有效地处理常见的语义挑战。这些挑战包括否定和主观性，使用提示中包含常见失败模式的提示</p>
</li>
</ul>
<h3 id="4-3-Reasoning-in-Multimodal-Generation"><a href="#4-3-Reasoning-in-Multimodal-Generation" class="headerlink" title="4.3 Reasoning in Multimodal Generation"></a>4.3 Reasoning in Multimodal Generation</h3><ul>
<li>下图展示了模型在响应谜题提示时生成图像的示例，例如“黄石国家公园所在国家的国旗”。对于每个谜题，我们直接使用提示“生成{谜题}的图像”，而没有在提示中调用任何思维链（CoT）</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/24/lthoDXCW1NGiVZe.png" alt="image.png"></p>
<ul>
<li><p>当回答“一种乐器，这种乐器通常由提出狭义相对论的科学家演奏”时，模型需要隐式完成三个推理步骤：它识别出提出狭义相对论的科学家是阿尔伯特·爱因斯坦，认识到他偏爱的乐器是小提琴，然后直接生成正确的视觉token——小提琴——而无需在生成过程中显式分离这些步骤</p>
</li>
<li><p>这一结果表明，MetaMorph隐式解决了谜题，并在提示后立即生成了正确的视觉token</p>
</li>
<li><p>这些结果与LLM的物理学中的发现一致，作者认为LLM在自回归生成后续token之前预先计算了推理图。我们展示了这种能力即使在解码视觉token时也能转移到统一的多模态模型设置中</p>
</li>
</ul>
<hr>
<h2 id="5-Related-Work"><a href="#5-Related-Work" class="headerlink" title="5 Related Work"></a>5 Related Work</h2><h3 id="Instruction-tuning-and-visual-instruction-tuning"><a href="#Instruction-tuning-and-visual-instruction-tuning" class="headerlink" title="Instruction tuning and visual instruction tuning"></a>Instruction tuning and visual instruction tuning</h3><ul>
<li><p>指令微调通过微调预训练的LLM来学习交互的格式和风格。这个过程帮助模型有效地传达在预训练中获得的知识和能力</p>
</li>
<li><p>LLaVA将指令微调扩展到多模态领域</p>
</li>
</ul>
<h3 id="From-Multimodal-LLMs-to-unified-models"><a href="#From-Multimodal-LLMs-to-unified-models" class="headerlink" title="From Multimodal LLMs to unified models"></a>From Multimodal LLMs to unified models</h3><ul>
<li><p>最近构建统一模型的努力主要依赖于广泛的预训练或对数十亿级数据集进行大量微调</p>
</li>
<li><p>一些研究还使用连续嵌入来预测视觉token，整合视觉回归损失或利用基于扩散的方法</p>
</li>
<li><p>其他方法将多模态数据token化为离散token，然后使用自回归transformer进行训练</p>
</li>
<li><p>最近的研究还探索了结合自回归和扩散目标的混合策略</p>
</li>
<li><p>与之前的研究不同，我们展示了统一模型可以在指令微调期间在低数据量下有效训练，同时提供了关于视觉理解和视觉生成之间相互关系的见解</p>
</li>
</ul>
<hr>
<h2 id="6-Discussion"><a href="#6-Discussion" class="headerlink" title="6 Discussion"></a>6 Discussion</h2><ul>
<li><p>在这项工作中，我们提出了VPiT——这是对视觉指令微调的一个简单而有效的扩展——它使LLM能够预测多模态token</p>
</li>
<li><p>VPiT解锁了比视觉问答更广泛的指令微调数据的使用，例如文本到图像和纯图像和视频数据</p>
</li>
<li><p>通过对照实验，我们发现视觉生成能力作为视觉理解能力提升的自然副产品出现，并且只需要少量的额外生成数据</p>
</li>
<li><p>此外，我们发现虽然视觉理解和生成是相互促进的，但增加更多的视觉理解数据在提高整体性能方面比增加生成数据更为显著</p>
</li>
<li><p>基于这些见解，我们通过使用VPiT微调LLaMA-3.1 8B来训练MetaMorph。通过简单的训练过程，MetaMorph在视觉理解和生成方面都取得了竞争性的表现</p>
</li>
<li><p>MetaMorph可以在视觉生成过程中利用基础LLM的世界知识和推理能力</p>
</li>
<li><p>总体而言，LLM可能具有与统一和多功能模型相似的表示空间。我们希望这项工作的见解能够激发更多关于开发LLM以实现通用智能的探索</p>
</li>
</ul>
<hr>
<h1 id="DeepSeek-要点总结"><a href="#DeepSeek-要点总结" class="headerlink" title="DeepSeek 要点总结"></a>DeepSeek 要点总结</h1><h2 id="论文提纲与核心要点总结"><a href="#论文提纲与核心要点总结" class="headerlink" title="论文提纲与核心要点总结"></a>论文提纲与核心要点总结</h2><ul>
<li>标题: MetaMorph: 通过指令微调实现多模态理解和生成</li>
<li>作者: Shengbang Tong 等（Meta、纽约大学团队）</li>
</ul>
<h2 id="1-研究背景与问题"><a href="#1-研究背景与问题" class="headerlink" title="1. 研究背景与问题"></a>1. 研究背景与问题</h2><h3 id="多模态LLM现状："><a href="#多模态LLM现状：" class="headerlink" title="多模态LLM现状："></a>多模态LLM现状：</h3><ul>
<li>现有统一模型（理解和生成）需数十亿级数据预训练，计算成本高昂。</li>
</ul>
<h3 id="核心问题："><a href="#核心问题：" class="headerlink" title="核心问题："></a>核心问题：</h3><ul>
<li><p>能否通过轻量级指令微调解锁LLM的潜在视觉生成能力？</p>
</li>
<li><p>视觉理解与生成是否相互促进？</p>
</li>
</ul>
<h2 id="2-核心方法：视觉预测指令微调（VPiT）"><a href="#2-核心方法：视觉预测指令微调（VPiT）" class="headerlink" title="2. 核心方法：视觉预测指令微调（VPiT）"></a>2. 核心方法：视觉预测指令微调（VPiT）</h2><h3 id="方法设计："><a href="#方法设计：" class="headerlink" title="方法设计："></a>方法设计：</h3><ul>
<li>统一自回归框架：将文本和视觉token共同作为输入输出，保留LLM原有架构，仅添加轻量级视觉头部。</li>
</ul>
<h3 id="数据格式："><a href="#数据格式：" class="headerlink" title="数据格式："></a>数据格式：</h3><ul>
<li><p>文本通过标准tokenizer处理，图像通过预训练视觉编码器（如SigLIP）编码为连续视觉token。</p>
</li>
<li><p>引入特殊标记（<image_start>和<image_end>）标识视觉内容边界。</p>
</li>
</ul>
<h3 id="训练目标："><a href="#训练目标：" class="headerlink" title="训练目标："></a>训练目标：</h3><ul>
<li>文本生成：交叉熵损失；视觉生成：预测视觉token与编码器输出的余弦相似度损失。</li>
</ul>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><ul>
<li>通过微调扩散模型将生成的视觉token映射为像素图像（如Stable Diffusion）。</li>
</ul>
<h3 id="创新点："><a href="#创新点：" class="headerlink" title="创新点："></a>创新点：</h3><ul>
<li><p>无需复杂架构改动：直接扩展现有视觉指令微调框架，实现多模态生成。</p>
</li>
<li><p>数据高效：联合训练理解与生成任务，显著降低生成数据需求（仅需200k样本）。</p>
</li>
</ul>
<h2 id="3-关键实验发现"><a href="#3-关键实验发现" class="headerlink" title="3. 关键实验发现"></a>3. 关键实验发现</h2><h3 id="视觉生成的涌现性："><a href="#视觉生成的涌现性：" class="headerlink" title="视觉生成的涌现性："></a>视觉生成的涌现性：</h3><ul>
<li><p>生成能力是视觉理解的副产品：联合训练理解任务（如VQA）时，仅需少量生成数据即可解锁高质量生成能力（5k样本即可生成合理图像）。</p>
</li>
<li><p>仅生成数据训练效果差：单独训练生成任务需数百万样本，且性能显著低于联合训练（FID分数差距达50+）。</p>
</li>
</ul>
<h3 id="理解与生成的协同性与不对称性："><a href="#理解与生成的协同性与不对称性：" class="headerlink" title="理解与生成的协同性与不对称性："></a>理解与生成的协同性与不对称性：</h3><ul>
<li><p>协同性：增加理解数据（VQA）可提升生成质量（FID↓），增加生成数据亦可提升理解能力（VQA分数↑）。</p>
</li>
<li><p>不对称性：理解数据对生成能力的贡献远大于生成数据对理解的贡献（7M VQA数据+200k生成数据 &gt; 1M VQA数据+4M生成数据）。</p>
</li>
</ul>
<h3 id="任务相关性："><a href="#任务相关性：" class="headerlink" title="任务相关性："></a>任务相关性：</h3><ul>
<li>通用视觉任务（如物体识别）与生成能力高度相关（Pearson ρ&gt;0.85），知识型任务（如MMMU）相关性较弱。</li>
</ul>
<h3 id="模型扩展性："><a href="#模型扩展性：" class="headerlink" title="模型扩展性："></a>模型扩展性：</h3><ul>
<li>VPiT在不同规模LLM（如LLaMA-3 8B&#x2F;70B）上均有效，更大模型性能更优。</li>
</ul>
<h2 id="4-MetaMorph模型表现"><a href="#4-MetaMorph模型表现" class="headerlink" title="4. MetaMorph模型表现"></a>4. MetaMorph模型表现</h2><h3 id="多任务性能："><a href="#多任务性能：" class="headerlink" title="多任务性能："></a>多任务性能：</h3><ul>
<li><p>视觉理解：在9个ImageQA基准（如MMBench、ScienceQA）中优于多数统一模型（如EMU-3、Chameleon）。</p>
</li>
<li><p>视觉生成：FID分数（11.8）接近Stable Diffusion（9.6），但能生成更复杂的语义内容。</p>
</li>
</ul>
<h3 id="LLM知识迁移："><a href="#LLM知识迁移：" class="headerlink" title="LLM知识迁移："></a>LLM知识迁移：</h3><ul>
<li><p>生成专业术语图像（如“Chhogori”山峰、“Oncilla”野猫），优于传统T2I模型（如Stable Diffusion）。</p>
</li>
<li><p>解决需多步推理的生成任务（如“黄石国家公园所在国家的国旗→美国国旗”）。</p>
</li>
</ul>
<h3 id="隐式推理能力："><a href="#隐式推理能力：" class="headerlink" title="隐式推理能力："></a>隐式推理能力：</h3><ul>
<li>无需显式思维链（CoT），直接生成符合逻辑的视觉内容（如“爱因斯坦演奏的乐器→小提琴”）。</li>
</ul>
<h2 id="5-贡献与意义"><a href="#5-贡献与意义" class="headerlink" title="5. 贡献与意义"></a>5. 贡献与意义</h2><h3 id="方法论贡献："><a href="#方法论贡献：" class="headerlink" title="方法论贡献："></a>方法论贡献：</h3><ul>
<li><p>提出VPiT框架，证明LLM可通过轻量级指令微调实现高效多模态统一建模。</p>
</li>
<li><p>揭示LLM具备潜在视觉先验能力，仅需少量数据激活。</p>
</li>
</ul>
<h3 id="实践意义："><a href="#实践意义：" class="headerlink" title="实践意义："></a>实践意义：</h3><ul>
<li><p>降低多模态模型训练成本，推动开源模型发展。</p>
</li>
<li><p>为结合LLM知识与生成任务（如教育、设计）提供新思路。</p>
</li>
</ul>
<h2 id="6-未来方向"><a href="#6-未来方向" class="headerlink" title="6. 未来方向"></a>6. 未来方向</h2><ul>
<li><p>探索更复杂的多模态任务（如视频生成与推理）。</p>
</li>
<li><p>研究LLM视觉先验的边界（如3D生成、物理规律理解）。</p>
</li>
<li><p>优化扩散模型与LLM的协同训练策略。</p>
</li>
</ul>
<hr>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/MetaMorph/">MetaMorph</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2025/03/24/b8F6sUEPxJuMcQn.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/2c51f7b2.html" title="Yo'LLaVA:Your Personalized Language and Vision Assistant"><img class="cover" src="https://s2.loli.net/2025/03/27/FuhmV3ksSe1Lbd9.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">Yo'LLaVA:Your Personalized Language and Vision Assistant</div></div></a></div><div class="next-post pull-right"><a href="/post/2412426.html" title="VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model"><img class="cover" src="https://s2.loli.net/2025/03/23/mwWIq2fTsbhSEYu.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2024/07/16/lsEXfWtGT6eRu7k.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">w434</div><div class="author-info__description">An undergraduate majoring in AI at PKU.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">123</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">45</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">30</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/w434"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/w434" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcom to my blog.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#MetaMorph-Multimodal-Understanding-and-Generation-via-Instruction-Tuning-%E9%80%9A%E8%BF%87%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90"><span class="toc-number">1.</span> <span class="toc-text">MetaMorph:Multimodal Understanding and Generation via Instruction Tuning(通过指令微调实现多模态理解和生成)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-number">1.2.</span> <span class="toc-text">1 Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Visual-Predictive-Instruction-Tuning-%E8%A7%86%E8%A7%89%E9%A2%84%E6%B5%8B%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83"><span class="toc-number">1.3.</span> <span class="toc-text">2 Visual-Predictive Instruction Tuning(视觉预测指令微调)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-From-Unimodal-to-Multimodal-Next-Token-Prediction"><span class="toc-number">1.3.1.</span> <span class="toc-text">2.1 From Unimodal to Multimodal Next-Token Prediction</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Tokenizing-multimodal-data"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Tokenizing multimodal data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Model-architecture"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">Model architecture</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Loss-functions"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">Loss functions</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Using-Broad-Types-of-Data"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.2 Using Broad Types of Data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Mapping-Tokens-to-Images-through-Diffusion-%E9%80%9A%E8%BF%87%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%B0%86token%E6%98%A0%E5%B0%84%E5%88%B0%E5%9B%BE%E5%83%8F"><span class="toc-number">1.3.3.</span> <span class="toc-text">2.3 Mapping Tokens to Images through Diffusion(通过扩散模型将token映射到图像)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Findings-on-Unlocking-Visual-Generation"><span class="toc-number">1.4.</span> <span class="toc-text">3 Findings on Unlocking Visual Generation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Visual-Generation-Can-Be-Unlocked-Efficiently-by-Joint-Training-with-Visual-Understanding-%E8%A7%86%E8%A7%89%E7%94%9F%E6%88%90%E5%8F%AF%E4%BB%A5%E9%80%9A%E8%BF%87%E4%B8%8E%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3%E8%81%94%E5%90%88%E8%AE%AD%E7%BB%83%E9%AB%98%E6%95%88%E8%A7%A3%E9%94%81"><span class="toc-number">1.4.1.</span> <span class="toc-text">3.1 Visual Generation Can Be Unlocked Efficiently by Joint Training with Visual Understanding(视觉生成可以通过与视觉理解联合训练高效解锁)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Visual-Understanding-and-Generation-are-Mutually-Beneficial-%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90%E6%98%AF%E7%9B%B8%E4%BA%92%E4%BF%83%E8%BF%9B%E7%9A%84"><span class="toc-number">1.4.2.</span> <span class="toc-text">3.2 Visual Understanding and Generation are Mutually Beneficial(视觉理解和生成是相互促进的)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#More-understanding-data-leads-to-better-understanding-and-generation"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">More understanding data leads to better understanding and generation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#More-generation-data-leads-to-better-understanding-and-generation"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">More generation data leads to better understanding and generation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#This-synergy-scales-across-different-LLMs-%E8%BF%99%E7%A7%8D%E5%8D%8F%E5%90%8C%E4%BD%9C%E7%94%A8%E5%9C%A8%E4%B8%8D%E5%90%8CLLM%E4%B9%8B%E9%97%B4%E5%85%B7%E6%9C%89%E6%89%A9%E5%B1%95%E6%80%A7"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">This synergy scales across different LLMs(这种协同作用在不同LLM之间具有扩展性)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Understanding-Data-Contributes-More"><span class="toc-number">1.4.3.</span> <span class="toc-text">3.3 Understanding Data Contributes More</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-Certain-Understanding-Tasks-Correlate-More-with-Generation-Performance"><span class="toc-number">1.4.4.</span> <span class="toc-text">3.4 Certain Understanding Tasks Correlate More with Generation Performance</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-MetaMorph-Model"><span class="toc-number">1.5.</span> <span class="toc-text">4 MetaMorph Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Competitive-Performance-in-Understanding-and-Generation"><span class="toc-number">1.5.1.</span> <span class="toc-text">4.1 Competitive Performance in Understanding and Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-MetaMorph-can-Leverage-LLM-Knowledge-for-Visual-Generation-MetaMorph%E5%8F%AF%E4%BB%A5%E5%88%A9%E7%94%A8LLM%E7%9F%A5%E8%AF%86%E8%BF%9B%E8%A1%8C%E8%A7%86%E8%A7%89%E7%94%9F%E6%88%90"><span class="toc-number">1.5.2.</span> <span class="toc-text">4.2 MetaMorph can Leverage LLM Knowledge for Visual Generation(MetaMorph可以利用LLM知识进行视觉生成)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Reasoning-in-Multimodal-Generation"><span class="toc-number">1.5.3.</span> <span class="toc-text">4.3 Reasoning in Multimodal Generation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Related-Work"><span class="toc-number">1.6.</span> <span class="toc-text">5 Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Instruction-tuning-and-visual-instruction-tuning"><span class="toc-number">1.6.1.</span> <span class="toc-text">Instruction tuning and visual instruction tuning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#From-Multimodal-LLMs-to-unified-models"><span class="toc-number">1.6.2.</span> <span class="toc-text">From Multimodal LLMs to unified models</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Discussion"><span class="toc-number">1.7.</span> <span class="toc-text">6 Discussion</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DeepSeek-%E8%A6%81%E7%82%B9%E6%80%BB%E7%BB%93"><span class="toc-number">2.</span> <span class="toc-text">DeepSeek 要点总结</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E6%8F%90%E7%BA%B2%E4%B8%8E%E6%A0%B8%E5%BF%83%E8%A6%81%E7%82%B9%E6%80%BB%E7%BB%93"><span class="toc-number">2.1.</span> <span class="toc-text">论文提纲与核心要点总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF%E4%B8%8E%E9%97%AE%E9%A2%98"><span class="toc-number">2.2.</span> <span class="toc-text">1. 研究背景与问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81LLM%E7%8E%B0%E7%8A%B6%EF%BC%9A"><span class="toc-number">2.2.1.</span> <span class="toc-text">多模态LLM现状：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98%EF%BC%9A"><span class="toc-number">2.2.2.</span> <span class="toc-text">核心问题：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95%EF%BC%9A%E8%A7%86%E8%A7%89%E9%A2%84%E6%B5%8B%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%EF%BC%88VPiT%EF%BC%89"><span class="toc-number">2.3.</span> <span class="toc-text">2. 核心方法：视觉预测指令微调（VPiT）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E8%AE%BE%E8%AE%A1%EF%BC%9A"><span class="toc-number">2.3.1.</span> <span class="toc-text">方法设计：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%EF%BC%9A"><span class="toc-number">2.3.2.</span> <span class="toc-text">数据格式：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87%EF%BC%9A"><span class="toc-number">2.3.3.</span> <span class="toc-text">训练目标：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">2.3.4.</span> <span class="toc-text">可视化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9%EF%BC%9A"><span class="toc-number">2.3.5.</span> <span class="toc-text">创新点：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%85%B3%E9%94%AE%E5%AE%9E%E9%AA%8C%E5%8F%91%E7%8E%B0"><span class="toc-number">2.4.</span> <span class="toc-text">3. 关键实验发现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%86%E8%A7%89%E7%94%9F%E6%88%90%E7%9A%84%E6%B6%8C%E7%8E%B0%E6%80%A7%EF%BC%9A"><span class="toc-number">2.4.1.</span> <span class="toc-text">视觉生成的涌现性：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%8D%8F%E5%90%8C%E6%80%A7%E4%B8%8E%E4%B8%8D%E5%AF%B9%E7%A7%B0%E6%80%A7%EF%BC%9A"><span class="toc-number">2.4.2.</span> <span class="toc-text">理解与生成的协同性与不对称性：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E7%9B%B8%E5%85%B3%E6%80%A7%EF%BC%9A"><span class="toc-number">2.4.3.</span> <span class="toc-text">任务相关性：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%89%A9%E5%B1%95%E6%80%A7%EF%BC%9A"><span class="toc-number">2.4.4.</span> <span class="toc-text">模型扩展性：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-MetaMorph%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%8E%B0"><span class="toc-number">2.5.</span> <span class="toc-text">4. MetaMorph模型表现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%80%A7%E8%83%BD%EF%BC%9A"><span class="toc-number">2.5.1.</span> <span class="toc-text">多任务性能：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM%E7%9F%A5%E8%AF%86%E8%BF%81%E7%A7%BB%EF%BC%9A"><span class="toc-number">2.5.2.</span> <span class="toc-text">LLM知识迁移：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%90%E5%BC%8F%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%EF%BC%9A"><span class="toc-number">2.5.3.</span> <span class="toc-text">隐式推理能力：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%B4%A1%E7%8C%AE%E4%B8%8E%E6%84%8F%E4%B9%89"><span class="toc-number">2.6.</span> <span class="toc-text">5. 贡献与意义</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E8%AE%BA%E8%B4%A1%E7%8C%AE%EF%BC%9A"><span class="toc-number">2.6.1.</span> <span class="toc-text">方法论贡献：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E8%B7%B5%E6%84%8F%E4%B9%89%EF%BC%9A"><span class="toc-number">2.6.2.</span> <span class="toc-text">实践意义：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91"><span class="toc-number">2.7.</span> <span class="toc-text">6. 未来方向</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/18cfc8b1.html" title="生成模型基础 05 Debugging"><img src="https://s2.loli.net/2025/09/16/g4dekXTQ6smLh3v.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="生成模型基础 05 Debugging"/></a><div class="content"><a class="title" href="/post/18cfc8b1.html" title="生成模型基础 05 Debugging">生成模型基础 05 Debugging</a><time datetime="2025-10-18T12:25:00.000Z" title="Created 2025-10-18 20:25:00">2025-10-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/68bc60d9.html" title="生成模型基础 04 Autoregressive models"><img src="https://s2.loli.net/2025/09/16/g4dekXTQ6smLh3v.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="生成模型基础 04 Autoregressive models"/></a><div class="content"><a class="title" href="/post/68bc60d9.html" title="生成模型基础 04 Autoregressive models">生成模型基础 04 Autoregressive models</a><time datetime="2025-10-14T05:54:00.000Z" title="Created 2025-10-14 13:54:00">2025-10-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/232f050.html" title="生成模型基础 03 Transformer Model"><img src="https://s2.loli.net/2025/09/16/g4dekXTQ6smLh3v.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="生成模型基础 03 Transformer Model"/></a><div class="content"><a class="title" href="/post/232f050.html" title="生成模型基础 03 Transformer Model">生成模型基础 03 Transformer Model</a><time datetime="2025-09-29T14:10:00.000Z" title="Created 2025-09-29 22:10:00">2025-09-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/cb50706b.html" title="智能机器人概论 07 运动控制"><img src="https://s2.loli.net/2025/09/18/dOm6PbFqzIQnipV.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="智能机器人概论 07 运动控制"/></a><div class="content"><a class="title" href="/post/cb50706b.html" title="智能机器人概论 07 运动控制">智能机器人概论 07 运动控制</a><time datetime="2025-09-27T07:42:00.000Z" title="Created 2025-09-27 15:42:00">2025-09-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/d21bb118.html" title="智能机器人概论 05 激光雷达"><img src="https://s2.loli.net/2025/09/18/dOm6PbFqzIQnipV.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="智能机器人概论 05 激光雷达"/></a><div class="content"><a class="title" href="/post/d21bb118.html" title="智能机器人概论 05 激光雷达">智能机器人概论 05 激光雷达</a><time datetime="2025-09-27T07:27:00.000Z" title="Created 2025-09-27 15:27:00">2025-09-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By w434</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcom to my blog.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">简</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>