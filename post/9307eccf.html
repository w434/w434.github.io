<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLaMA2 自学笔记 | w434's blog</title><meta name="author" content="w434"><meta name="copyright" content="w434"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LLaMA2模型 秉承“小模型+大数据”的设计理念，LLaMA2在LLaMA1的基础上进一步优化和扩充了训练数据，将语料库的规模扩展至约7TB，实现了对更丰富语言和领域资源的覆盖  在预训练阶段之后，LLaMA2采纳了人类反馈强化学习的方法，进一步提升了模型的性能：  使用了大规模且公开的指令微调数据集对模型进行有监督的微调 LLaMA2还训练了RLHF奖励模型，并基于近似策略优化（Proxima">
<meta property="og:type" content="article">
<meta property="og:title" content="LLaMA2 自学笔记">
<meta property="og:url" content="http://example.com/post/9307eccf.html">
<meta property="og:site_name" content="w434&#39;s blog">
<meta property="og:description" content="LLaMA2模型 秉承“小模型+大数据”的设计理念，LLaMA2在LLaMA1的基础上进一步优化和扩充了训练数据，将语料库的规模扩展至约7TB，实现了对更丰富语言和领域资源的覆盖  在预训练阶段之后，LLaMA2采纳了人类反馈强化学习的方法，进一步提升了模型的性能：  使用了大规模且公开的指令微调数据集对模型进行有监督的微调 LLaMA2还训练了RLHF奖励模型，并基于近似策略优化（Proxima">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2025/03/07/u2SEqVOMvexjFKJ.jpg">
<meta property="article:published_time" content="2025-03-09T13:00:00.000Z">
<meta property="article:modified_time" content="2025-03-09T13:47:20.301Z">
<meta property="article:author" content="w434">
<meta property="article:tag" content="LLaMA">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2025/03/07/u2SEqVOMvexjFKJ.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/post/9307eccf.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLaMA2 自学笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-09 21:47:20'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2024/07/16/lsEXfWtGT6eRu7k.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">120</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">45</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">30</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/notes/"><i class="fa-fw fa-solid fa-school"></i><span> 课程笔记</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-face-smile"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/matches/"><i class="fa-fw fa-solid fa-futbol"></i><span> 足球赛事</span></a></li><li><a class="site-page child" href="/chess/"><i class="fa-fw fa-solid fa-chess"></i><span> 国际象棋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw fa-solid fa-film"></i><span> 影评&amp;演出评论&amp;书评</span></a></li><li><a class="site-page child" href="/jottings/"><i class="fa-fw fa-solid fa-book"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/varietyShow/"><i class="fa-fw fa-solid fa-tv"></i><span> 综艺</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/musics/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2025/03/07/u2SEqVOMvexjFKJ.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="w434's blog"><span class="site-name">w434's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/notes/"><i class="fa-fw fa-solid fa-school"></i><span> 课程笔记</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-face-smile"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/matches/"><i class="fa-fw fa-solid fa-futbol"></i><span> 足球赛事</span></a></li><li><a class="site-page child" href="/chess/"><i class="fa-fw fa-solid fa-chess"></i><span> 国际象棋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw fa-solid fa-film"></i><span> 影评&amp;演出评论&amp;书评</span></a></li><li><a class="site-page child" href="/jottings/"><i class="fa-fw fa-solid fa-book"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/varietyShow/"><i class="fa-fw fa-solid fa-tv"></i><span> 综艺</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/musics/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">LLaMA2 自学笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-03-09T13:00:00.000Z" title="Created 2025-03-09 21:00:00">2025-03-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-03-09T13:47:20.301Z" title="Updated 2025-03-09 21:47:20">2025-03-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="LLaMA2 自学笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="LLaMA2模型"><a href="#LLaMA2模型" class="headerlink" title="LLaMA2模型"></a>LLaMA2模型</h1><ul>
<li><p>秉承“小模型+大数据”的设计理念，LLaMA2在LLaMA1的基础上进一步优化和扩充了训练数据，将语料库的规模扩展至约7TB，实现了对更丰富语言和领域资源的<br>覆盖</p>
</li>
<li><p>在预训练阶段之后，LLaMA2采纳了人类反馈强化学习的方法，进一步提升了模型的性能：</p>
<ol>
<li>使用了大规模且公开的指令微调数据集对模型进行有监督的微调</li>
<li>LLaMA2还训练了RLHF奖励模型，并基于近似策略优化（Proximal Policy Optimization, PPO）以及拒绝采样（Rejection Sampling）进行强化学习对模型进行更新</li>
</ol>
</li>
<li><p>在模型架构上，LLaMA2继承了LLaMA1的架构</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/09/eCjS5JB2vKW8lok.png" alt="image.png"></p>
<ul>
<li>LLaMA2-34B和LLaMA270B在decode 阶段的 kv cache 优化上做了改变，还额外增加了分组查询注意力（GroupedQueryAttention,GQA），以提升计算效率</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/09/sZRvYx8ghmwyfOX.png" alt="image.png"></p>
<ul>
<li>在分组查询注意力机制下，键（key）以及值（value）不再与查询（query）一一对应，而是一组查询共享相同的键和值，从而有效降低内存占用并减少模型总参数量</li>
</ul>
<h2 id="分组查询注意力"><a href="#分组查询注意力" class="headerlink" title="分组查询注意力"></a>分组查询注意力</h2><ul>
<li><p>MQA，全称 Multi Query Attention, GQA 由 google 提出的 MQA 变种，全称 Group-Query Attention，都是多头注意力（MHA）的变体，本质上是一种共用 KV cache 的优化方法</p>
</li>
<li><p>kv cache 优化三种方案：MHA、 MQA 和 GQA 的原理及区别如下：</p>
<ol>
<li>MHA(Multi-Head Attention)：QKV 三部分有相同数量的头（head），且一一对应。每次做 Attention，head1 的 QKV 就做好自己运算就可以，最后输出时将各个头的 self-attention output 相拼接</li>
<li>MQA 则是让 Q 仍然保持原来的头数，但 K 和 V 只有一个头，相当于所有的 Q 头共享一个 K 和 V 头，所以叫做 Multi-Query 。这直接让 KV cache 内存减少了 head_num 倍</li>
<li>GQA 是 MHA 和 MQA 的折中，将 Q 分成 8 组，每组共享相同的一个 kv 头，假设 Q 有 64 个头，则使用 GQA 技术后，kv 头数 &#x3D; 64 &#x2F; 8 &#x3D; 8 。这直接让 KV cache 内存减少了 8 倍</li>
</ol>
</li>
<li><p>MHA、 MQA 和 GQA 原理的可视化对比如下图所示:</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/09/tkVTpjxJlmSdBCw.png" alt="image.png"></p>
<ul>
<li>LLaMA2 官方实现的 GQA（包含了 kv cahce）代码如下所示（经简化）：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">repeat_kv</span>(<span class="params">x: torch.Tensor, n_rep: <span class="built_in">int</span></span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;torch.repeat_interleave(x, dim=2, repeats=n_rep)&quot;&quot;&quot;</span></span><br><span class="line">    bs, slen, n_kv_heads, head_dim = x.shape</span><br><span class="line">    <span class="comment"># 根据n_rep，拓展KV</span></span><br><span class="line">    <span class="keyword">if</span> n_rep == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">return</span> (x[:, :, :, <span class="literal">None</span>, :].expand(bs, slen, n_kv_heads, n_rep, head_dim).reshape(bs, slen, n_kv_heads * n_rep, head_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args: ModelArgs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        ...</span><br><span class="line">        <span class="variable language_">self</span>.n_local_heads = args.n_heads // model_parallel_size <span class="comment">#Q的头数</span></span><br><span class="line">        <span class="variable language_">self</span>.n_local_kv_heads = <span class="variable language_">self</span>.n_kv_heads // model_parallel_size  <span class="comment">#KV的头数</span></span><br><span class="line">        <span class="variable language_">self</span>.n_rep = <span class="variable language_">self</span>.n_local_heads // <span class="variable language_">self</span>.n_local_kv_heads </span><br><span class="line">        ...</span><br><span class="line">        <span class="variable language_">self</span>.wq = ColumnParallelLinear(args.dim,args.n_heads * <span class="variable language_">self</span>.head_dim, <span class="comment"># Q的头数* head_dim</span></span><br><span class="line">                                       ...)</span><br><span class="line">        <span class="variable language_">self</span>.wk = ColumnParallelLinear(args.dim,<span class="variable language_">self</span>.n_kv_heads * <span class="variable language_">self</span>.head_dim, <span class="comment"># K的头数* head_dim</span></span><br><span class="line">                                       ...)</span><br><span class="line">        <span class="variable language_">self</span>.wv = ColumnParallelLinear(args.dim,<span class="variable language_">self</span>.n_kv_heads * <span class="variable language_">self</span>.head_dim,<span class="comment"># V的头数* head_dim</span></span><br><span class="line">                                       ...)</span><br><span class="line">        <span class="variable language_">self</span>.wo = RowParallelLinear(args.n_heads * <span class="variable language_">self</span>.head_dim,args.dim,... )</span><br><span class="line">​</span><br><span class="line">        <span class="variable language_">self</span>.cache_k = torch.zeros((args.max_batch_size,args.max_seq_len,<span class="variable language_">self</span>.n_local_kv_heads, <span class="comment">#KV的头数</span></span><br><span class="line">                <span class="variable language_">self</span>.head_dim,)).cuda()</span><br><span class="line">        <span class="variable language_">self</span>.cache_v = torch.zeros((args.max_batch_size,args.max_seq_len,<span class="variable language_">self</span>.n_local_kv_heads,<span class="comment">#KV的头数         </span></span><br><span class="line">                                    <span class="variable language_">self</span>.head_dim,)).cuda()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        x: torch.Tensor,</span></span><br><span class="line"><span class="params">        start_pos: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        freqs_cis: torch.Tensor,</span></span><br><span class="line"><span class="params">        mask: <span class="type">Optional</span>[torch.Tensor],</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        bsz, seqlen, _ = x.shape</span><br><span class="line">        xq, xk, xv = <span class="variable language_">self</span>.wq(x), <span class="variable language_">self</span>.wk(x), <span class="variable language_">self</span>.wv(x)</span><br><span class="line">​</span><br><span class="line">        xq = xq.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        xk = xk.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_kv_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        xv = xv.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_kv_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) <span class="comment">#嵌入RoPE位置编码</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># 按此时序列的句子长度把kv添加到cache中</span></span><br><span class="line">        <span class="comment"># 初始在prompt阶段seqlen&gt;=1, 后续生成过程中seqlen==1</span></span><br><span class="line">        <span class="variable language_">self</span>.cache_k[:bsz, start_pos : start_pos + seqlen] = xk</span><br><span class="line">        <span class="variable language_">self</span>.cache_v[:bsz, start_pos : start_pos + seqlen] = xv</span><br><span class="line">        <span class="comment"># 读取新进来的token所计算得到的k和v</span></span><br><span class="line">        keys = <span class="variable language_">self</span>.cache_k[:bsz, : start_pos + seqlen]</span><br><span class="line">        values = <span class="variable language_">self</span>.cache_v[:bsz, : start_pos + seqlen]</span><br><span class="line">​</span><br><span class="line">        <span class="comment"># repeat k/v heads if n_kv_heads &lt; n_heads</span></span><br><span class="line">        keys = repeat_kv(keys, <span class="variable language_">self</span>.n_rep)  <span class="comment"># (bs, seqlen, n_local_heads, head_dim)</span></span><br><span class="line">        values = repeat_kv(values, <span class="variable language_">self</span>.n_rep)  <span class="comment"># (bs, seqlen, n_local_heads, head_dim)</span></span><br><span class="line"></span><br><span class="line">        xq = xq.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># (bs, n_local_heads, seqlen, head_dim)</span></span><br><span class="line">        keys = keys.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        values = values.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment">#计算q*k</span></span><br><span class="line">        scores = torch.matmul(xq, keys.transpose(<span class="number">2</span>, <span class="number">3</span>)) / math.sqrt(<span class="variable language_">self</span>.head_dim)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment">#加入mask，使得前面的token在于后面的token计算attention时得分为0，mask掉</span></span><br><span class="line">            scores = scores + mask  <span class="comment"># (bs, n_local_heads, seqlen, cache_len + seqlen)</span></span><br><span class="line">        scores = F.softmax(scores.<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq)</span><br><span class="line">        output = torch.matmul(scores, values)  <span class="comment"># (bs, n_local_heads, seqlen, head_dim)</span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(bsz, seqlen, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.wo(output)</span><br></pre></td></tr></table></figure>

<h2 id="近似策略优化（Proximal-Policy-Optimization-PPO）"><a href="#近似策略优化（Proximal-Policy-Optimization-PPO）" class="headerlink" title="近似策略优化（Proximal Policy Optimization, PPO）"></a>近似策略优化（Proximal Policy Optimization, PPO）</h2><ul>
<li><p>PPO是OpenAI spinning up下的第三个算法，翻译为“近端策略优化”</p>
</li>
<li><p>PPO的创造是基于和TRPO相同的问题：在策略梯度定理的步长α的选取中，如何选取合适的步长，使得更新的参数尽可能对应最好的策略，但也不至于走得太远，以至于导致性能崩溃</p>
</li>
<li><p>TRPO的方法是使用将优化函数二阶展开的方法进行优化，而PPO则采用将泰勒一阶展开的方法并使用了一些trick来保证新旧策略之间的距离不要过大</p>
</li>
<li><p>PPO算法能够简单快速的达到TRPO相同的效果</p>
</li>
<li><p>PPO有两种不同的实现：</p>
<ol>
<li>基于惩罚的PPO：将KL散度不是作为一个硬约束，而是作为一个惩罚函数加在优化函数上</li>
<li>基于Clip的PPO：将优化函数转化成了一个Clip函数，效果好于前者</li>
</ol>
</li>
</ul>
<h3 id="基于Clip的PPO"><a href="#基于Clip的PPO" class="headerlink" title="基于Clip的PPO"></a>基于Clip的PPO</h3><ul>
<li>对于策略梯度这类方法中最重要的就是如何将最大化策略收益转换成一个优化策略参数的函数，所以我们首先来看PPO中的策略优化函数，可以描述为：</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/09/FfiQMCXJATYHkWb.png" alt="image.png"></p>
<ul>
<li>其中epsilon是控制新旧策略距离的超参数。clip函数可以通过下面这张图来表示清楚:</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/09/Ji9gUINDu3E7Qtj.png" alt="image.png"></p>
<p><img src="https://s2.loli.net/2025/03/09/oWzGaIBN1mHnqPt.png" alt="image.png"></p>
<p><img src="https://s2.loli.net/2025/03/09/6hCLoFuRwiUzBA8.png" alt="image.png"></p>
<ul>
<li><p>没有了原来kl散度的约束，那么我们就可以直接用优化函数对θ进行策略更新</p>
</li>
<li><p>如果这个时候我们直接用一阶展开其实就是利用梯度来进行更新，那么我们就可以直接用loss的方式来对策略进行更新，回归到了我们最开始的策略梯度方法</p>
</li>
<li><p>具体PPO算法的过程可以描述成这样：</p>
<ol>
<li>初始化环境和策略参数</li>
<li>策略和环境采样得到回报等数据</li>
<li>计算折扣累计回报</li>
<li>计算通过critic模型计算状态价值并通过状态价值计算优势函数(这样计算出来的可以减小方差)</li>
<li>通过优势函数函数和策略的概率计算出来loss(和之前策略梯度那里很类似，不过这里有一个clip)</li>
<li>用loss来更新策略</li>
<li>用折扣累计汇报和critic模型计算状态价值来更新critic模型(回到第2步)</li>
</ol>
</li>
<li><p>伪代码如下：</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/09/oOQWDgF9lqhnAmt.png" alt="image.png"></p>
<h2 id="拒绝采样（Rejection-Sampling）"><a href="#拒绝采样（Rejection-Sampling）" class="headerlink" title="拒绝采样（Rejection Sampling）"></a>拒绝采样（Rejection Sampling）</h2><h3 id="什么是拒绝采样（Rejection-Sampling）"><a href="#什么是拒绝采样（Rejection-Sampling）" class="headerlink" title="什么是拒绝采样（Rejection Sampling）"></a>什么是拒绝采样（Rejection Sampling）</h3><ul>
<li><p>AI 生成的答案并不总是正确的，有时候它会输出胡言乱语、逻辑错误或者无意义的推理链</p>
</li>
<li><p>如果不进行筛选，这些错误答案可能会影响模型的学习过程，甚至让 AI 形成错误的推理模式</p>
</li>
<li><p>为了解决这个问题，拒绝采样（Rejection Sampling, RS）让 AI 在训练过程中优中选优，只保留最优质的推理答案，从而提升整体推理能力</p>
</li>
</ul>
<h3 id="拒绝采样的核心思想"><a href="#拒绝采样的核心思想" class="headerlink" title="拒绝采样的核心思想"></a>拒绝采样的核心思想</h3><ul>
<li>拒绝采样的步骤如下：</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/09/5JrQbvFTi9wG4cK.png" alt="image.png"></p>
<hr>
<h1 id="LLaMA2-model-py"><a href="#LLaMA2-model-py" class="headerlink" title="LLaMA2 model.py"></a>LLaMA2 model.py</h1><h2 id="Grouped-Query-Attention-GQA-的引入"><a href="#Grouped-Query-Attention-GQA-的引入" class="headerlink" title="Grouped Query Attention (GQA) 的引入"></a>Grouped Query Attention (GQA) 的引入</h2><ul>
<li><p>代码位置：ModelArgs类、Attention类、repeat_kv函数</p>
</li>
<li><p>改进点：</p>
<ol>
<li>LLaMA2新增了n_kv_heads参数，允许键值头（KV Heads）数量少于查询头（Q Heads）</li>
<li>当n_kv_heads &lt; n_heads时，通过repeat_kv函数将KV头重复多次以匹配Q头数量（如将1个KV头分配给多个Q头），减少显存占用并提升计算效率</li>
</ol>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ModelArgs中新增n_kv_heads参数</span></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArgs</span>:</span><br><span class="line">    n_kv_heads: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>  <span class="comment"># 允许自定义KV头数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Attention类初始化时处理GQA逻辑</span></span><br><span class="line"><span class="variable language_">self</span>.n_kv_heads = args.n_heads <span class="keyword">if</span> args.n_kv_heads <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> args.n_kv_heads</span><br><span class="line"><span class="variable language_">self</span>.n_rep = <span class="variable language_">self</span>.n_local_heads // <span class="variable language_">self</span>.n_local_kv_heads  <span class="comment"># 计算重复次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># repeat_kv函数实现KV头复制</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">repeat_kv</span>(<span class="params">x: torch.Tensor, n_rep: <span class="built_in">int</span></span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="keyword">return</span> x[:, :, :, <span class="literal">None</span>, :].expand(...).reshape(...)  <span class="comment"># 通过扩展和重塑复制KV头</span></span><br></pre></td></tr></table></figure>

<h2 id="FFN层动态维度调整"><a href="#FFN层动态维度调整" class="headerlink" title="FFN层动态维度调整"></a>FFN层动态维度调整</h2><ul>
<li><p>代码位置：FeedForward类</p>
</li>
<li><p>改进点：</p>
<ol>
<li>新增ffn_dim_multiplier参数，允许自定义FFN中间层的缩放比例（默认隐藏层维度为4*dim，LLaMA1固定为此值）</li>
<li>通过ffn_dim_multiplier可灵活调整模型容量，例如增大倍数以提升模型表现，或减小倍数以降低计算量</li>
</ol>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># FeedForward初始化时动态计算隐藏层维度</span></span><br><span class="line"><span class="keyword">if</span> ffn_dim_multiplier <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    hidden_dim = <span class="built_in">int</span>(ffn_dim_multiplier * hidden_dim)  <span class="comment"># 自定义缩放</span></span><br><span class="line">hidden_dim = multiple_of * ((hidden_dim + multiple_of - <span class="number">1</span>) // multiple_of)  <span class="comment"># 确保为multiple_of的倍数</span></span><br></pre></td></tr></table></figure>

<h2 id="旋转位置编码（RoPE）优化"><a href="#旋转位置编码（RoPE）优化" class="headerlink" title="旋转位置编码（RoPE）优化"></a>旋转位置编码（RoPE）优化</h2><ul>
<li><p>代码位置：precompute_freqs_cis、apply_rotary_emb</p>
</li>
<li><p>改进点：</p>
<ol>
<li>频率矩阵预计算长度扩展：precompute_freqs_cis中计算max_seq_len * 2的频率，可能支持更长的上下文或更灵活的位置插值（如ALiBi外推）</li>
<li>张量重塑优化：apply_rotary_emb中通过flatten(3)合并维度，提升计算效率</li>
</ol>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LLaMA2预计算更长的频率矩阵</span></span><br><span class="line"><span class="variable language_">self</span>.freqs_cis = precompute_freqs_cis(</span><br><span class="line">    <span class="variable language_">self</span>.params.dim // <span class="variable language_">self</span>.params.n_heads, </span><br><span class="line">    <span class="variable language_">self</span>.params.max_seq_len * <span class="number">2</span>  <span class="comment"># 长度扩展为2倍</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更高效的张量操作</span></span><br><span class="line">xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(<span class="number">3</span>)  <span class="comment"># 合并最后两个维度</span></span><br></pre></td></tr></table></figure>

<h2 id="模型并行与参数初始化"><a href="#模型并行与参数初始化" class="headerlink" title="模型并行与参数初始化"></a>模型并行与参数初始化</h2><ul>
<li><p>代码位置：ColumnParallelLinear、RowParallelLinear</p>
</li>
<li><p>改进点：</p>
<ol>
<li>显式初始化方法：并行线性层中通过init_method&#x3D;lambda x: x指定初始化方式（可能采用更合理的分布）</li>
<li>更精细的并行划分：根据model_parallel_size动态分配本地头数（n_local_heads），提升多卡并行效率</li>
</ol>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 并行线性层使用lambda初始化</span></span><br><span class="line"><span class="variable language_">self</span>.wq = ColumnParallelLinear(</span><br><span class="line">    args.dim, args.n_heads * <span class="variable language_">self</span>.head_dim,</span><br><span class="line">    init_method=<span class="keyword">lambda</span> x: x  <span class="comment"># 可能采用默认初始化或与LLaMA1不同</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 动态计算本地头数</span></span><br><span class="line">model_parallel_size = fs_init.get_model_parallel_world_size()</span><br><span class="line"><span class="variable language_">self</span>.n_local_heads = args.n_heads // model_parallel_size</span><br></pre></td></tr></table></figure>

<h2 id="推理优化"><a href="#推理优化" class="headerlink" title="推理优化"></a>推理优化</h2><ul>
<li><p>代码位置：Transformer.forward</p>
</li>
<li><p>改进点：使用@torch.inference_mode()装饰器替代torch.no_grad()，减少内存占用并加速推理（inference_mode禁用梯度计算更彻底）</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.inference_mode()  </span><span class="comment"># 更高效的推理模式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens: torch.Tensor, start_pos: <span class="built_in">int</span></span>):</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<h2 id="掩码生成优化"><a href="#掩码生成优化" class="headerlink" title="掩码生成优化"></a>掩码生成优化</h2><ul>
<li><p>代码位置：Transformer.forward中的mask生成</p>
</li>
<li><p>改进点：当seqlen &gt; 1时动态生成因果掩码（torch.triu），支持可变长度输入并减少计算冗余</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mask = torch.full((<span class="number">1</span>, <span class="number">1</span>, seqlen, seqlen), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>), device=tokens.device)</span><br><span class="line">mask = torch.triu(mask, diagonal=start_pos + <span class="number">1</span>)  <span class="comment"># 动态生成三角掩码</span></span><br></pre></td></tr></table></figure>

<h2 id="关键模块解析"><a href="#关键模块解析" class="headerlink" title="关键模块解析"></a>关键模块解析</h2><ul>
<li>模型参数定义 (ModelArgs)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArgs</span>:</span><br><span class="line">    dim: <span class="built_in">int</span> = <span class="number">4096</span>                <span class="comment"># 向量维度（每个token的表示维度）</span></span><br><span class="line">    n_layers: <span class="built_in">int</span> = <span class="number">32</span>             <span class="comment"># Transformer层数</span></span><br><span class="line">    n_heads: <span class="built_in">int</span> = <span class="number">32</span>              <span class="comment"># 注意力头总数</span></span><br><span class="line">    n_kv_heads: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>  <span class="comment"># Key/Value头数（GQA特性）</span></span><br><span class="line">    vocab_size: <span class="built_in">int</span> = -<span class="number">1</span>            <span class="comment"># 词表大小（由tokenizer确定）</span></span><br><span class="line">    multiple_of: <span class="built_in">int</span> = <span class="number">256</span>          <span class="comment"># 确保FFN层维度是该值的倍数</span></span><br><span class="line">    ffn_dim_multiplier: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span>  <span class="comment"># FFN层维度缩放因子</span></span><br><span class="line">    norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>          <span class="comment"># LayerNorm的epsilon值</span></span><br><span class="line">    </span><br><span class="line">    max_batch_size: <span class="built_in">int</span> = <span class="number">32</span>        <span class="comment"># 最大批处理大小</span></span><br><span class="line">    max_seq_len: <span class="built_in">int</span> = <span class="number">2048</span>         <span class="comment"># 最大序列长度</span></span><br></pre></td></tr></table></figure>

<ul>
<li>归一化层 (RMSNorm)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RMSNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, eps: <span class="built_in">float</span> = <span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.ones(dim))  <span class="comment"># 可学习的缩放参数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_norm</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 核心计算：x / sqrt(mean(x^2) + eps)</span></span><br><span class="line">        <span class="keyword">return</span> x * torch.rsqrt(x.<span class="built_in">pow</span>(<span class="number">2</span>).mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + <span class="variable language_">self</span>.eps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        output = <span class="variable language_">self</span>._norm(x.<span class="built_in">float</span>()).type_as(x)  <span class="comment"># 计算归一化</span></span><br><span class="line">        <span class="keyword">return</span> output * <span class="variable language_">self</span>.weight                <span class="comment"># 应用缩放</span></span><br></pre></td></tr></table></figure>

<ul>
<li>旋转位置编码 (Rotary Positional Embedding)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">precompute_freqs_cis</span>(<span class="params">dim: <span class="built_in">int</span>, end: <span class="built_in">int</span>, theta: <span class="built_in">float</span> = <span class="number">10000.0</span></span>):</span><br><span class="line">    <span class="comment"># 预计算旋转角度（复数形式）</span></span><br><span class="line">    freqs = <span class="number">1.0</span> / (theta ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>)[: (dim//<span class="number">2</span>)].<span class="built_in">float</span>() / dim))</span><br><span class="line">    t = torch.arange(end, device=freqs.device)</span><br><span class="line">    freqs = torch.outer(t, freqs)                  <span class="comment"># 外积生成频率矩阵</span></span><br><span class="line">    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  <span class="comment"># 转换为复数形式</span></span><br><span class="line">    <span class="keyword">return</span> freqs_cis                               <span class="comment"># (seq_len, dim//2)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_emb</span>(<span class="params">xq, xk, freqs_cis</span>):</span><br><span class="line">    <span class="comment"># 将输入转换为复数形式</span></span><br><span class="line">    xq_ = torch.view_as_complex(xq.<span class="built_in">float</span>().reshape(*xq.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">    xk_ = torch.view_as_complex(xk.<span class="built_in">float</span>().reshape(*xk.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调整频率矩阵形状以匹配输入</span></span><br><span class="line">    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 应用旋转（复数乘法）</span></span><br><span class="line">    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(<span class="number">3</span>)</span><br><span class="line">    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> xq_out.type_as(xq), xk_out.type_as(xk)</span><br></pre></td></tr></table></figure>

<ul>
<li>注意力机制 (Grouped-Query Attention)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args: ModelArgs</span>):</span><br><span class="line">        <span class="comment"># 关键参数计算</span></span><br><span class="line">        <span class="variable language_">self</span>.n_kv_heads = args.n_kv_heads <span class="keyword">or</span> args.n_heads  <span class="comment"># 如果未指定则等于n_heads</span></span><br><span class="line">        <span class="variable language_">self</span>.n_rep = <span class="variable language_">self</span>.n_local_heads // <span class="variable language_">self</span>.n_local_kv_heads  <span class="comment"># 每个KV头重复次数</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 线性变换层（模型并行）</span></span><br><span class="line">        <span class="variable language_">self</span>.wq = ColumnParallelLinear(...)  <span class="comment"># 处理Query</span></span><br><span class="line">        <span class="variable language_">self</span>.wk = ColumnParallelLinear(...)  <span class="comment"># 处理Key</span></span><br><span class="line">        <span class="variable language_">self</span>.wv = ColumnParallelLinear(...)  <span class="comment"># 处理Value</span></span><br><span class="line">        <span class="variable language_">self</span>.wo = RowParallelLinear(...)     <span class="comment"># 输出层</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># KV缓存（用于生成式推理）</span></span><br><span class="line">        <span class="variable language_">self</span>.cache_k = torch.zeros(...).cuda()  <span class="comment"># (batch, seq, n_kv_heads, dim)</span></span><br><span class="line">        <span class="variable language_">self</span>.cache_v = torch.zeros(...).cuda()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, start_pos, freqs_cis, mask</span>):</span><br><span class="line">        <span class="comment"># 步骤1：线性变换得到Q/K/V</span></span><br><span class="line">        xq, xk, xv = <span class="variable language_">self</span>.wq(x), <span class="variable language_">self</span>.wk(x), <span class="variable language_">self</span>.wv(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 步骤2：重塑形状为 (bs, seq_len, n_heads, head_dim)</span></span><br><span class="line">        xq = xq.view(...)</span><br><span class="line">        xk = xk.view(...)</span><br><span class="line">        xv = xv.view(...)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 步骤3：应用旋转位置编码</span></span><br><span class="line">        xq, xk = apply_rotary_emb(xq, xk, freqs_cis)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 步骤4：更新KV缓存（用于增量生成）</span></span><br><span class="line">        <span class="variable language_">self</span>.cache_k[:bsz, start_pos : start_pos+seqlen] = xk</span><br><span class="line">        <span class="variable language_">self</span>.cache_v[:bsz, start_pos : start_pos+seqlen] = xv</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 步骤5：重复KV头以匹配Q的数量（GQA核心）</span></span><br><span class="line">        keys = repeat_kv(keys, <span class="variable language_">self</span>.n_rep)</span><br><span class="line">        values = repeat_kv(values, <span class="variable language_">self</span>.n_rep)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 步骤6：计算注意力分数</span></span><br><span class="line">        scores = torch.matmul(xq, keys.transpose(<span class="number">2</span>,<span class="number">3</span>)) / math.sqrt(<span class="variable language_">self</span>.head_dim)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores += mask  <span class="comment"># 应用因果掩码</span></span><br><span class="line">        scores = F.softmax(scores.<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 步骤7：加权求和得到输出</span></span><br><span class="line">        output = torch.matmul(scores, values)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.wo(output)  <span class="comment"># 输出线性变换</span></span><br></pre></td></tr></table></figure>

<ul>
<li>前馈网络 (FeedForward)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, multiple_of, ffn_dim_multiplier</span>):</span><br><span class="line">        <span class="comment"># 动态计算中间层维度</span></span><br><span class="line">        hidden_dim = <span class="built_in">int</span>(<span class="number">2</span> * hidden_dim / <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">if</span> ffn_dim_multiplier:  <span class="comment"># 允许自定义缩放</span></span><br><span class="line">            hidden_dim = <span class="built_in">int</span>(ffn_dim_multiplier * hidden_dim)</span><br><span class="line">        hidden_dim = multiple_of * ((hidden_dim + multiple_of -<span class="number">1</span>) // multiple_of)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 三层线性变换（SwiGLU结构）</span></span><br><span class="line">        <span class="variable language_">self</span>.w1 = ColumnParallelLinear(...)  <span class="comment"># 门控分支</span></span><br><span class="line">        <span class="variable language_">self</span>.w2 = RowParallelLinear(...)     <span class="comment"># 输出层</span></span><br><span class="line">        <span class="variable language_">self</span>.w3 = ColumnParallelLinear(...)  <span class="comment"># 主干分支</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># SwiGLU激活：silu(w1(x)) * w3(x)</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.w2(F.silu(<span class="variable language_">self</span>.w1(x)) * <span class="variable language_">self</span>.w3(x))</span><br></pre></td></tr></table></figure>

<ul>
<li>Transformer块 (TransformerBlock)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer_id: <span class="built_in">int</span>, args: ModelArgs</span>):</span><br><span class="line">        <span class="comment"># 主要组件</span></span><br><span class="line">        <span class="variable language_">self</span>.attention = Attention(args)          <span class="comment"># 注意力层</span></span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = FeedForward(...)      <span class="comment"># FFN层</span></span><br><span class="line">        <span class="variable language_">self</span>.attention_norm = RMSNorm(...)        <span class="comment"># 注意力前归一化</span></span><br><span class="line">        <span class="variable language_">self</span>.ffn_norm = RMSNorm(...)              <span class="comment"># FFN前归一化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, start_pos, freqs_cis, mask</span>):</span><br><span class="line">        <span class="comment"># 残差连接结构</span></span><br><span class="line">        h = x + <span class="variable language_">self</span>.attention(<span class="variable language_">self</span>.attention_norm(x), start_pos, freqs_cis, mask)</span><br><span class="line">        out = h + <span class="variable language_">self</span>.feed_forward(<span class="variable language_">self</span>.ffn_norm(h))</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<ul>
<li>整体模型架构 (Transformer)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params: ModelArgs</span>):</span><br><span class="line">        <span class="comment"># 核心组件</span></span><br><span class="line">        <span class="variable language_">self</span>.tok_embeddings = ParallelEmbedding(...)  <span class="comment"># 词嵌入层</span></span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.ModuleList([...])            <span class="comment"># 堆叠Transformer块</span></span><br><span class="line">        <span class="variable language_">self</span>.norm = RMSNorm(params.dim, eps=params.norm_eps)  <span class="comment"># 最终归一化</span></span><br><span class="line">        <span class="variable language_">self</span>.output = ColumnParallelLinear(...)       <span class="comment"># 输出投影层</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 预计算旋转编码（长度是max_seq_len的2倍）</span></span><br><span class="line">        <span class="variable language_">self</span>.freqs_cis = precompute_freqs_cis(..., <span class="variable language_">self</span>.params.max_seq_len*<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.inference_mode()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens: torch.Tensor, start_pos: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="comment"># 步骤1：词嵌入</span></span><br><span class="line">        h = <span class="variable language_">self</span>.tok_embeddings(tokens)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 步骤2：获取当前token的位置编码</span></span><br><span class="line">        freqs_cis = <span class="variable language_">self</span>.freqs_cis[start_pos : start_pos + seqlen]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 步骤3：生成因果掩码（仅处理新生成的位置）</span></span><br><span class="line">        <span class="keyword">if</span> seqlen &gt; <span class="number">1</span>:</span><br><span class="line">            mask = torch.triu(torch.full(...), diagonal=start_pos+<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 步骤4：逐层处理</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            h = layer(h, start_pos, freqs_cis, mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 步骤5：最终归一化+输出投影</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.output(<span class="variable language_">self</span>.norm(h))</span><br></pre></td></tr></table></figure>

<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Copyright (c) Meta Platforms, Inc. and affiliates.</span></span><br><span class="line"><span class="comment"># This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>, <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> fairscale.nn.model_parallel.initialize <span class="keyword">as</span> fs_init</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> fairscale.nn.model_parallel.layers <span class="keyword">import</span> (</span><br><span class="line">    ColumnParallelLinear,</span><br><span class="line">    ParallelEmbedding,</span><br><span class="line">    RowParallelLinear,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArgs</span>:</span><br><span class="line">    dim: <span class="built_in">int</span> = <span class="number">4096</span></span><br><span class="line">    n_layers: <span class="built_in">int</span> = <span class="number">32</span></span><br><span class="line">    n_heads: <span class="built_in">int</span> = <span class="number">32</span></span><br><span class="line">    n_kv_heads: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span><br><span class="line">    vocab_size: <span class="built_in">int</span> = -<span class="number">1</span>  <span class="comment"># defined later by tokenizer</span></span><br><span class="line">    multiple_of: <span class="built_in">int</span> = <span class="number">256</span>  <span class="comment"># make SwiGLU hidden layer size multiple of large power of 2</span></span><br><span class="line">    ffn_dim_multiplier: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span></span><br><span class="line">    norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line">    max_batch_size: <span class="built_in">int</span> = <span class="number">32</span></span><br><span class="line">    max_seq_len: <span class="built_in">int</span> = <span class="number">2048</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RMSNorm</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, eps: <span class="built_in">float</span> = <span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize the RMSNorm normalization layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dim (int): The dimension of the input tensor.</span></span><br><span class="line"><span class="string">            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Attributes:</span></span><br><span class="line"><span class="string">            eps (float): A small value added to the denominator for numerical stability.</span></span><br><span class="line"><span class="string">            weight (nn.Parameter): Learnable scaling parameter.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.ones(dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_norm</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply the RMSNorm normalization to the input tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (torch.Tensor): The input tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            torch.Tensor: The normalized tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x * torch.rsqrt(x.<span class="built_in">pow</span>(<span class="number">2</span>).mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + <span class="variable language_">self</span>.eps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Forward pass through the RMSNorm layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (torch.Tensor): The input tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            torch.Tensor: The output tensor after applying RMSNorm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        output = <span class="variable language_">self</span>._norm(x.<span class="built_in">float</span>()).type_as(x)</span><br><span class="line">        <span class="keyword">return</span> output * <span class="variable language_">self</span>.weight</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">precompute_freqs_cis</span>(<span class="params">dim: <span class="built_in">int</span>, end: <span class="built_in">int</span>, theta: <span class="built_in">float</span> = <span class="number">10000.0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This function calculates a frequency tensor with complex exponentials using the given dimension &#x27;dim&#x27;</span></span><br><span class="line"><span class="string">    and the end index &#x27;end&#x27;. The &#x27;theta&#x27; parameter scales the frequencies.</span></span><br><span class="line"><span class="string">    The returned tensor contains complex values in complex64 data type.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Dimension of the frequency tensor.</span></span><br><span class="line"><span class="string">        end (int): End index for precomputing frequencies.</span></span><br><span class="line"><span class="string">        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        torch.Tensor: Precomputed frequency tensor with complex exponentials.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    freqs = <span class="number">1.0</span> / (theta ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>)[: (dim // <span class="number">2</span>)].<span class="built_in">float</span>() / dim))</span><br><span class="line">    t = torch.arange(end, device=freqs.device)  <span class="comment"># type: ignore</span></span><br><span class="line">    freqs = torch.outer(t, freqs).<span class="built_in">float</span>()  <span class="comment"># type: ignore</span></span><br><span class="line">    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  <span class="comment"># complex64</span></span><br><span class="line">    <span class="keyword">return</span> freqs_cis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reshape_for_broadcast</span>(<span class="params">freqs_cis: torch.Tensor, x: torch.Tensor</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Reshape frequency tensor for broadcasting it with another tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This function reshapes the frequency tensor to have the same shape as the target tensor &#x27;x&#x27;</span></span><br><span class="line"><span class="string">    for the purpose of broadcasting the frequency tensor during element-wise operations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.</span></span><br><span class="line"><span class="string">        x (torch.Tensor): Target tensor for broadcasting compatibility.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        torch.Tensor: Reshaped frequency tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">        AssertionError: If the frequency tensor doesn&#x27;t match the expected shape.</span></span><br><span class="line"><span class="string">        AssertionError: If the target tensor &#x27;x&#x27; doesn&#x27;t have the expected number of dimensions.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ndim = x.ndim</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= <span class="number">1</span> &lt; ndim</span><br><span class="line">    <span class="keyword">assert</span> freqs_cis.shape == (x.shape[<span class="number">1</span>], x.shape[-<span class="number">1</span>])</span><br><span class="line">    shape = [d <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">or</span> i == ndim - <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(x.shape)]</span><br><span class="line">    <span class="keyword">return</span> freqs_cis.view(*shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_emb</span>(<span class="params"></span></span><br><span class="line"><span class="params">    xq: torch.Tensor,</span></span><br><span class="line"><span class="params">    xk: torch.Tensor,</span></span><br><span class="line"><span class="params">    freqs_cis: torch.Tensor,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, torch.Tensor]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Apply rotary embeddings to input tensors using the given frequency tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This function applies rotary embeddings to the given query &#x27;xq&#x27; and key &#x27;xk&#x27; tensors using the provided</span></span><br><span class="line"><span class="string">    frequency tensor &#x27;freqs_cis&#x27;. The input tensors are reshaped as complex numbers, and the frequency tensor</span></span><br><span class="line"><span class="string">    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are</span></span><br><span class="line"><span class="string">    returned as real tensors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        xq (torch.Tensor): Query tensor to apply rotary embeddings.</span></span><br><span class="line"><span class="string">        xk (torch.Tensor): Key tensor to apply rotary embeddings.</span></span><br><span class="line"><span class="string">        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    xq_ = torch.view_as_complex(xq.<span class="built_in">float</span>().reshape(*xq.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">    xk_ = torch.view_as_complex(xk.<span class="built_in">float</span>().reshape(*xk.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)</span><br><span class="line">    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(<span class="number">3</span>)</span><br><span class="line">    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> xq_out.type_as(xq), xk_out.type_as(xk)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">repeat_kv</span>(<span class="params">x: torch.Tensor, n_rep: <span class="built_in">int</span></span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;torch.repeat_interleave(x, dim=2, repeats=n_rep)&quot;&quot;&quot;</span></span><br><span class="line">    bs, slen, n_kv_heads, head_dim = x.shape</span><br><span class="line">    <span class="keyword">if</span> n_rep == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        x[:, :, :, <span class="literal">None</span>, :]</span><br><span class="line">        .expand(bs, slen, n_kv_heads, n_rep, head_dim)</span><br><span class="line">        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multi-head attention module.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args: ModelArgs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize the Attention module.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            args (ModelArgs): Model configuration parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Attributes:</span></span><br><span class="line"><span class="string">            n_kv_heads (int): Number of key and value heads.</span></span><br><span class="line"><span class="string">            n_local_heads (int): Number of local query heads.</span></span><br><span class="line"><span class="string">            n_local_kv_heads (int): Number of local key and value heads.</span></span><br><span class="line"><span class="string">            n_rep (int): Number of repetitions for local heads.</span></span><br><span class="line"><span class="string">            head_dim (int): Dimension size of each attention head.</span></span><br><span class="line"><span class="string">            wq (ColumnParallelLinear): Linear transformation for queries.</span></span><br><span class="line"><span class="string">            wk (ColumnParallelLinear): Linear transformation for keys.</span></span><br><span class="line"><span class="string">            wv (ColumnParallelLinear): Linear transformation for values.</span></span><br><span class="line"><span class="string">            wo (RowParallelLinear): Linear transformation for output.</span></span><br><span class="line"><span class="string">            cache_k (torch.Tensor): Cached keys for attention.</span></span><br><span class="line"><span class="string">            cache_v (torch.Tensor): Cached values for attention.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.n_kv_heads = args.n_heads <span class="keyword">if</span> args.n_kv_heads <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> args.n_kv_heads</span><br><span class="line">        model_parallel_size = fs_init.get_model_parallel_world_size()</span><br><span class="line">        <span class="variable language_">self</span>.n_local_heads = args.n_heads // model_parallel_size</span><br><span class="line">        <span class="variable language_">self</span>.n_local_kv_heads = <span class="variable language_">self</span>.n_kv_heads // model_parallel_size</span><br><span class="line">        <span class="variable language_">self</span>.n_rep = <span class="variable language_">self</span>.n_local_heads // <span class="variable language_">self</span>.n_local_kv_heads</span><br><span class="line">        <span class="variable language_">self</span>.head_dim = args.dim // args.n_heads</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.wq = ColumnParallelLinear(</span><br><span class="line">            args.dim,</span><br><span class="line">            args.n_heads * <span class="variable language_">self</span>.head_dim,</span><br><span class="line">            bias=<span class="literal">False</span>,</span><br><span class="line">            gather_output=<span class="literal">False</span>,</span><br><span class="line">            init_method=<span class="keyword">lambda</span> x: x,</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.wk = ColumnParallelLinear(</span><br><span class="line">            args.dim,</span><br><span class="line">            <span class="variable language_">self</span>.n_kv_heads * <span class="variable language_">self</span>.head_dim,</span><br><span class="line">            bias=<span class="literal">False</span>,</span><br><span class="line">            gather_output=<span class="literal">False</span>,</span><br><span class="line">            init_method=<span class="keyword">lambda</span> x: x,</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.wv = ColumnParallelLinear(</span><br><span class="line">            args.dim,</span><br><span class="line">            <span class="variable language_">self</span>.n_kv_heads * <span class="variable language_">self</span>.head_dim,</span><br><span class="line">            bias=<span class="literal">False</span>,</span><br><span class="line">            gather_output=<span class="literal">False</span>,</span><br><span class="line">            init_method=<span class="keyword">lambda</span> x: x,</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.wo = RowParallelLinear(</span><br><span class="line">            args.n_heads * <span class="variable language_">self</span>.head_dim,</span><br><span class="line">            args.dim,</span><br><span class="line">            bias=<span class="literal">False</span>,</span><br><span class="line">            input_is_parallel=<span class="literal">True</span>,</span><br><span class="line">            init_method=<span class="keyword">lambda</span> x: x,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.cache_k = torch.zeros(</span><br><span class="line">            (</span><br><span class="line">                args.max_batch_size,</span><br><span class="line">                args.max_seq_len,</span><br><span class="line">                <span class="variable language_">self</span>.n_local_kv_heads,</span><br><span class="line">                <span class="variable language_">self</span>.head_dim,</span><br><span class="line">            )</span><br><span class="line">        ).cuda()</span><br><span class="line">        <span class="variable language_">self</span>.cache_v = torch.zeros(</span><br><span class="line">            (</span><br><span class="line">                args.max_batch_size,</span><br><span class="line">                args.max_seq_len,</span><br><span class="line">                <span class="variable language_">self</span>.n_local_kv_heads,</span><br><span class="line">                <span class="variable language_">self</span>.head_dim,</span><br><span class="line">            )</span><br><span class="line">        ).cuda()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        x: torch.Tensor,</span></span><br><span class="line"><span class="params">        start_pos: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        freqs_cis: torch.Tensor,</span></span><br><span class="line"><span class="params">        mask: <span class="type">Optional</span>[torch.Tensor],</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Forward pass of the attention module.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (torch.Tensor): Input tensor.</span></span><br><span class="line"><span class="string">            start_pos (int): Starting position for caching.</span></span><br><span class="line"><span class="string">            freqs_cis (torch.Tensor): Precomputed frequency tensor.</span></span><br><span class="line"><span class="string">            mask (torch.Tensor, optional): Attention mask tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            torch.Tensor: Output tensor after attention.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        bsz, seqlen, _ = x.shape</span><br><span class="line">        xq, xk, xv = <span class="variable language_">self</span>.wq(x), <span class="variable language_">self</span>.wk(x), <span class="variable language_">self</span>.wv(x)</span><br><span class="line"></span><br><span class="line">        xq = xq.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        xk = xk.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_kv_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        xv = xv.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_kv_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.cache_k = <span class="variable language_">self</span>.cache_k.to(xq)</span><br><span class="line">        <span class="variable language_">self</span>.cache_v = <span class="variable language_">self</span>.cache_v.to(xq)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.cache_k[:bsz, start_pos : start_pos + seqlen] = xk</span><br><span class="line">        <span class="variable language_">self</span>.cache_v[:bsz, start_pos : start_pos + seqlen] = xv</span><br><span class="line"></span><br><span class="line">        keys = <span class="variable language_">self</span>.cache_k[:bsz, : start_pos + seqlen]</span><br><span class="line">        values = <span class="variable language_">self</span>.cache_v[:bsz, : start_pos + seqlen]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># repeat k/v heads if n_kv_heads &lt; n_heads</span></span><br><span class="line">        keys = repeat_kv(keys, <span class="variable language_">self</span>.n_rep)  <span class="comment"># (bs, seqlen, n_local_heads, head_dim)</span></span><br><span class="line">        values = repeat_kv(values, <span class="variable language_">self</span>.n_rep)  <span class="comment"># (bs, seqlen, n_local_heads, head_dim)</span></span><br><span class="line"></span><br><span class="line">        xq = xq.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># (bs, n_local_heads, seqlen, head_dim)</span></span><br><span class="line">        keys = keys.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        values = values.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        scores = torch.matmul(xq, keys.transpose(<span class="number">2</span>, <span class="number">3</span>)) / math.sqrt(<span class="variable language_">self</span>.head_dim)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = scores + mask  <span class="comment"># (bs, n_local_heads, seqlen, cache_len + seqlen)</span></span><br><span class="line">        scores = F.softmax(scores.<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq)</span><br><span class="line">        output = torch.matmul(scores, values)  <span class="comment"># (bs, n_local_heads, seqlen, head_dim)</span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(bsz, seqlen, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.wo(output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        dim: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        hidden_dim: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        multiple_of: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        ffn_dim_multiplier: <span class="type">Optional</span>[<span class="built_in">float</span>],</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize the FeedForward module.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dim (int): Input dimension.</span></span><br><span class="line"><span class="string">            hidden_dim (int): Hidden dimension of the feedforward layer.</span></span><br><span class="line"><span class="string">            multiple_of (int): Value to ensure hidden dimension is a multiple of this value.</span></span><br><span class="line"><span class="string">            ffn_dim_multiplier (float, optional): Custom multiplier for hidden dimension. Defaults to None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Attributes:</span></span><br><span class="line"><span class="string">            w1 (ColumnParallelLinear): Linear transformation for the first layer.</span></span><br><span class="line"><span class="string">            w2 (RowParallelLinear): Linear transformation for the second layer.</span></span><br><span class="line"><span class="string">            w3 (ColumnParallelLinear): Linear transformation for the third layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        hidden_dim = <span class="built_in">int</span>(<span class="number">2</span> * hidden_dim / <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># custom dim factor multiplier</span></span><br><span class="line">        <span class="keyword">if</span> ffn_dim_multiplier <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            hidden_dim = <span class="built_in">int</span>(ffn_dim_multiplier * hidden_dim)</span><br><span class="line">        hidden_dim = multiple_of * ((hidden_dim + multiple_of - <span class="number">1</span>) // multiple_of)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.w1 = ColumnParallelLinear(</span><br><span class="line">            dim, hidden_dim, bias=<span class="literal">False</span>, gather_output=<span class="literal">False</span>, init_method=<span class="keyword">lambda</span> x: x</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.w2 = RowParallelLinear(</span><br><span class="line">            hidden_dim, dim, bias=<span class="literal">False</span>, input_is_parallel=<span class="literal">True</span>, init_method=<span class="keyword">lambda</span> x: x</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.w3 = ColumnParallelLinear(</span><br><span class="line">            dim, hidden_dim, bias=<span class="literal">False</span>, gather_output=<span class="literal">False</span>, init_method=<span class="keyword">lambda</span> x: x</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.w2(F.silu(<span class="variable language_">self</span>.w1(x)) * <span class="variable language_">self</span>.w3(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer_id: <span class="built_in">int</span>, args: ModelArgs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize a TransformerBlock.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            layer_id (int): Identifier for the layer.</span></span><br><span class="line"><span class="string">            args (ModelArgs): Model configuration parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Attributes:</span></span><br><span class="line"><span class="string">            n_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">            dim (int): Dimension size of the model.</span></span><br><span class="line"><span class="string">            head_dim (int): Dimension size of each attention head.</span></span><br><span class="line"><span class="string">            attention (Attention): Attention module.</span></span><br><span class="line"><span class="string">            feed_forward (FeedForward): FeedForward module.</span></span><br><span class="line"><span class="string">            layer_id (int): Identifier for the layer.</span></span><br><span class="line"><span class="string">            attention_norm (RMSNorm): Layer normalization for attention output.</span></span><br><span class="line"><span class="string">            ffn_norm (RMSNorm): Layer normalization for feedforward output.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.n_heads = args.n_heads</span><br><span class="line">        <span class="variable language_">self</span>.dim = args.dim</span><br><span class="line">        <span class="variable language_">self</span>.head_dim = args.dim // args.n_heads</span><br><span class="line">        <span class="variable language_">self</span>.attention = Attention(args)</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = FeedForward(</span><br><span class="line">            dim=args.dim,</span><br><span class="line">            hidden_dim=<span class="number">4</span> * args.dim,</span><br><span class="line">            multiple_of=args.multiple_of,</span><br><span class="line">            ffn_dim_multiplier=args.ffn_dim_multiplier,</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.layer_id = layer_id</span><br><span class="line">        <span class="variable language_">self</span>.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)</span><br><span class="line">        <span class="variable language_">self</span>.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        x: torch.Tensor,</span></span><br><span class="line"><span class="params">        start_pos: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        freqs_cis: torch.Tensor,</span></span><br><span class="line"><span class="params">        mask: <span class="type">Optional</span>[torch.Tensor],</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Perform a forward pass through the TransformerBlock.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (torch.Tensor): Input tensor.</span></span><br><span class="line"><span class="string">            start_pos (int): Starting position for attention caching.</span></span><br><span class="line"><span class="string">            freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies.</span></span><br><span class="line"><span class="string">            mask (torch.Tensor, optional): Masking tensor for attention. Defaults to None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            torch.Tensor: Output tensor after applying attention and feedforward layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        h = x + <span class="variable language_">self</span>.attention.forward(</span><br><span class="line">            <span class="variable language_">self</span>.attention_norm(x), start_pos, freqs_cis, mask</span><br><span class="line">        )</span><br><span class="line">        out = h + <span class="variable language_">self</span>.feed_forward.forward(<span class="variable language_">self</span>.ffn_norm(h))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params: ModelArgs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize a Transformer model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            params (ModelArgs): Model configuration parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Attributes:</span></span><br><span class="line"><span class="string">            params (ModelArgs): Model configuration parameters.</span></span><br><span class="line"><span class="string">            vocab_size (int): Vocabulary size.</span></span><br><span class="line"><span class="string">            n_layers (int): Number of layers in the model.</span></span><br><span class="line"><span class="string">            tok_embeddings (ParallelEmbedding): Token embeddings.</span></span><br><span class="line"><span class="string">            layers (torch.nn.ModuleList): List of Transformer blocks.</span></span><br><span class="line"><span class="string">            norm (RMSNorm): Layer normalization for the model output.</span></span><br><span class="line"><span class="string">            output (ColumnParallelLinear): Linear layer for final output.</span></span><br><span class="line"><span class="string">            freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.params = params</span><br><span class="line">        <span class="variable language_">self</span>.vocab_size = params.vocab_size</span><br><span class="line">        <span class="variable language_">self</span>.n_layers = params.n_layers</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.tok_embeddings = ParallelEmbedding(</span><br><span class="line">            params.vocab_size, params.dim, init_method=<span class="keyword">lambda</span> x: x</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.layers = torch.nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> layer_id <span class="keyword">in</span> <span class="built_in">range</span>(params.n_layers):</span><br><span class="line">            <span class="variable language_">self</span>.layers.append(TransformerBlock(layer_id, params))</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.norm = RMSNorm(params.dim, eps=params.norm_eps)</span><br><span class="line">        <span class="variable language_">self</span>.output = ColumnParallelLinear(</span><br><span class="line">            params.dim, params.vocab_size, bias=<span class="literal">False</span>, init_method=<span class="keyword">lambda</span> x: x</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.freqs_cis = precompute_freqs_cis(</span><br><span class="line">            <span class="variable language_">self</span>.params.dim // <span class="variable language_">self</span>.params.n_heads, <span class="variable language_">self</span>.params.max_seq_len * <span class="number">2</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.inference_mode()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens: torch.Tensor, start_pos: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Perform a forward pass through the Transformer model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            tokens (torch.Tensor): Input token indices.</span></span><br><span class="line"><span class="string">            start_pos (int): Starting position for attention caching.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            torch.Tensor: Output logits after applying the Transformer model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        _bsz, seqlen = tokens.shape</span><br><span class="line">        h = <span class="variable language_">self</span>.tok_embeddings(tokens)</span><br><span class="line">        <span class="variable language_">self</span>.freqs_cis = <span class="variable language_">self</span>.freqs_cis.to(h.device)</span><br><span class="line">        freqs_cis = <span class="variable language_">self</span>.freqs_cis[start_pos : start_pos + seqlen]</span><br><span class="line"></span><br><span class="line">        mask = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> seqlen &gt; <span class="number">1</span>:</span><br><span class="line">            mask = torch.full(</span><br><span class="line">                (<span class="number">1</span>, <span class="number">1</span>, seqlen, seqlen), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>), device=tokens.device</span><br><span class="line">            )</span><br><span class="line">            mask = torch.triu(mask, diagonal=start_pos + <span class="number">1</span>).type_as(h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            h = layer(h, start_pos, freqs_cis, mask)</span><br><span class="line">        h = <span class="variable language_">self</span>.norm(h)</span><br><span class="line">        output = <span class="variable language_">self</span>.output(h).<span class="built_in">float</span>()</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<hr>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLaMA/">LLaMA</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2025/03/07/u2SEqVOMvexjFKJ.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/e080db9.html" title="LLaMA3 自学笔记"><img class="cover" src="https://s2.loli.net/2025/03/07/u2SEqVOMvexjFKJ.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">LLaMA3 自学笔记</div></div></a></div><div class="next-post pull-right"><a href="/post/1c1af71d.html" title="LLaMA1 自学笔记"><img class="cover" src="https://s2.loli.net/2025/03/07/u2SEqVOMvexjFKJ.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">LLaMA1 自学笔记</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/post/1c1af71d.html" title="LLaMA1 自学笔记"><img class="cover" src="https://s2.loli.net/2025/03/07/u2SEqVOMvexjFKJ.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-07</div><div class="title">LLaMA1 自学笔记</div></div></a></div><div><a href="/post/e080db9.html" title="LLaMA3 自学笔记"><img class="cover" src="https://s2.loli.net/2025/03/07/u2SEqVOMvexjFKJ.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-10</div><div class="title">LLaMA3 自学笔记</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2024/07/16/lsEXfWtGT6eRu7k.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">w434</div><div class="author-info__description">An undergraduate majoring in AI at PKU.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">120</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">45</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">30</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/w434"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/w434" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcom to my blog.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#LLaMA2%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">LLaMA2模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%BB%84%E6%9F%A5%E8%AF%A2%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.1.</span> <span class="toc-text">分组查询注意力</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%91%E4%BC%BC%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%EF%BC%88Proximal-Policy-Optimization-PPO%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">近似策略优化（Proximal Policy Optimization, PPO）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8EClip%E7%9A%84PPO"><span class="toc-number">1.2.1.</span> <span class="toc-text">基于Clip的PPO</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%92%E7%BB%9D%E9%87%87%E6%A0%B7%EF%BC%88Rejection-Sampling%EF%BC%89"><span class="toc-number">1.3.</span> <span class="toc-text">拒绝采样（Rejection Sampling）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%8B%92%E7%BB%9D%E9%87%87%E6%A0%B7%EF%BC%88Rejection-Sampling%EF%BC%89"><span class="toc-number">1.3.1.</span> <span class="toc-text">什么是拒绝采样（Rejection Sampling）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8B%92%E7%BB%9D%E9%87%87%E6%A0%B7%E7%9A%84%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-number">1.3.2.</span> <span class="toc-text">拒绝采样的核心思想</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#LLaMA2-model-py"><span class="toc-number">2.</span> <span class="toc-text">LLaMA2 model.py</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Grouped-Query-Attention-GQA-%E7%9A%84%E5%BC%95%E5%85%A5"><span class="toc-number">2.1.</span> <span class="toc-text">Grouped Query Attention (GQA) 的引入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FFN%E5%B1%82%E5%8A%A8%E6%80%81%E7%BB%B4%E5%BA%A6%E8%B0%83%E6%95%B4"><span class="toc-number">2.2.</span> <span class="toc-text">FFN层动态维度调整</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88RoPE%EF%BC%89%E4%BC%98%E5%8C%96"><span class="toc-number">2.3.</span> <span class="toc-text">旋转位置编码（RoPE）优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C%E4%B8%8E%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">2.4.</span> <span class="toc-text">模型并行与参数初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96"><span class="toc-number">2.5.</span> <span class="toc-text">推理优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A9%E7%A0%81%E7%94%9F%E6%88%90%E4%BC%98%E5%8C%96"><span class="toc-number">2.6.</span> <span class="toc-text">掩码生成优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E6%9E%90"><span class="toc-number">2.7.</span> <span class="toc-text">关键模块解析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-number">2.8.</span> <span class="toc-text">完整代码</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/cb50706b.html" title="智能机器人概论 07 运动控制"><img src="https://s2.loli.net/2025/09/18/dOm6PbFqzIQnipV.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="智能机器人概论 07 运动控制"/></a><div class="content"><a class="title" href="/post/cb50706b.html" title="智能机器人概论 07 运动控制">智能机器人概论 07 运动控制</a><time datetime="2025-09-27T07:42:00.000Z" title="Created 2025-09-27 15:42:00">2025-09-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/d21bb118.html" title="智能机器人概论 05 激光雷达"><img src="https://s2.loli.net/2025/09/18/dOm6PbFqzIQnipV.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="智能机器人概论 05 激光雷达"/></a><div class="content"><a class="title" href="/post/d21bb118.html" title="智能机器人概论 05 激光雷达">智能机器人概论 05 激光雷达</a><time datetime="2025-09-27T07:27:00.000Z" title="Created 2025-09-27 15:27:00">2025-09-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/e12e9f40.html" title="智能机器人概论 04 惯性传感器"><img src="https://s2.loli.net/2025/09/18/dOm6PbFqzIQnipV.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="智能机器人概论 04 惯性传感器"/></a><div class="content"><a class="title" href="/post/e12e9f40.html" title="智能机器人概论 04 惯性传感器">智能机器人概论 04 惯性传感器</a><time datetime="2025-09-27T06:48:00.000Z" title="Created 2025-09-27 14:48:00">2025-09-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/dfe68b41.html" title="智能机器人概论 03 运动传感器与位姿估计"><img src="https://s2.loli.net/2025/09/18/dOm6PbFqzIQnipV.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="智能机器人概论 03 运动传感器与位姿估计"/></a><div class="content"><a class="title" href="/post/dfe68b41.html" title="智能机器人概论 03 运动传感器与位姿估计">智能机器人概论 03 运动传感器与位姿估计</a><time datetime="2025-09-18T06:25:00.000Z" title="Created 2025-09-18 14:25:00">2025-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/c8c0e06c.html" title="智能机器人概论 02 机器人传感器"><img src="https://s2.loli.net/2025/09/18/dOm6PbFqzIQnipV.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="智能机器人概论 02 机器人传感器"/></a><div class="content"><a class="title" href="/post/c8c0e06c.html" title="智能机器人概论 02 机器人传感器">智能机器人概论 02 机器人传感器</a><time datetime="2025-09-18T05:50:00.000Z" title="Created 2025-09-18 13:50:00">2025-09-18</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By w434</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcom to my blog.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">简</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>