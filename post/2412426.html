<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model | w434's blog</title><meta name="author" content="w434"><meta name="copyright" content="w434"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model(视觉自回归多模态大语言模型中的统一理解与生成)Abstract VARGPT是一种新颖的多模态大语言模型（MLLM），它在单一的自回归框架内统一了视觉理解和生成  VARGPT采用下一个tok">
<meta property="og:type" content="article">
<meta property="og:title" content="VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model">
<meta property="og:url" content="http://example.com/post/2412426.html">
<meta property="og:site_name" content="w434&#39;s blog">
<meta property="og:description" content="VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model(视觉自回归多模态大语言模型中的统一理解与生成)Abstract VARGPT是一种新颖的多模态大语言模型（MLLM），它在单一的自回归框架内统一了视觉理解和生成  VARGPT采用下一个tok">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2025/03/23/mwWIq2fTsbhSEYu.jpg">
<meta property="article:published_time" content="2025-03-23T09:33:00.000Z">
<meta property="article:modified_time" content="2025-03-23T13:18:46.148Z">
<meta property="article:author" content="w434">
<meta property="article:tag" content="VARGPT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2025/03/23/mwWIq2fTsbhSEYu.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/post/2412426.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-23 21:18:46'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2024/07/16/lsEXfWtGT6eRu7k.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">108</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">38</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">25</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/notes/"><i class="fa-fw fa-solid fa-school"></i><span> 课程笔记</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-face-smile"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/matches/"><i class="fa-fw fa-solid fa-futbol"></i><span> 足球赛事</span></a></li><li><a class="site-page child" href="/chess/"><i class="fa-fw fa-solid fa-chess"></i><span> 国际象棋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw fa-solid fa-film"></i><span> 影评&amp;演出评论&amp;书评</span></a></li><li><a class="site-page child" href="/jottings/"><i class="fa-fw fa-solid fa-book"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/varietyShow/"><i class="fa-fw fa-solid fa-tv"></i><span> 综艺</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/musics/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2025/03/23/mwWIq2fTsbhSEYu.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="w434's blog"><span class="site-name">w434's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/notes/"><i class="fa-fw fa-solid fa-school"></i><span> 课程笔记</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-face-smile"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/matches/"><i class="fa-fw fa-solid fa-futbol"></i><span> 足球赛事</span></a></li><li><a class="site-page child" href="/chess/"><i class="fa-fw fa-solid fa-chess"></i><span> 国际象棋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw fa-solid fa-film"></i><span> 影评&amp;演出评论&amp;书评</span></a></li><li><a class="site-page child" href="/jottings/"><i class="fa-fw fa-solid fa-book"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/varietyShow/"><i class="fa-fw fa-solid fa-tv"></i><span> 综艺</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/musics/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-03-23T09:33:00.000Z" title="Created 2025-03-23 17:33:00">2025-03-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-03-23T13:18:46.148Z" title="Updated 2025-03-23 21:18:46">2025-03-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="VARGPT-Unified-Understanding-and-Generation-in-a-Visual-Autoregressive-Multimodal-Large-Language-Model-视觉自回归多模态大语言模型中的统一理解与生成"><a href="#VARGPT-Unified-Understanding-and-Generation-in-a-Visual-Autoregressive-Multimodal-Large-Language-Model-视觉自回归多模态大语言模型中的统一理解与生成" class="headerlink" title="VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model(视觉自回归多模态大语言模型中的统一理解与生成)"></a>VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model(视觉自回归多模态大语言模型中的统一理解与生成)</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><p>VARGPT是一种新颖的多模态大语言模型（MLLM），它在单一的自回归框架内统一了视觉理解和生成</p>
</li>
<li><p>VARGPT采用下一个token预测范式进行视觉理解，采用下一个尺度预测范式进行视觉自回归生成</p>
</li>
<li><p>VARGPT创新性地扩展了LLaVA架构，在MLLMs中实现了高效的尺度自回归视觉生成，同时无缝地支持单一模型框架内的混合模态输入和输出</p>
</li>
<li><p>VARGPT经过三个阶段的一体化训练过程，包括预训练阶段和两个混合视觉指令微调阶段</p>
</li>
<li><p>统一训练策略旨在实现视觉和文本特征的对齐，增强理解和生成的指令跟随能力，并提高视觉生成质量</p>
</li>
<li><p>VARGPT自然地支持自回归视觉生成和指令到图像合成的能力，展示了其在视觉理解和生成任务中的多功能性</p>
</li>
</ul>
<hr>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul>
<li><p>近年来，多模态人工智能在理解和生成这两个核心领域取得了重大突破。多模态大语言模型（MLLMs）通过利用LLMs的强大泛化能力，展示了在多模态数据理解方面的卓越能力</p>
</li>
<li><p>去噪扩散概率模型（DDPMs）在图像生成领域带来了重大进展，在文本到视觉模态生成方面取得了卓越的性能</p>
</li>
<li><p>受自回归LLMs的有利特性启发，如scaling law，许多工作探索了通过预测下一个token或下一个尺度进行自回归视觉生成</p>
</li>
<li><p>鉴于这些在视觉理解和生成方面的成就，最近的研究一直在探索能够处理理解和生成的统一模型，从而设计了各种统一架构以实现这一目标</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/FtbIfngGhMwiLy2.png" alt="image.png"></p>
<ul>
<li>最近的研究尝试将来自这两个不同领域的模型（例如，LLMs和DDPMs）组合起来，形成一个能够处理多模态理解和生成的统一系统</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/7DMlce8u4NTdwfK.png" alt="image.png"></p>
<ul>
<li><p>我们致力于在视觉自回归MLLM中统一视觉生成和理解，自然地支持混合模态输入和输出</p>
</li>
<li><p>与所有现有的统一模型不同，我们提出在统一模型中将理解和生成建模为两个不同的范式：分别预测下一个token进行视觉理解，预测下一个尺度进行视觉生成，并训练一个名为VARGPT的新型统一模型：</p>
</li>
<li><p>在模型架构方面，我们的VARGPT的核心结构借鉴了LLaVA-1.5-7B，同时我们还额外加入了一个视觉解码器和两个额外的视觉特征投影器用于视觉生成</p>
</li>
<li><p>VARGPT采用自回归方法来预测下一个文本token以进行视觉理解和问答。当预测到用于视觉生成的特殊token时，模型自回归地预测下一个尺度的token，并通过视觉解码器获得最终的输出图像。所提出的架构使VARGPT能够在视觉自回归MLLM中实现统一的理解和生成。</p>
</li>
<li><p>在训练方法方面，我们采用统一的指令微调来学习视觉理解和视觉生成。通过构建视觉token预测作为指令跟随格式，将指令微调扩展到视觉生成，并将构建的视觉生成指令数据集与来自LLaVA-1.5的多轮对话指令数据集结合进行混合训练</p>
</li>
<li><p>通过提出的统一指令微调，我们同时赋予MLLMs理解和生成能力</p>
</li>
<li><p>训练过程分为三个阶段，包括一个预训练阶段和两个指令微调阶段：</p>
<ol>
<li>在预训练的第一阶段，模型学习文本和视觉空间之间的特征映射</li>
<li>在第二和第三阶段的指令微调中，VARGPT分别增强了其在视觉问答和指令到图像生成方面的能力</li>
</ol>
</li>
<li><p>关于训练数据集，通过统一的指令跟随格式，我们在混合视觉指令微调中统一了理解和生成的训练</p>
</li>
<li><p>VARGPT能够实现显著的视觉理解能力，并赋予MLLMs视觉生成能力，自然地支持混合模态输入和输出</p>
</li>
<li><p>VARGPT是第一个支持预测下一个token进行理解任务和预测下一个尺度进行生成任务的统一模型，同时在理解能力方面超越了众多规模相当的MLLMs和统一模型</p>
</li>
<li><p>主要贡献有三点：</p>
<ol>
<li>探索了一种新颖的自回归视觉理解和生成的统一架构，该架构采用下一个token范式进行视觉理解，采用下一个尺度范式进行视觉生成</li>
<li>开发了VARGPT，一个支持混合模态输入和输出的统一模型，通过新颖的架构、提出的三阶段训练策略和386万样本的统一指令微调数据集</li>
<li>VARGPT在众多多模态理解和以视觉为中心的基准测试中表现出色，同时在自回归文本到图像合成方面展示了卓越的能力</li>
</ol>
</li>
</ul>
<hr>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><h3 id="Visual-Generation"><a href="#Visual-Generation" class="headerlink" title="Visual Generation"></a>Visual Generation</h3><ul>
<li><p>扩散模型将图像生成视为从噪声到图像的反向扩散过程</p>
</li>
<li><p>扩散模型的进展主要集中在采样方法和架构设计，从而产生了令人印象深刻的模型</p>
</li>
<li><p>在扩散模型取得显著进展的背景下，基于流的生成模型作为一种简化框架出现，推动了先进视觉生成模型的发展</p>
</li>
<li><p>自回归模型采用GPT风格的技术来预测序列中的下一个token，一些工作使用类似于VQGAN的视觉tokenizer将图像转换为离散token，从而实现对视觉数据的token化，并采用类似于GPT风格的预测方法</p>
</li>
<li><p>另一类基于预测下一个尺度的自回归模型，如VAR、HART和Infinity，引起了关注，并已被验证可能具有与scaling law一致的性质</p>
</li>
<li><p>在本工作中，我们的统一自回归框架通过预测下一个尺度的范式完成了图像生成任务</p>
</li>
</ul>
<h3 id="Multimodel-Large-Language-Model"><a href="#Multimodel-Large-Language-Model" class="headerlink" title="Multimodel Large Language Model"></a>Multimodel Large Language Model</h3><ul>
<li><p>LLMs的进展推动了MLLMs的发展</p>
</li>
<li><p>MLLMs利用预训练的LLMs作为文本解码器，通过连接器将文本和图像与视觉编码器集成</p>
</li>
<li><p>LLaVA使用来自各种任务（如视觉问答和图像描述）的数据以指令格式微调模型，使模型能够理解新指令并泛化到未见过的任务</p>
</li>
<li><p>LLaVA-1.5和LLaVA-NeXT系列通过更多样化和更高质量的数据进一步提升了视觉理解性能</p>
</li>
<li><p>通过架构优化、创新训练范式和引入多样化数据，一系列先进的MLLMs应运而生，如Qwen-VL、mPLUG-Owl2、InternVL、InstructBLIP</p>
</li>
</ul>
<h3 id="Unified-Models-For-Visual-Understanding-and-Generation"><a href="#Unified-Models-For-Visual-Understanding-and-Generation" class="headerlink" title="Unified Models For Visual Understanding and Generation"></a>Unified Models For Visual Understanding and Generation</h3><ul>
<li><p>近年来，研究人员致力于在模型中统一理解和生成能力</p>
</li>
<li><p>大多数现有方法尝试将预训练的扩散模型与现有系统集成。然而这些系统本质上将扩散模型视为外部工具，而不是将其作为MLLMs的内在生成能力</p>
</li>
<li><p>Show-o通过结合自回归和（离散）扩散建模，能够自适应地处理各种混合模态的输入和输出</p>
</li>
<li><p>Li等人采用跨模态最大似然估计框架，显著改进了现有的基于扩散的多模态模型</p>
</li>
<li><p>LWM和Chameleon利用VQ tokenizer对图像进行编码，从而同时支持多模态理解和生成</p>
</li>
<li><p>Janus通过将视觉编码解耦为单独的路径进一步增强了模型的灵活性和性能</p>
</li>
<li><p>Dual Diffusion研究了使用两个扩散模型进行理解和生成</p>
</li>
<li><p>Liquid在同一空间中学习图像和文本嵌入，并使用预测下一个token的范式实现自回归视觉理解和生成</p>
</li>
<li><p>与所有现有统一模型不同，我们提出在统一模型中将理解和生成建模为两个不同的范式：分别预测下一个token进行视觉理解，预测下一个尺度进行视觉生成</p>
</li>
</ul>
<hr>
<h2 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3. Methodology"></a>3. Methodology</h2><h3 id="3-1-Model-Architecture"><a href="#3-1-Model-Architecture" class="headerlink" title="3.1. Model Architecture"></a>3.1. Model Architecture</h3><ul>
<li>VARGPT统一了视觉理解和生成，其架构下图所示</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/tTLfM479gwoaGhV.png" alt="image.png"></p>
<ul>
<li>架构遵循预测下一个token进行理解和问答的范式，并遵循预测下一个尺度进行图像生成的范式</li>
</ul>
<h4 id="Visual-understanding-via-next-token-prediction"><a href="#Visual-understanding-via-next-token-prediction" class="headerlink" title="Visual understanding via next-token prediction"></a>Visual understanding via next-token prediction</h4><ul>
<li><p>对于视觉理解，我们的模型架构参考了LLaVA-1.5的结构，使用Vicuna-7B-v1.5作为LLM θ，并采用CLIP的视觉编码器（ViT&#x2F;14）作为视觉编码器，以及一个两层的线性网络作为投影器</p>
</li>
<li><p>最初，用于视觉理解的图像Ximg通过视觉编码器处理以生成嵌入Himg，然后通过接口（例如线性层）进行修改，以与查询Xquery获得的文本嵌入Htxt对齐</p>
</li>
<li><p>组合后的数据作为LLM的输入，LLM自回归生成文本输出Ytxt：其中Y_t^{txt} 表示Ytxt的第t个token，Y_{&lt;t}{txt}表示在第t步之前生成的token序列</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/TCOJloj8ftUduiI.png" alt="image.png"></p>
<ul>
<li>为了保持LLM的因果注意力特性，我们对所有输入LLM的token应用因果注意力掩码，包括用于生成图像的token</li>
</ul>
<h4 id="Visual-generation-via-next-scale-prediction"><a href="#Visual-generation-via-next-scale-prediction" class="headerlink" title="Visual generation via next-scale prediction"></a>Visual generation via next-scale prediction</h4><ul>
<li><p>对于视觉生成，我们遵循VAR中的大多数设置，并采用多尺度图像tokenizer进行视觉token编码和解码</p>
</li>
<li><p>我们构建了两个图像生成投影器，用于在LLM的输入和输出处转换用于生成的视觉特征</p>
</li>
<li><p>我们构建了一个额外的2B视觉解码器φ，包含30层Transformer，用于解码视觉特征，这在一定程度上可以避免文本解码器中的知识与图像生成知识之间的冲突</p>
</li>
<li><p>通过视觉解码器获得的图像特征将通过多尺度VAE解码器进一步解码，以生成可用的图像</p>
</li>
<li><p>与文本解码器（即LLM）不同，视觉解码器使用遵循VAR中的块因果注意力的注意力机制，以支持预测下一个尺度的token</p>
</li>
<li><p>此外，在将用于视觉生成的特征输入视觉解码器之前，我们添加了绝对位置编码，以进一步区分视觉token的位置信息</p>
</li>
<li><p>形式上，我们将图像的多尺度特征图定义为(R1, R2, …, RK)，通过多尺度tokenizer获得。因此，下一个尺度的图像token将以自回归方式生成：</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/K9PVUDkev78b4ma.png" alt="image.png"></p>
<h4 id="Prompt-template-for-mixed-modal-generation"><a href="#Prompt-template-for-mixed-modal-generation" class="headerlink" title="Prompt template for mixed-modal generation"></a>Prompt template for mixed-modal generation</h4><ul>
<li><p>为了区分用于文本生成和图像合成的token，我们设计了一些特殊的token标记</p>
</li>
<li><p>我们使用<image_gen>来填充图像生成token的位置，<image_gen_start>表示图像生成token的开始，<image_gen_end>表示生成的结束</p>
</li>
<li><p>当VARGPT生成<image_gen_start>token时，与<image_gen>token相关的特征通过投影器处理，随后输入视觉解码器以生成图像生成所需的特征</p>
</li>
<li><p>在视觉理解任务中，我们使用&lt;’image’&gt; token作为输入图像的表示</p>
</li>
</ul>
<h4 id="Classifier-free-guidance-CFG-无分类器引导"><a href="#Classifier-free-guidance-CFG-无分类器引导" class="headerlink" title="Classifier-free guidance (CFG)(无分类器引导)"></a>Classifier-free guidance (CFG)(无分类器引导)</h4><ul>
<li><p>CFG显著增强了生成扩散模型生成高保真样本的能力</p>
</li>
<li><p>该方法将条件生成模型与同时训练的无条件模型的分布估计相结合，从而提高了生成的整体质量</p>
</li>
<li><p>受DALL-E 2、VAR和VAR-CLIP的启发，使用高斯噪声作为输入来模拟无条件生成。随后通过从条件生成的logits分布中减去无条件生成的概率来获得视觉输出的最终分布</p>
</li>
</ul>
<h3 id="3-2-Training"><a href="#3-2-Training" class="headerlink" title="3.2. Training"></a>3.2. Training</h3><ul>
<li>对于VARGPT模型训练，我们提出了一个阶段的预训练过程和两个阶段的指令微调过程，如下图所示</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/CRuaKEfTFniZSzV.png" alt="image.png"></p>
<h4 id="3-2-1-Stage-1-Pretraining"><a href="#3-2-1-Stage-1-Pretraining" class="headerlink" title="3.2.1. Stage-1: Pretraining"></a>3.2.1. Stage-1: Pretraining</h4><ul>
<li><p>使用来自ImageNet的图像作为图像源，构建用于预训练两个图像生成投影器的训练数据</p>
</li>
<li><p>将预训练数据构建为1.28M的单轮对话数据(具体见第4节)</p>
</li>
<li><p>此预训练阶段的主要目的是训练投影器，初步对齐图像生成特征与文本特征。在预训练期间，冻结了除两个图像生成投影器之外的所有参数</p>
</li>
</ul>
<h4 id="3-2-2-Stage-2-SFT-for-Visual-Understanding"><a href="#3-2-2-Stage-2-SFT-for-Visual-Understanding" class="headerlink" title="3.2.2. Stage-2: SFT for Visual Understanding"></a>3.2.2. Stage-2: SFT for Visual Understanding</h4><ul>
<li><p>在第二阶段，我们解冻了语言模型和视觉编码器特征输出的投影器，并使用我们策划的多轮对话和理解数据集进行训练</p>
</li>
<li><p>此阶段的主要目的是确保VARGPT保持出色的多轮对话、视觉理解和问答能力</p>
</li>
<li><p>在此阶段，我们引入了5K个来自我们构建的Imagenet-Instruct数据集的样本，使VARGPT能够区分视觉理解和视觉生成任务</p>
</li>
<li><p>当用户输入生成指令时，VARGPT可以通过输出特殊token <image_gen_start>来准确响应，以开始自回归视觉生成</p>
</li>
</ul>
<h4 id="3-2-3-Stage-3-SFT-for-Visual-Generation"><a href="#3-2-3-Stage-3-SFT-for-Visual-Generation" class="headerlink" title="3.2.3. Stage-3: SFT for Visual Generation"></a>3.2.3. Stage-3: SFT for Visual Generation</h4><ul>
<li><p>与第二阶段相比，第三阶段主要旨在通过监督微调提高VARGPT的指令到图像生成能力</p>
</li>
<li><p>在此阶段，我们解冻了视觉解码器和用于视觉生成的两个投影器，同时冻结其他参数以进行SFT</p>
</li>
<li><p>第三阶段的训练数据包括从ImageNet构建的1400K指令对（详见第4节）</p>
</li>
</ul>
<hr>
<h2 id="4-Unified-Instruction-following-Data"><a href="#4-Unified-Instruction-following-Data" class="headerlink" title="4. Unified Instruction-following Data"></a>4. Unified Instruction-following Data</h2><ul>
<li><p>本节描述了三个不同阶段使用的训练数据集的来源以及各种数据类型在其中的比例表示</p>
</li>
<li><p>值得注意的是，我们引入了图像生成指令跟随数据集，如下图所示，详细阐述了其来源以及通过应用LLMs生成的方法。通过这种方法，我们将视觉理解和生成的训练方法统一为视觉指令微调。</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/64LigEesQz8KMmb.png" alt="image.png"></p>
<h3 id="4-1-Generation-Instruction-following-Dataset"><a href="#4-1-Generation-Instruction-following-Dataset" class="headerlink" title="4.1. Generation Instruction-following Dataset"></a>4.1. Generation Instruction-following Dataset</h3><ul>
<li>我们构建了两个图像生成指令跟随数据集：ImageNet-Instruct-130K和ImageNet-Instruct-1270K</li>
</ul>
<h4 id="ImageNet-1K-VL-Enriched"><a href="#ImageNet-1K-VL-Enriched" class="headerlink" title="ImageNet-1K-VL-Enriched"></a>ImageNet-1K-VL-Enriched</h4><ul>
<li><p>使用ImageNet-1K-VL-Enriched数据集作为我们的基础数据集</p>
</li>
<li><p>ImageNet-1K-VL-Enriched是ImageNet数据集的增强版本，其中图像描述是使用BLIP2描述模型生成的</p>
</li>
</ul>
<h4 id="Constructing-ImageNet-Instruct-130K-through-Deepseek-LLM"><a href="#Constructing-ImageNet-Instruct-130K-through-Deepseek-LLM" class="headerlink" title="Constructing ImageNet-Instruct-130K through Deepseek-LLM"></a>Constructing ImageNet-Instruct-130K through Deepseek-LLM</h4><ul>
<li><p>为了构建指令微调数据集的问答格式，我们利用Deepseek-V3 Chat LLM（以下简称LLM）生成提示和答案的种子格式（Prompt_limit_seeds和Answer_limit_seeds）</p>
</li>
<li><p>如下图左所示，Prompt_limit_seeds有效地模拟了用户请求，而Answer_limit_seeds模拟了VLLM与用户之间的对话</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/6TrWzoc4I5DlupU.png" alt="image.png"></p>
<ul>
<li>我们从种子池中随机选择prompt_limit_seed、image_cap_limit_seed和answer_limit_seed作为LLM调用模板的元素</li>
</ul>
<h4 id="LLM-invocation-template-LLM调用模板"><a href="#LLM-invocation-template-LLM调用模板" class="headerlink" title="LLM invocation template(LLM调用模板)"></a>LLM invocation template(LLM调用模板)</h4><ul>
<li><p>我们从基础数据集中随机选择了4个图像描述样本作为4-shot示例，以指导大模型生成相应的对话样本，如上图右所示</p>
</li>
<li><p>在生成的提示和答案中添加了相关约束，以确保输出尽可能合规和多样化</p>
</li>
<li><p>随机采样了130K个图像描述数据样本，从而创建了130K个ImageNet图像生成指令微调数据集的样本，将其命名为ImageNet-Instruct-130K</p>
</li>
</ul>
<h3 id="4-2-Data-Composition-in-Three-Training-Stages"><a href="#4-2-Data-Composition-in-Three-Training-Stages" class="headerlink" title="4.2. Data Composition in Three Training Stages"></a>4.2. Data Composition in Three Training Stages</h3><h4 id="Stage-1"><a href="#Stage-1" class="headerlink" title="Stage-1"></a>Stage-1</h4><ul>
<li><p>用于阶段1预训练的ImageNet-Instruct-class数据集包含128万个单轮对话样本，这些样本来自ImageNet，专注于学习类别与图像之间的对应关系</p>
</li>
<li><p>假设类别为“鱼”，则格式如下：{‘prompt’: ‘请为我生成一张鱼的图像。’, ‘answer’: ‘生成的鱼的图像如下&lt;’image’&gt;}</p>
</li>
</ul>
<h4 id="Stage-2"><a href="#Stage-2" class="headerlink" title="Stage-2"></a>Stage-2</h4><ul>
<li>我们在阶段2中的混合指令微调数据集来自LLaVA-1.5、LLaVA-OneVision和ImageNet-Instruct-130K</li>
</ul>
<h4 id="Stage-3"><a href="#Stage-3" class="headerlink" title="Stage-3"></a>Stage-3</h4><ul>
<li><p>在阶段3中，除了构建的ImageNet-Instruct-130K外，我们还创建了一个更大的图像生成指令跟随数据集ImageNet-Instruct-1270K</p>
</li>
<li><p>与ImageNet-Instruct-130K相比，它具有更多样化的提示和答案模板（多达400个）。提示和答案的构建涉及将模板与描述直接连接</p>
</li>
</ul>
<hr>
<h2 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5.Experiments"></a>5.Experiments</h2><ul>
<li><p>对于用于图像生成任务的图像，我们统一将其调整为256x256像素，并应用与VAR中一致的预处理技术</p>
</li>
<li><p>对于用于视觉理解任务的图像，我们遵循LLaVA-1.5框架中建立的预处理协议</p>
</li>
<li><p>语言模型、视觉编码器和视觉特征映射器使用LLaVA-1.5-7B-hf架构进行初始化</p>
</li>
<li><p>视觉解码器使用VAR-d30参数进行初始化，包含约20亿个模型参数</p>
</li>
<li><p>VARGPT中的视觉生成特征映射器在预训练的第一阶段进行随机初始化并初步更新</p>
</li>
<li><p>采用类似于VAR的多尺度VQ-VAE进行图像token化，以促进下一个尺度预测范式</p>
</li>
<li><p>VARGPT模型的top-k和top-p采样参数分别设置为900和0.95。此外，CFG尺度参数配置为1.5</p>
</li>
<li><p>我们在11个基准测试上评估了VARGPT在视觉理解方面的有效性，包括学术任务导向的基准测试和最近为指令跟随MLLMs提出的基准测试</p>
</li>
<li><p>对于视觉生成评估，我们构建了一个包含50,000个文本指令的评估数据集，以评估模型的生成能力</p>
</li>
<li><p>使用CLIP分数来评估文本指令与生成图像之间的CLIP分数</p>
</li>
<li><p>使用Frechet Inception Distance (FID)度量来评估我们的VARGPT模型生成的图像样本的质量，该模型在ImageNet-1K数据集上进行了训练</p>
</li>
</ul>
<h3 id="5-1-MainResults"><a href="#5-1-MainResults" class="headerlink" title="5.1.MainResults"></a>5.1.MainResults</h3><h4 id="Evaluation-on-Multi-modal-Benchmarks"><a href="#Evaluation-on-Multi-modal-Benchmarks" class="headerlink" title="Evaluation on Multi-modal Benchmarks"></a>Evaluation on Multi-modal Benchmarks</h4><ul>
<li>进行了零样本多模态评估，并将我们的VARGPT与各种用于视觉理解的多模态模型进行了比较</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/89fLlqezNtwdsg3.png" alt="image.png"></p>
<ul>
<li>据结果有几个详细的观察：<ol>
<li>我们的方法在大多数现有的用于视觉理解的MLLMs基线（包括LLaVA-1.5 、MiniGPT-4、InstructBLIP和Qwen-VL）上显著优于它们。VARGPT在所有基准测试和一些视觉幻觉评估基准测试（即POPE）上实现了更高的性能，这证明了我们的方法在视觉生成方面的优越性和泛化能力</li>
<li>尽管我们的视觉理解核心架构与LLaVA-1.5相似，但我们的方法实现了显著更好的性能，并且还在单一大型模型中支持视觉生成能力</li>
</ol>
</li>
</ul>
<h4 id="Evaluation-on-visual-question-answering-tasks"><a href="#Evaluation-on-visual-question-answering-tasks" class="headerlink" title="Evaluation on visual question answering tasks"></a>Evaluation on visual question answering tasks</h4><ul>
<li>将各种视觉问答任务与现有方法进行了比较，结果如下表所示：</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/bGaYThMLHUD3eN7.png" alt="image.png"></p>
<ul>
<li>观察结果：<ol>
<li>VARGPT在大多数理解基准测试上始终取得最佳结果，超过了具有相同参数规模的用于视觉理解的MLLMs。进一步证明了我们的VARGPT的有效性</li>
<li>除了实现显著的理解能力（如在SciQA-img基准测试上比LLaVA-1.5高出12.2%），VARGPT还可以支持视觉生成能力</li>
</ol>
</li>
</ul>
<h4 id="Evaluation-on-Instruction-to-image-Task"><a href="#Evaluation-on-Instruction-to-image-Task" class="headerlink" title="Evaluation on Instruction-to-image Task"></a>Evaluation on Instruction-to-image Task</h4><ul>
<li><p>了评估我们的VARGPT的视觉生成能力，我们构建了一个基于指令的问答生成评估数据集，包含50,000个样本</p>
</li>
<li><p>该数据集中的指令描述来自ImageNet-1K图像描述，每个类别限制50个样本以确保类别之间的平衡表示</p>
</li>
<li><p>为了定量评估VARGPT的指令跟随能力，我们评估了两个关键指标：</p>
<ol>
<li>50,000个生成图像与ImageNet-1k数据集之间的FID分数</li>
<li>指令与生成图像之间的CLIP分数</li>
</ol>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/veUJgRqX5O8QBP7.png" alt="image.png"></p>
<ul>
<li><p>VARGPT能够生成高质量且紧密遵循给定指令的图像</p>
</li>
<li><p>值得注意的是，VARGPT展示了在单一对话中无缝集成文本描述和图像生成的能力，利用单一统一模型进行多模态输入和输出。这一能力进一步强调了VARGPT在统一视觉生成和理解任务中的独特优势</p>
</li>
<li><p>VARGPT使用的图像生成数据集（1.28M ImageNet）与其他统一模型（例如Show-1: 36M, VILA-U: 15M, Liquid: 30M图像）相比，规模较小且质量较低。因此，VARGPT的图像生成性能目前落后于这些方法</p>
</li>
<li><p>然而，通过数据扩展提高质量的潜力为未来的研究和开发提供了一个有前景的方向</p>
</li>
</ul>
<h3 id="5-2-Method-Analysis"><a href="#5-2-Method-Analysis" class="headerlink" title="5.2. Method Analysis"></a>5.2. Method Analysis</h3><ul>
<li>我们在模型参数、训练设置和数据效率方面对VARGPT进行了消融实验，以详细评估各个组件的有效性</li>
</ul>
<h4 id="Effect-of-the-Training-Strategies-on-Generation"><a href="#Effect-of-the-Training-Strategies-on-Generation" class="headerlink" title="Effect of the Training Strategies on Generation"></a>Effect of the Training Strategies on Generation</h4><ul>
<li>在我们的训练协议中省略任何一个阶段或组合阶段都会导致模型视觉生成性能的显著下降</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/g1HsDhr9iAoUp7I.png" alt="image.png"></p>
<ul>
<li><p>值得注意的是，移除第三阶段（涉及指令微调）会导致生成图像的质量和模型遵循给定指令的能力大幅下降</p>
</li>
<li><p>这些发现强调了每个训练阶段在提高模型视觉生成质量和文本到图像能力方面的关键作用</p>
</li>
<li><p>此外，我们进行了额外的实验，在第三阶段训练期间选择性地冻结映射器和视觉解码器参数。我们的观察表明，在第三阶段缺少这些组件的微调也会导致性能下降</p>
</li>
<li><p>这些结果为我们的三阶段训练策略的有效性提供了有力的证据，在各种消融场景中观察到的性能一致下降，强化了每个提议组件和阶段的重要性</p>
</li>
</ul>
<h4 id="Effect-of-the-Training-Strategies-on-Understanding"><a href="#Effect-of-the-Training-Strategies-on-Understanding" class="headerlink" title="Effect of the Training Strategies on Understanding"></a>Effect of the Training Strategies on Understanding</h4><ul>
<li><p>为了评估我们的训练策略对视觉理解能力的有效性，我们通过在第二阶段训练期间选择性地冻结组件进行了消融研究</p>
</li>
<li><p>我们分别进行了实验，在第二阶段进行指令微调时冻结映射器或LLM骨干，我们观察到在这两种情况下性能显著下降</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/PKvFeimLnMUNgWR.png" alt="image.png"></p>
<ul>
<li><p>这些结果进一步验证了我们的训练策略在增强视觉理解能力方面的有效性</p>
</li>
<li><p>这一经验证据强调了在指令微调阶段允许映射器和LLM骨干适应的重要性，突出了我们提出的训练方法对模型整体视觉理解能力的协同效应</p>
</li>
</ul>
<h4 id="Effect-of-the-Data-Efficiency-on-Understanding"><a href="#Effect-of-the-Data-Efficiency-on-Understanding" class="headerlink" title="Effect of the Data Efficiency on Understanding"></a>Effect of the Data Efficiency on Understanding</h4><ul>
<li><p>我们进行了实验来分析我们在第二阶段训练中使用的混合数据集</p>
</li>
<li><p>移除502K或665K理解数据集会对模型的理解性能产生负面影响</p>
</li>
<li><p>相反，当我们进一步纳入我们构建的生成指令数据集时，它增强了模型区分理解和生成指令的能力，并准确提高了VARGPT输出视觉生成特殊token（即<image_gen_start>、<image_gen>和<image_gen_end>）的能力，而不会显著影响其理解性能</p>
</li>
</ul>
<h4 id="Visualization-of-Training-Loss-Curve"><a href="#Visualization-of-Training-Loss-Curve" class="headerlink" title="Visualization of Training Loss Curve"></a>Visualization of Training Loss Curve</h4><ul>
<li>在下图中进一步展示了模型训练过程中第二阶段和第三阶段的损失曲线</li>
</ul>
<p><img src="https://s2.loli.net/2025/03/23/2ZqUydKw9AkQLCp.png" alt="image.png"></p>
<ul>
<li><p>这些损失曲线中观察到的趋势显示出合理且一致的下降，为我们的学习策略的有效性提供了经验支持</p>
</li>
<li><p>对这些曲线的分析表明，训练损失随着时间的推移呈现出原则性的下降，这在很大程度上证实了我们提出的学习方法的有效性</p>
</li>
<li><p>对第三阶段损失曲线的更仔细检查表明，模型的视觉生成能力仍有显著的优化潜力。这一观察表明，延长训练时间和扩展训练数据集可以在第三阶段进一步提高视觉生成性能</p>
</li>
</ul>
<hr>
<h2 id="6-Conclusion-Limitation-and-Future-Work"><a href="#6-Conclusion-Limitation-and-Future-Work" class="headerlink" title="6. Conclusion, Limitation and Future Work"></a>6. Conclusion, Limitation and Future Work</h2><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><ul>
<li><p>VARGPT是一种新颖的MLLMs，成功地将视觉理解和生成集成在一个统一的自回归框架中</p>
</li>
<li><p>通过创新的下一个token和下一个尺度预测范式，VARGPT扩展了传统MLLMs的能力，包括高效的视觉自回归生成</p>
</li>
<li><p>该模型的三阶段训练管道利用特别策划的数据集，实现了视觉和文本特征之间的有效对齐，增强了理解和生成能力</p>
</li>
<li><p>VARGPT在各种以视觉为中心的任务中表现出优于现有模型（如LLaVA-1.5）的性能。此外，它在自回归视觉生成和文本到图像合成方面表现出卓越的熟练度</p>
</li>
<li><p>这些成就突显了VARGPT的多功能性和推动多模态AI领域发展的潜力，为未来统一多模态模型的研究提供了有意义的探索</p>
</li>
</ul>
<h3 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h3><ul>
<li><p>由于我们的视觉生成数据集主要来自ImageNet的图像，VARGPT与某些扩散模型（如SDv2.1和更先进的模型如FLUX）之间仍存在明显的质量差距，这些模型在广泛的高质量图像数据集上进行了预训练。这种差异主要归因于训练数据的不同</p>
</li>
<li><p>在整个训练过程中，生成图像的分辨率始终设置为256x256像素。因此，当前版本的VARGPT仅支持在此分辨率下的自回归图像生成</p>
</li>
<li><p>尽管VARGPT在指令理解和指令到图像生成方面表现出初步的熟练度，在大多数情况下能够有效遵循用户输入指令，但在某些情况下，指令中的某些细微细节可能无法充分反映在生成的图像中</p>
</li>
<li><p>这一局限性在某些情况下表现出来，表明模型在全面捕捉和渲染复杂指令细节方面仍有改进空间</p>
</li>
</ul>
<h3 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h3><ul>
<li><p>为了实现更高质量的图像生成并支持更高分辨率的输出，我们计划改进下一个尺度预测模型架构，扩展图像数据集，提高图像质量，并实现动态分辨率能力</p>
</li>
<li><p>在VARGPT的后续版本中，我们打算探索支持统一的自回归视频理解和生成</p>
</li>
</ul>
<hr>
<h1 id="deepseek要点总结"><a href="#deepseek要点总结" class="headerlink" title="deepseek要点总结"></a>deepseek要点总结</h1><p>论文提纲与核心要点总结<br>标题: VARGPT: 视觉自回归多模态大语言模型中的统一理解与生成</p>
<hr>
<ol>
<li>研究背景与动机<br>•	现有问题:</li>
</ol>
<ul>
<li>   现有MLLMs（如LLaVA）专注于视觉理解但缺乏生成能力，扩散模型（如DDPMs）擅长生成但无法理解。</li>
<li>   现有统一模型多依赖外部工具（如扩散模型），未实现端到端混合模态输入输出。<br>•	目标:<br>构建单一模型，支持<strong>视觉理解（文本生成）与视觉生成（图像合成）</strong>的统一框架，突破模态割裂。</li>
</ul>
<hr>
<ol start="2">
<li>核心创新点</li>
<li>   双范式架构设计:</li>
</ol>
<ul>
<li>   视觉理解: 基于LLaVA-1.5，采用next-token预测生成文本（问答&#x2F;描述）。</li>
<li>   视觉生成: 新增视觉解码器与投影器，采用next-scale预测自回归生成多尺度图像。</li>
<li>   混合模态支持: 通过特殊标记（如<image_gen>）实现文本与图像输出的无缝切换。</li>
</ul>
<ol start="2">
<li>   三阶段训练策略:</li>
</ol>
<ul>
<li>   阶段1（预训练）: 对齐视觉-文本特征（ImageNet数据）。</li>
<li>   阶段2（指令微调）: 优化多轮对话与视觉理解能力（LLaVA-1.5数据）。</li>
<li>   阶段3（生成微调）: 提升指令到图像生成质量（ImageNet-Instruct数据）。</li>
</ul>
<ol start="3">
<li>   数据构建:</li>
</ol>
<ul>
<li>   构建ImageNet-Instruct指令数据集（130K~1.4M样本），通过LLM生成多样化指令模板。</li>
</ul>
<hr>
<ol start="3">
<li>实验与性能验证</li>
<li>   视觉理解任务:</li>
</ol>
<ul>
<li>   在MMBench、SEED-Bench、MMMU等基准中超越LLaVA-1.5、Qwen-VL等模型（如MMBench得分67.6 vs. LLaVA-1.5的62.7）。</li>
<li>   减少视觉幻觉（POPE准确率84.4%），推理能力更强。</li>
</ul>
<ol start="2">
<li>   视觉生成任务:</li>
</ol>
<ul>
<li>   支持混合模态输出（文本+图像），生成分辨率256×256。</li>
<li>   CLIP分数（27.4）与FID（12.6）优于基线，但受限于训练数据（ImageNet）规模，暂落后于大规模扩散模型。</li>
</ul>
<ol start="3">
<li>   消融实验:</li>
</ol>
<ul>
<li>   三阶段训练缺一不可，冻结视觉解码器或投影器显著降低性能。</li>
</ul>
<hr>
<ol start="4">
<li>局限性与未来方向<br>•	当前局限:</li>
</ol>
<ul>
<li>   生成质量受限于ImageNet数据规模与分辨率（仅256×256）。</li>
<li>   复杂指令细节生成能力待提升。<br>•	未来计划:</li>
<li>   扩展高质量数据集，支持动态分辨率生成。</li>
<li>   探索视频理解与生成统一框架。</li>
</ul>
<hr>
<ol start="5">
<li>核心贡献总结</li>
<li>   首创双范式统一模型: 通过next-token（理解）与next-scale（生成）预测，实现端到端多模态统一。</li>
<li>   高效训练框架: 三阶段分步优化，兼顾特征对齐与任务性能。</li>
<li>   开源数据与代码: 构建大规模指令数据集，模型与代码开源（项目页: vargpt-1.github.io)。</li>
</ol>
<hr>
<p>阅读价值:<br>•	技术突破: 首次在单一自回归模型中融合视觉理解与生成，为多模态统一模型提供新思路。<br>•	应用潜力: 可拓展至教育、设计、交互式AI等领域，支持复杂多模态任务。</p>
<hr>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/VARGPT/">VARGPT</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2025/03/23/mwWIq2fTsbhSEYu.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/646ae2df.html" title="MetaMorph:Multimodal Understanding and Generation via Instruction Tuning"><img class="cover" src="https://s2.loli.net/2025/03/24/b8F6sUEPxJuMcQn.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">MetaMorph:Multimodal Understanding and Generation via Instruction Tuning</div></div></a></div><div class="next-post pull-right"><a href="/post/a99d5b60.html" title="角色动画与运动仿真 05 Data-driven Character Animation"><img class="cover" src="https://s2.loli.net/2025/03/12/otfm56i8uqDXbKQ.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">角色动画与运动仿真 05 Data-driven Character Animation</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2024/07/16/lsEXfWtGT6eRu7k.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">w434</div><div class="author-info__description">An undergraduate majoring in AI at PKU.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">108</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">38</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">25</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/w434"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/w434" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcom to my blog.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#VARGPT-Unified-Understanding-and-Generation-in-a-Visual-Autoregressive-Multimodal-Large-Language-Model-%E8%A7%86%E8%A7%89%E8%87%AA%E5%9B%9E%E5%BD%92%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E7%BB%9F%E4%B8%80%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90"><span class="toc-number">1.</span> <span class="toc-text">VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model(视觉自回归多模态大语言模型中的统一理解与生成)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-number">1.2.</span> <span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Related-Work"><span class="toc-number">1.3.</span> <span class="toc-text">2. Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Visual-Generation"><span class="toc-number">1.3.1.</span> <span class="toc-text">Visual Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multimodel-Large-Language-Model"><span class="toc-number">1.3.2.</span> <span class="toc-text">Multimodel Large Language Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Unified-Models-For-Visual-Understanding-and-Generation"><span class="toc-number">1.3.3.</span> <span class="toc-text">Unified Models For Visual Understanding and Generation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Methodology"><span class="toc-number">1.4.</span> <span class="toc-text">3. Methodology</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Model-Architecture"><span class="toc-number">1.4.1.</span> <span class="toc-text">3.1. Model Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Visual-understanding-via-next-token-prediction"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">Visual understanding via next-token prediction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Visual-generation-via-next-scale-prediction"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">Visual generation via next-scale prediction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Prompt-template-for-mixed-modal-generation"><span class="toc-number">1.4.1.3.</span> <span class="toc-text">Prompt template for mixed-modal generation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Classifier-free-guidance-CFG-%E6%97%A0%E5%88%86%E7%B1%BB%E5%99%A8%E5%BC%95%E5%AF%BC"><span class="toc-number">1.4.1.4.</span> <span class="toc-text">Classifier-free guidance (CFG)(无分类器引导)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Training"><span class="toc-number">1.4.2.</span> <span class="toc-text">3.2. Training</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-Stage-1-Pretraining"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">3.2.1. Stage-1: Pretraining</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-Stage-2-SFT-for-Visual-Understanding"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">3.2.2. Stage-2: SFT for Visual Understanding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-Stage-3-SFT-for-Visual-Generation"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">3.2.3. Stage-3: SFT for Visual Generation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Unified-Instruction-following-Data"><span class="toc-number">1.5.</span> <span class="toc-text">4. Unified Instruction-following Data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Generation-Instruction-following-Dataset"><span class="toc-number">1.5.1.</span> <span class="toc-text">4.1. Generation Instruction-following Dataset</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ImageNet-1K-VL-Enriched"><span class="toc-number">1.5.1.1.</span> <span class="toc-text">ImageNet-1K-VL-Enriched</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Constructing-ImageNet-Instruct-130K-through-Deepseek-LLM"><span class="toc-number">1.5.1.2.</span> <span class="toc-text">Constructing ImageNet-Instruct-130K through Deepseek-LLM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LLM-invocation-template-LLM%E8%B0%83%E7%94%A8%E6%A8%A1%E6%9D%BF"><span class="toc-number">1.5.1.3.</span> <span class="toc-text">LLM invocation template(LLM调用模板)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Data-Composition-in-Three-Training-Stages"><span class="toc-number">1.5.2.</span> <span class="toc-text">4.2. Data Composition in Three Training Stages</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Stage-1"><span class="toc-number">1.5.2.1.</span> <span class="toc-text">Stage-1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Stage-2"><span class="toc-number">1.5.2.2.</span> <span class="toc-text">Stage-2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Stage-3"><span class="toc-number">1.5.2.3.</span> <span class="toc-text">Stage-3</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Experiments"><span class="toc-number">1.6.</span> <span class="toc-text">5.Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-MainResults"><span class="toc-number">1.6.1.</span> <span class="toc-text">5.1.MainResults</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Evaluation-on-Multi-modal-Benchmarks"><span class="toc-number">1.6.1.1.</span> <span class="toc-text">Evaluation on Multi-modal Benchmarks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Evaluation-on-visual-question-answering-tasks"><span class="toc-number">1.6.1.2.</span> <span class="toc-text">Evaluation on visual question answering tasks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Evaluation-on-Instruction-to-image-Task"><span class="toc-number">1.6.1.3.</span> <span class="toc-text">Evaluation on Instruction-to-image Task</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Method-Analysis"><span class="toc-number">1.6.2.</span> <span class="toc-text">5.2. Method Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Effect-of-the-Training-Strategies-on-Generation"><span class="toc-number">1.6.2.1.</span> <span class="toc-text">Effect of the Training Strategies on Generation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Effect-of-the-Training-Strategies-on-Understanding"><span class="toc-number">1.6.2.2.</span> <span class="toc-text">Effect of the Training Strategies on Understanding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Effect-of-the-Data-Efficiency-on-Understanding"><span class="toc-number">1.6.2.3.</span> <span class="toc-text">Effect of the Data Efficiency on Understanding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Visualization-of-Training-Loss-Curve"><span class="toc-number">1.6.2.4.</span> <span class="toc-text">Visualization of Training Loss Curve</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Conclusion-Limitation-and-Future-Work"><span class="toc-number">1.7.</span> <span class="toc-text">6. Conclusion, Limitation and Future Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusion"><span class="toc-number">1.7.1.</span> <span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Limitation"><span class="toc-number">1.7.2.</span> <span class="toc-text">Limitation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Future-Work"><span class="toc-number">1.7.3.</span> <span class="toc-text">Future Work</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#deepseek%E8%A6%81%E7%82%B9%E6%80%BB%E7%BB%93"><span class="toc-number">2.</span> <span class="toc-text">deepseek要点总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/c9144437.html" title="What is torch.nn really? 自学笔记"><img src="https://s2.loli.net/2024/09/30/C7lO2rZFBpEjMRX.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="What is torch.nn really? 自学笔记"/></a><div class="content"><a class="title" href="/post/c9144437.html" title="What is torch.nn really? 自学笔记">What is torch.nn really? 自学笔记</a><time datetime="2025-08-27T04:59:00.000Z" title="Created 2025-08-27 12:59:00">2025-08-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/1dfc20ef.html" title="Visual Instruction Tuning"><img src="https://s2.loli.net/2025/03/27/FuhmV3ksSe1Lbd9.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Visual Instruction Tuning"/></a><div class="content"><a class="title" href="/post/1dfc20ef.html" title="Visual Instruction Tuning">Visual Instruction Tuning</a><time datetime="2025-04-13T10:16:00.000Z" title="Created 2025-04-13 18:16:00">2025-04-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/b84b1ac8.html" title="自然语言处理 lec8 dp"><img src="https://s2.loli.net/2025/03/18/WoNFLe1mGTDzSsb.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="自然语言处理 lec8 dp"/></a><div class="content"><a class="title" href="/post/b84b1ac8.html" title="自然语言处理 lec8 dp">自然语言处理 lec8 dp</a><time datetime="2025-04-13T04:42:00.000Z" title="Created 2025-04-13 12:42:00">2025-04-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/f5256aa3.html" title="自然语言处理 lec7 phrase"><img src="https://s2.loli.net/2025/03/18/WoNFLe1mGTDzSsb.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="自然语言处理 lec7 phrase"/></a><div class="content"><a class="title" href="/post/f5256aa3.html" title="自然语言处理 lec7 phrase">自然语言处理 lec7 phrase</a><time datetime="2025-04-13T04:41:00.000Z" title="Created 2025-04-13 12:41:00">2025-04-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/eedd8f2c.html" title="自然语言处理 lec6 sper"><img src="https://s2.loli.net/2025/03/18/WoNFLe1mGTDzSsb.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="自然语言处理 lec6 sper"/></a><div class="content"><a class="title" href="/post/eedd8f2c.html" title="自然语言处理 lec6 sper">自然语言处理 lec6 sper</a><time datetime="2025-04-13T04:40:00.000Z" title="Created 2025-04-13 12:40:00">2025-04-13</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By w434</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcom to my blog.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">简</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>